{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5etkhPh0EAmh","outputId":"b16eb6fb-8b2e-4d30-d3a3-eb43c47a88af"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Collecting transformers\n","  Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.32.1\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n"]}],"source":["! pip install torch\n","! pip install transformers\n","! pip install scikit-learn\n","! pip install tqdm\n","! pip install numpy\n","! pip install datasets\n","! pip install nltk\n","import nltk\n","nltk.download('stopwords')\n","! pip install scipy\n","! pip install transformers[torch] accelerate\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I0HsCcLWtyXp"},"outputs":[],"source":["from torch import nn\n","from transformers import Trainer\n","\n","\n","class MultilabelTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.pop(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","        loss_fct = nn.BCEWithLogitsLoss()\n","        loss = loss_fct(logits.view(-1, self.model.config.num_labels),\n","                        labels.float().view(-1, self.model.config.num_labels))\n","        return (loss, outputs) if return_outputs else loss\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"srSNhaFn51Vd"},"outputs":[],"source":["import logging\n","import os\n","from dataclasses import dataclass\n","from enum import Enum\n","from typing import List, Optional\n","\n","import tqdm\n","import re\n","\n","from filelock import FileLock\n","from transformers import PreTrainedTokenizer, is_tf_available, is_torch_available\n","import datasets\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","@dataclass(frozen=True)\n","class InputFeatures:\n","    \"\"\"\n","    A single set of features of data.\n","    Property names are the same names as the corresponding inputs to a model.\n","    \"\"\"\n","\n","    input_ids: List[List[int]]\n","    attention_mask: Optional[List[List[int]]]\n","    token_type_ids: Optional[List[List[int]]]\n","    label: Optional[int]\n","\n","\n","class Split(Enum):\n","    train = \"train\"\n","    dev = \"dev\"\n","    test = \"test\"\n","\n","\n","if is_torch_available():\n","    import torch\n","    from torch.utils.data.dataset import Dataset\n","\n","    class MultipleChoiceDataset(Dataset):\n","        \"\"\"\n","        PyTorch multiple choice dataset class\n","        \"\"\"\n","\n","        features: List[InputFeatures]\n","\n","        def __init__(\n","            self,\n","            tokenizer: PreTrainedTokenizer,\n","            task: str,\n","            max_seq_length: Optional[int] = None,\n","            overwrite_cache=False,\n","            mode: Split = Split.train,\n","        ):\n","            dataset = datasets.load_dataset('lex_glue', task)\n","            tokenizer_name = re.sub('[^a-z]+', ' ', tokenizer.name_or_path).title().replace(' ', '')\n","            cached_features_file = os.path.join(\n","                '.cache',\n","                task,\n","                \"cached_{}_{}_{}_{}\".format(\n","                    mode.value,\n","                    tokenizer_name,\n","                    str(max_seq_length),\n","                    task,\n","                ),\n","            )\n","\n","            # Make sure only the first process in distributed training processes the dataset,\n","            # and the others will use the cache.\n","            lock_path = cached_features_file + \".lock\"\n","            if not os.path.exists(os.path.join('.cache', task)):\n","                if not os.path.exists('.cache'):\n","                    os.mkdir('.cache')\n","                os.mkdir(os.path.join('.cache', task))\n","            with FileLock(lock_path):\n","\n","                if os.path.exists(cached_features_file) and not overwrite_cache:\n","                    logger.info(f\"Loading features from cached file {cached_features_file}\")\n","                    self.features = torch.load(cached_features_file)\n","                else:\n","                    logger.info(f\"Creating features from dataset file at {task}\")\n","                    if mode == Split.dev:\n","                        examples = dataset['validation']\n","                    elif mode == Split.test:\n","                        examples = dataset['test']\n","                    elif mode == Split.train:\n","                        examples = dataset['train']\n","                    logger.info(\"Training examples: %s\", len(examples))\n","                    self.features = convert_examples_to_features(\n","                        examples,\n","                        max_seq_length,\n","                        tokenizer,\n","                    )\n","                    logger.info(\"Saving features into cached file %s\", cached_features_file)\n","                    torch.save(self.features, cached_features_file)\n","\n","        def __len__(self):\n","            return len(self.features)\n","\n","        def __getitem__(self, i) -> InputFeatures:\n","            return self.features[i]\n","\n","\n","if is_tf_available():\n","    import tensorflow as tf\n","\n","    class TFMultipleChoiceDataset:\n","        \"\"\"\n","        TensorFlow multiple choice dataset class\n","        \"\"\"\n","\n","        features: List[InputFeatures]\n","\n","        def __init__(\n","            self,\n","            tokenizer: PreTrainedTokenizer,\n","            task: str,\n","            max_seq_length: Optional[int] = 256,\n","            overwrite_cache=False,\n","            mode: Split = Split.train,\n","        ):\n","            dataset = datasets.load_dataset('lex_glue')\n","\n","            logger.info(f\"Creating features from dataset file at {task}\")\n","            if mode == Split.dev:\n","                examples = dataset['validation']\n","            elif mode == Split.test:\n","                examples = dataset['test']\n","            else:\n","                examples = dataset['train']\n","            logger.info(f\"{mode.name.title()} examples: %s\", len(examples))\n","\n","            self.features = convert_examples_to_features(\n","                examples,\n","                max_seq_length,\n","                tokenizer,\n","            )\n","\n","            def gen():\n","                for (ex_index, ex) in tqdm.tqdm(enumerate(self.features), desc=\"convert examples to features\"):\n","                    if ex_index % 10000 == 0:\n","                        logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n","\n","                    yield (\n","                        {\n","                            \"input_ids\": ex.input_ids,\n","                            \"attention_mask\": ex.attention_mask,\n","                            \"token_type_ids\": ex.token_type_ids,\n","                        },\n","                        ex.label,\n","                    )\n","\n","            self.dataset = tf.data.Dataset.from_generator(\n","                gen,\n","                (\n","                    {\n","                        \"input_ids\": tf.int32,\n","                        \"attention_mask\": tf.int32,\n","                        \"token_type_ids\": tf.int32,\n","                    },\n","                    tf.int64,\n","                ),\n","                (\n","                    {\n","                        \"input_ids\": tf.TensorShape([None, None]),\n","                        \"attention_mask\": tf.TensorShape([None, None]),\n","                        \"token_type_ids\": tf.TensorShape([None, None]),\n","                    },\n","                    tf.TensorShape([]),\n","                ),\n","            )\n","\n","        def get_dataset(self):\n","            self.dataset = self.dataset.apply(tf.data.experimental.assert_cardinality(len(self.features)))\n","\n","            return self.dataset\n","\n","        def __len__(self):\n","            return len(self.features)\n","\n","        def __getitem__(self, i) -> InputFeatures:\n","            return self.features[i]\n","\n","\n","def convert_examples_to_features(\n","    examples: datasets.Dataset,\n","    max_length: int,\n","    tokenizer: PreTrainedTokenizer,\n",") -> List[InputFeatures]:\n","    \"\"\"\n","    Loads a data file into a list of `InputFeatures`\n","    \"\"\"\n","    features = []\n","    for (ex_index, example) in tqdm.tqdm(enumerate(examples), desc=\"convert examples to features\"):\n","        if ex_index % 10000 == 0:\n","            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n","        choices_inputs = []\n","        for ending_idx, ending in enumerate(example['endings']):\n","            context = example['context']\n","            inputs = tokenizer(\n","                context,\n","                ending,\n","                add_special_tokens=True,\n","                max_length=max_length,\n","                padding=\"max_length\",\n","                truncation=True,\n","            )\n","\n","            choices_inputs.append(inputs)\n","\n","        label = example['label']\n","\n","        input_ids = [x[\"input_ids\"] for x in choices_inputs]\n","        attention_mask = (\n","            [x[\"attention_mask\"] for x in choices_inputs] if \"attention_mask\" in choices_inputs[0] else None\n","        )\n","        token_type_ids = (\n","            [x[\"token_type_ids\"] for x in choices_inputs] if \"token_type_ids\" in choices_inputs[0] else None\n","        )\n","\n","        features.append(\n","            InputFeatures(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                token_type_ids=token_type_ids,\n","                label=label,\n","            )\n","        )\n","\n","    for f in features[:2]:\n","        logger.info(\"*** Example ***\")\n","        logger.info(\"feature: %s\" % f)\n","\n","    return features\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":10,"status":"error","timestamp":1690394816701,"user":{"displayName":"sweta simran","userId":"15795161217929105177"},"user_tz":-60},"id":"_AsypDRFtg93","outputId":"ca44a612-10f0-4f37-9db7-798cccd17a58"},"outputs":[{"name":"stderr","output_type":"stream","text":["usage: ipykernel_launcher.py [-h] --model_name_or_path MODEL_NAME_OR_PATH\n","                             [--config_name CONFIG_NAME]\n","                             [--tokenizer_name TOKENIZER_NAME]\n","                             [--cache_dir CACHE_DIR] [--task_name TASK_NAME]\n","                             [--max_seq_length MAX_SEQ_LENGTH]\n","                             [--pad_to_max_length [PAD_TO_MAX_LENGTH]]\n","                             [--no_pad_to_max_length]\n","                             [--max_train_samples MAX_TRAIN_SAMPLES]\n","                             [--max_eval_samples MAX_EVAL_SAMPLES]\n","                             [--max_predict_samples MAX_PREDICT_SAMPLES]\n","                             [--overwrite_cache [OVERWRITE_CACHE]]\n","                             --output_dir OUTPUT_DIR\n","                             [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]\n","                             [--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]]\n","                             [--do_predict [DO_PREDICT]]\n","                             [--evaluation_strategy {no,steps,epoch}]\n","                             [--prediction_loss_only [PREDICTION_LOSS_ONLY]]\n","                             [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n","                             [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n","                             [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n","                             [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n","                             [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n","                             [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n","                             [--eval_delay EVAL_DELAY]\n","                             [--learning_rate LEARNING_RATE]\n","                             [--weight_decay WEIGHT_DECAY]\n","                             [--adam_beta1 ADAM_BETA1]\n","                             [--adam_beta2 ADAM_BETA2]\n","                             [--adam_epsilon ADAM_EPSILON]\n","                             [--max_grad_norm MAX_GRAD_NORM]\n","                             [--num_train_epochs NUM_TRAIN_EPOCHS]\n","                             [--max_steps MAX_STEPS]\n","                             [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup,inverse_sqrt,reduce_lr_on_plateau}]\n","                             [--warmup_ratio WARMUP_RATIO]\n","                             [--warmup_steps WARMUP_STEPS]\n","                             [--log_level {debug,info,warning,error,critical,passive}]\n","                             [--log_level_replica {debug,info,warning,error,critical,passive}]\n","                             [--log_on_each_node [LOG_ON_EACH_NODE]]\n","                             [--no_log_on_each_node]\n","                             [--logging_dir LOGGING_DIR]\n","                             [--logging_strategy {no,steps,epoch}]\n","                             [--logging_first_step [LOGGING_FIRST_STEP]]\n","                             [--logging_steps LOGGING_STEPS]\n","                             [--logging_nan_inf_filter [LOGGING_NAN_INF_FILTER]]\n","                             [--no_logging_nan_inf_filter]\n","                             [--save_strategy {no,steps,epoch}]\n","                             [--save_steps SAVE_STEPS]\n","                             [--save_total_limit SAVE_TOTAL_LIMIT]\n","                             [--save_safetensors [SAVE_SAFETENSORS]]\n","                             [--save_on_each_node [SAVE_ON_EACH_NODE]]\n","                             [--no_cuda [NO_CUDA]]\n","                             [--use_mps_device [USE_MPS_DEVICE]] [--seed SEED]\n","                             [--data_seed DATA_SEED]\n","                             [--jit_mode_eval [JIT_MODE_EVAL]]\n","                             [--use_ipex [USE_IPEX]] [--bf16 [BF16]]\n","                             [--fp16 [FP16]] [--fp16_opt_level FP16_OPT_LEVEL]\n","                             [--half_precision_backend {auto,cuda_amp,apex,cpu_amp}]\n","                             [--bf16_full_eval [BF16_FULL_EVAL]]\n","                             [--fp16_full_eval [FP16_FULL_EVAL]] [--tf32 TF32]\n","                             [--local_rank LOCAL_RANK]\n","                             [--ddp_backend {nccl,gloo,mpi,ccl}]\n","                             [--tpu_num_cores TPU_NUM_CORES]\n","                             [--tpu_metrics_debug [TPU_METRICS_DEBUG]]\n","                             [--debug DEBUG [DEBUG ...]]\n","                             [--dataloader_drop_last [DATALOADER_DROP_LAST]]\n","                             [--eval_steps EVAL_STEPS]\n","                             [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n","                             [--past_index PAST_INDEX] [--run_name RUN_NAME]\n","                             [--disable_tqdm DISABLE_TQDM]\n","                             [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]\n","                             [--no_remove_unused_columns]\n","                             [--label_names LABEL_NAMES [LABEL_NAMES ...]]\n","                             [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]\n","                             [--metric_for_best_model METRIC_FOR_BEST_MODEL]\n","                             [--greater_is_better GREATER_IS_BETTER]\n","                             [--ignore_data_skip [IGNORE_DATA_SKIP]]\n","                             [--sharded_ddp SHARDED_DDP] [--fsdp FSDP]\n","                             [--fsdp_min_num_params FSDP_MIN_NUM_PARAMS]\n","                             [--fsdp_config FSDP_CONFIG]\n","                             [--fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP]\n","                             [--deepspeed DEEPSPEED]\n","                             [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\n","                             [--optim {adamw_hf,adamw_torch,adamw_torch_fused,adamw_torch_xla,adamw_apex_fused,adafactor,adamw_anyprecision,sgd,adagrad,adamw_bnb_8bit,adamw_8bit,lion_8bit,lion_32bit,paged_adamw_32bit,paged_adamw_8bit,paged_lion_32bit,paged_lion_8bit}]\n","                             [--optim_args OPTIM_ARGS]\n","                             [--adafactor [ADAFACTOR]]\n","                             [--group_by_length [GROUP_BY_LENGTH]]\n","                             [--length_column_name LENGTH_COLUMN_NAME]\n","                             [--report_to REPORT_TO [REPORT_TO ...]]\n","                             [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]\n","                             [--ddp_bucket_cap_mb DDP_BUCKET_CAP_MB]\n","                             [--ddp_broadcast_buffers DDP_BROADCAST_BUFFERS]\n","                             [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]\n","                             [--no_dataloader_pin_memory]\n","                             [--skip_memory_metrics [SKIP_MEMORY_METRICS]]\n","                             [--no_skip_memory_metrics]\n","                             [--use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]]\n","                             [--push_to_hub [PUSH_TO_HUB]]\n","                             [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n","                             [--hub_model_id HUB_MODEL_ID]\n","                             [--hub_strategy {end,every_save,checkpoint,all_checkpoints}]\n","                             [--hub_token HUB_TOKEN]\n","                             [--hub_private_repo [HUB_PRIVATE_REPO]]\n","                             [--gradient_checkpointing [GRADIENT_CHECKPOINTING]]\n","                             [--include_inputs_for_metrics [INCLUDE_INPUTS_FOR_METRICS]]\n","                             [--fp16_backend {auto,cuda_amp,apex,cpu_amp}]\n","                             [--push_to_hub_model_id PUSH_TO_HUB_MODEL_ID]\n","                             [--push_to_hub_organization PUSH_TO_HUB_ORGANIZATION]\n","                             [--push_to_hub_token PUSH_TO_HUB_TOKEN]\n","                             [--mp_parameters MP_PARAMETERS]\n","                             [--auto_find_batch_size [AUTO_FIND_BATCH_SIZE]]\n","                             [--full_determinism [FULL_DETERMINISM]]\n","                             [--torchdynamo TORCHDYNAMO]\n","                             [--ray_scope RAY_SCOPE]\n","                             [--ddp_timeout DDP_TIMEOUT]\n","                             [--torch_compile [TORCH_COMPILE]]\n","                             [--torch_compile_backend TORCH_COMPILE_BACKEND]\n","                             [--torch_compile_mode TORCH_COMPILE_MODE]\n","                             [--xpu_backend {mpi,ccl,gloo}] [--ptl PTL]\n","ipykernel_launcher.py: error: the following arguments are required: --model_name_or_path, --output_dir\n"]},{"ename":"SystemExit","evalue":"ignored","output_type":"error","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"]}],"source":["#!/usr/bin/env python\n","# coding=utf-8\n","\"\"\" Finetuning models on CaseHOLD (e.g. Bert, RoBERTa, LEGAL-BERT).\"\"\"\n","\n","import logging\n","import os\n","from dataclasses import dataclass, field\n","from typing import Optional\n","\n","import numpy as np\n","import random\n","import shutil\n","import glob\n","\n","import transformers\n","from transformers import (\n","\tAutoConfig,\n","\tAutoModelForMultipleChoice,\n","\tAutoTokenizer,\n","\tEvalPrediction,\n","\tHfArgumentParser,\n","\tTrainer,\n","\tTrainingArguments,\n","\tset_seed,\n",")\n","from transformers.trainer_utils import is_main_process\n","from transformers import EarlyStoppingCallback\n","#from casehold_helpers import MultipleChoiceDataset, Split\n","from sklearn.metrics import f1_score\n","#from models.deberta import DebertaForMultipleChoice\n","\n","\n","logger = logging.getLogger(__name__)\n","output_dir = os.getcwd()\n","\n","@dataclass\n","class ModelArguments:\n","\t\"\"\"\n","\tArguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n","\t\"\"\"\n","\n","\tmodel_name_or_path: str = field(\n","\t\tmetadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n","\t)\n","\tconfig_name: Optional[str] = field(\n","\t\tdefault=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n","\t)\n","\ttokenizer_name: Optional[str] = field(\n","\t\tdefault=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n","\t)\n","\tcache_dir: Optional[str] = field(\n","\t\tdefault=None,\n","\t\tmetadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n","\t)\n","\n","\n","@dataclass\n","class DataTrainingArguments:\n","\t\"\"\"\n","\tArguments pertaining to what data we are going to input our model for training and eval.\n","\t\"\"\"\n","\n","\ttask_name: str = field(default=\"case_hold\", metadata={\"help\": \"The name of the task to train on\"})\n","\tmax_seq_length: int = field(\n","\t\tdefault=256,\n","\t\tmetadata={\n","\t\t\t\"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n","\t\t\t\"than this will be truncated, sequences shorter will be padded.\"\n","\t\t},\n","\t)\n","\tpad_to_max_length: bool = field(\n","\t\tdefault=True,\n","\t\tmetadata={\n","\t\t\t\"help\": \"Whether to pad all samples to `max_seq_length`. \"\n","\t\t\t\"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n","\t\t},\n","\t)\n","\tmax_train_samples: Optional[int] = field(\n","\t\tdefault=None,\n","\t\tmetadata={\n","\t\t\t\"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n","\t\t\t\"value if set.\"\n","\t\t},\n","\t)\n","\tmax_eval_samples: Optional[int] = field(\n","\t\tdefault=None,\n","\t\tmetadata={\n","\t\t\t\"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n","\t\t\t\"value if set.\"\n","\t\t},\n","\t)\n","\tmax_predict_samples: Optional[int] = field(\n","\t\tdefault=None,\n","\t\tmetadata={\n","\t\t\t\"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n","\t\t\t\"value if set.\"\n","\t\t},\n","\t)\n","\toverwrite_cache: bool = field(\n","\t\tdefault=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n","\t)\n","\n","\n","def main():\n","\t# See all possible arguments in src/transformers/training_args.py\n","\t# or by passing the --help flag to this script.\n","\t# We now keep distinct sets of args, for a cleaner separation of concerns.\n","\n","\tparser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n","\t# Add custom arguments for computing pre-train loss\n","\tparser.add_argument(\"--ptl\", type=bool, default=False)\n","\tmodel_args, data_args, training_args, custom_args = parser.parse_args_into_dataclasses()\n","\n","\tif (\n","\t\tos.path.exists(training_args.output_dir)\n","\t\tand os.listdir(training_args.output_dir)\n","\t\tand training_args.do_train\n","\t\tand not training_args.overwrite_output_dir\n","\t):\n","\t\traise ValueError(\n","\t\t\tf\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n","\t\t)\n","\n","\t# Setup logging\n","\tlogging.basicConfig(\n","\t\tformat=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n","\t\tdatefmt=\"%m/%d/%Y %H:%M:%S\",\n","\t\tlevel=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n","\t)\n","\tlogger.warning(\n","\t\t\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n","\t\ttraining_args.local_rank,\n","\t\ttraining_args.device,\n","\t\ttraining_args.n_gpu,\n","\t\tbool(training_args.local_rank != -1),\n","\t\ttraining_args.fp16,\n","\t)\n","\t# Set the verbosity to info of the Transformers logger (on main process only):\n","\tif is_main_process(training_args.local_rank):\n","\t\ttransformers.utils.logging.set_verbosity_info()\n","\t\ttransformers.utils.logging.enable_default_handler()\n","\t\ttransformers.utils.logging.enable_explicit_format()\n","\tlogger.info(\"Training/evaluation parameters %s\", training_args)\n","\n","\t# Set seed\n","\tset_seed(training_args.seed)\n","\n","\t# Load pretrained model and tokenizer\n","\tconfig = AutoConfig.from_pretrained(\n","\t\tmodel_args.config_name if model_args.config_name else model_args.model_name_or_path,\n","\t\tnum_labels=5,\n","\t\tfinetuning_task=data_args.task_name,\n","\t\tcache_dir=model_args.cache_dir,\n","\t)\n","\n","\tif config.model_type == 'big_bird':\n","\t\tconfig.attention_type = 'original_full'\n","\telif config.model_type == 'longformer':\n","\t\tconfig.attention_window = [data_args.max_seq_length] * config.num_hidden_layers\n","\n","\ttokenizer = AutoTokenizer.from_pretrained(\n","\t\tmodel_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n","\t\tcache_dir=model_args.cache_dir,\n","\t\t# Default fast tokenizer is buggy on CaseHOLD task, switch to legacy tokenizer\n","\t\tuse_fast=True,\n","\t)\n","\n","\tif config.model_type != 'deberta':\n","\t\tmodel = AutoModelForMultipleChoice.from_pretrained(\n","\t\t\tmodel_args.model_name_or_path,\n","\t\t\tfrom_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","\t\t\tconfig=config,\n","\t\t\tcache_dir=model_args.cache_dir,\n","\t\t)\n","\telse:\n","\t\tmodel = DebertaForMultipleChoice.from_pretrained(\n","\t\t\tmodel_args.model_name_or_path,\n","\t\t\tfrom_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","\t\t\tconfig=config,\n","\t\t\tcache_dir=model_args.cache_dir,\n","\t\t)\n","\n","\ttrain_dataset = None\n","\teval_dataset = None\n","\n","\t# If do_train passed, train_dataset by default loads train split from file named train.csv in data directory\n","\tif training_args.do_train:\n","\t\ttrain_dataset = \\\n","\t\t\tMultipleChoiceDataset(\n","\t\t\t\ttokenizer=tokenizer,\n","\t\t\t\ttask=data_args.task_name,\n","\t\t\t\tmax_seq_length=data_args.max_seq_length,\n","\t\t\t\toverwrite_cache=data_args.overwrite_cache,\n","\t\t\t\tmode=Split.train,\n","\t\t\t)\n","\n","\t# If do_eval or do_predict passed, eval_dataset by default loads dev split from file named dev.csv in data directory\n","\tif training_args.do_eval:\n","\t\teval_dataset = \\\n","\t\t\tMultipleChoiceDataset(\n","\t\t\t\ttokenizer=tokenizer,\n","\t\t\t\ttask=data_args.task_name,\n","\t\t\t\tmax_seq_length=data_args.max_seq_length,\n","\t\t\t\toverwrite_cache=data_args.overwrite_cache,\n","\t\t\t\tmode=Split.dev,\n","\t\t\t)\n","\n","\tif training_args.do_predict:\n","\t\tpredict_dataset = \\\n","\t\t\tMultipleChoiceDataset(\n","\t\t\t\ttokenizer=tokenizer,\n","\t\t\t\ttask=data_args.task_name,\n","\t\t\t\tmax_seq_length=data_args.max_seq_length,\n","\t\t\t\toverwrite_cache=data_args.overwrite_cache,\n","\t\t\t\tmode=Split.test,\n","\t\t\t)\n","\n","\tif training_args.do_train:\n","\t\tif data_args.max_train_samples is not None:\n","\t\t\ttrain_dataset = train_dataset[:data_args.max_train_samples]\n","\t\t# Log a few random samples from the training set:\n","\t\tfor index in random.sample(range(len(train_dataset)), 3):\n","\t\t\tlogger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n","\n","\tif training_args.do_eval:\n","\t\tif data_args.max_eval_samples is not None:\n","\t\t\teval_dataset = eval_dataset[:data_args.max_eval_samples]\n","\n","\tif training_args.do_predict:\n","\t\tif data_args.max_predict_samples is not None:\n","\t\t\tpredict_dataset = predict_dataset[:data_args.max_predict_samples]\n","\n","\t# Define custom compute_metrics function, returns macro F1 metric for CaseHOLD task\n","\tdef compute_metrics(p: EvalPrediction):\n","\t\tpreds = np.argmax(p.predictions, axis=1)\n","\t\t# Compute macro and micro F1 for 5-class CaseHOLD task\n","\t\tmacro_f1 = f1_score(y_true=p.label_ids, y_pred=preds, average='macro', zero_division=0)\n","\t\tmicro_f1 = f1_score(y_true=p.label_ids, y_pred=preds, average='micro', zero_division=0)\n","\t\treturn {'macro-f1': macro_f1, 'micro-f1': micro_f1}\n","\n","\t# Initialize our Trainer\n","\ttrainer = Trainer(\n","\t\tmodel=model,\n","\t\targs=training_args,\n","\t\ttrain_dataset=train_dataset,\n","\t\teval_dataset=eval_dataset,\n","\t\tcompute_metrics=compute_metrics,\n","\t\tcallbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n","\t)\n","\n","\t# Training\n","\tif training_args.do_train:\n","\t\ttrainer.train(\n","\t\t\tmodel_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n","\t\t)\n","\t\ttrainer.save_model()\n","\t\t# Re-save the tokenizer for model sharing\n","\t\tif trainer.is_world_process_zero():\n","\t\t\ttokenizer.save_pretrained(training_args.output_dir)\n","\n","\t# Evaluation on eval_dataset\n","\tif training_args.do_eval:\n","\t\tlogger.info(\"*** Evaluate ***\")\n","\t\tmetrics = trainer.evaluate(eval_dataset=eval_dataset)\n","\n","\t\tmax_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n","\t\tmetrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n","\n","\t\ttrainer.log_metrics(\"eval\", metrics)\n","\t\ttrainer.save_metrics(\"eval\", metrics)\n","\n","\t# Predict on eval_dataset\n","\tif training_args.do_predict:\n","\t\tlogger.info(\"*** Predict ***\")\n","\n","\t\tpredictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix=\"predict\")\n","\n","\t\tmax_predict_samples = (\n","\t\t\tdata_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)\n","\t\t)\n","\t\tmetrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n","\n","\t\ttrainer.log_metrics(\"predict\", metrics)\n","\t\ttrainer.save_metrics(\"predict\", metrics)\n","\n","\t\toutput_predict_file = os.path.join(training_args.output_dir, \"test_predictions.csv\")\n","\t\tif trainer.is_world_process_zero():\n","\t\t\twith open(output_predict_file, \"w\") as writer:\n","\t\t\t\tfor index, pred_list in enumerate(predictions):\n","\t\t\t\t\tpred_line = '\\t'.join([f'{pred:.5f}' for pred in pred_list])\n","\t\t\t\t\twriter.write(f\"{index}\\t{pred_line}\\n\")\n","\n","\t# Clean up checkpoints\n","\tcheckpoints = [filepath for filepath in glob.glob(f'{training_args.output_dir}/*/') if '/checkpoint' in filepath]\n","\tfor checkpoint in checkpoints:\n","\t\tshutil.rmtree(checkpoint)\n","\n","\n","def _mp_fn(index):\n","\t# For xla_spawn (TPUs)\n","\tmain()\n","\n","\n","if __name__ == \"__main__\":\n","\tmain()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cJxEQIrKThcP"},"outputs":[],"source":["import json\n","import random\n","import tqdm\n","from collections import Counter\n","\n","# NOTE: The dataset has been first enriched with metadata from SEC-EDGAR\n","# to figure out the year of submission for the original filings. This\n","# part is missing from the script.\n","\n","# Parse original (augmented) dataset\n","categories = []\n","with open('ledgar.jsonl') as file:\n","    for line in tqdm.tqdm(file.readlines()):\n","        data = json.loads(line)\n","        categories.extend(data['labels'])\n","\n","# Find the top-100 labels.\n","categories = set([label for label, count in Counter(categories).most_common()[:100]])\n","\n","\n","# Subsample examples labeled with one of the top-100 labels.\n","with open('ledgar_small.jsonl', 'w') as out_file:\n","    with open('ledgar.jsonl') as file:\n","        for line in tqdm.tqdm(file.readlines()):\n","            data = json.loads(line)\n","            if set(data['labels']).intersection(categories):\n","                labels = set(data['labels']).intersection(categories)\n","                if len(labels) == 1:\n","                    data['labels'] = sorted(list(labels))\n","                    data.pop('clause_types', None)\n","                    out_file.write(json.dumps(data)+'\\n')\n","\n","\n","# Organize examples in clusters by year\n","years = []\n","samples = {year: [] for year in ['2016', '2017', '2018', '2019']}\n","with open('ledgar_small.jsonl') as file:\n","    for line in tqdm.tqdm(file.readlines()):\n","        data = json.loads(line)\n","        years.append(data['year'])\n","        data.pop('filer_cik', None)\n","        data.pop('filer_name', None)\n","        data.pop('filer_state', None)\n","        data.pop('filer_industry', None)\n","        samples[data['year']].append(data)\n","\n","\n","# Write final dataset 60k/10k/10k\n","random.seed(1)\n","with open('ledgar.jsonl', 'w') as file:\n","    final_samples = random.sample(samples['2016'], 30000)\n","    final_samples += random.sample(samples['2017'], 30000)\n","    for sample in final_samples:\n","        sample['data_type'] = 'train'\n","        file.write(json.dumps(sample) + '\\n')\n","    final_samples = random.sample(samples['2018'], 10000)\n","    for sample in final_samples:\n","        sample['data_type'] = 'dev'\n","        file.write(json.dumps(sample) + '\\n')\n","    final_samples = random.sample(samples['2019'], 10000)\n","    for sample in final_samples:\n","        sample['data_type'] = 'test'\n","        file.write(json.dumps(sample) + '\\n')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bmki_JYLHjkU"},"outputs":[],"source":["\n","from datasets import load_dataset\n","dataset_dict = load_dataset(\"lex_glue\",'ecthr_a')\n","#print(dataset)\n","#Divide into train,dev,test\n","\n","from sklearn.model_selection import train_test_split\n","\n","#data_list = list(dataset_dict.items())\n","\n","train_set_dict = dataset_dict['train'].data\n","test_set_dict = dataset_dict['test'].data\n","validation_set_dict = dataset_dict['validation'].data\n","print(train_set_dict[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["b3514618fe784696b1f92d3b0a2eba45","78163e08a3ca4e839a9d5e02c6a76b8f","2ae893aba8124b5c8fb2662682b91c10","844ce341d3f5416db18e8023b232de94","a424cd7b6a5e45408a13aa6c8cd22a22","d82b200fd9ee4b0dbd0efa3ddb704384","66f2f15ea05246c28c1a204c9836a1fe","175f9c4606a44269826bea6f85d0bcb9","fd38229f818f4aeeba121c404c615039","8fbbeb6ab86d45f3af0c23f3367607ef","9a8ffcbdeb7d44a58c098e567708ebaf","b32fea3637774c6ea5a28ca4833606dd","4628f77f505e43d8b85d03e1c3553466","8d7e655358124f109fea406a52dbb838","e8ff6445711c48b8a69d81bf08ac4592","ccc324b1a6a742d893f3b37707e043f1","7890672eec294c578a85d0aef2a97bc3","fcfff04f32524fa28f02686651e0211f","2c4e500778274b419b7bfc63f56e428e","8d977d7ac721400e94f8182f0313e1d9","5832b1b1c5d5415c83038598caf8cf6a","8aeb0d302fa14feabd080529ab5c57e4","cbdab9b8ebc64392b6f20859d78060a4","0b16d58053a045849897820e3a7ffe83","f7c34df14bf5459288935eaa5f2bb27a","0b5e53eb379a46fab9a80de7e89db759","a05ffac668684b9c8086cc6097700f5d","334076ec8d644e4e87e01cb287b64831","d9afaf3b0bed424a8c2336bc3a85db80","a1283fab055c415b98e9b3368c83c2ae","dfe8ea0932f442e5af001317921794a8","760dbdf67db245e69d427e622d902930","f6231b35767542d3a02b2d201a4a76e1","0faca49748a646b29484e57ee882fe7a","2ca65cea389c48b4a7ed61a1ad0dcac7","5822aa2747124f949a865101362b4799","0fc259f08b1d4db9b3dbbfbfadd56a5b","ed93be7ed0e94c4abef4b788c4f16c72","8502518447274e3f94837fe169609cdd","ccdf056b54d34126ab4e07762e63ac3a","b2bf3936028e46f284f0e133ab0442f0","bfbf019599b9459596a8680831405baa","05eb5a9b0b354ff4b05492396f31c59b","bb37f09a3d62447781b0b85dc630ab66","85785c406ebe4760afc670f7948842e2","5e71d56bed6849ba8fc65c11b063f9d2","044deea78dc04dfebd969c86d5929508","3fae098be5b348869e17596c5d27271a","4241120bd25a48569e9c388d5f971034","42686c3a3d9742fcb4709f5418c2c118","5ff8b9d7669a4458a6548c8f16564937","a9d28b43412d40008f1f07691eec9663","0bb9347b6f1049aba75419ce6122fbab","08db827145d747f197009e1fa3eb2bf7","d3c436fd27754afaa7af0853548a29b0","8d7822942e784eee999f80f963426de8","a88643a6e945447da49edac244c01613","442aa70f9886400d91db22c76f52c384","5928a32ca68f4b1c8b43972d06b15be9","63410da6bca94c4794fe26e8a7bd99cd","4f516cdc6c19457a884561016a018dc9","100222d038aa4f608f826703b3d8e5b2","5654469094e24821897d079767136228","be0edf59be5948f0a2613d2093b79c33","6f6499fd109240f6b13130c9e17189d3","62e79989e9bf4b24ba1f907057401aa9","3700b4a06c8643749c3fab13457c9126","32a2dea83fc14f38b9a7429de18b6f24","03394d00c4514c398c1db60b2e528e04","c12c74e23fba4d4c998b897b7d264e62","9d11e58451e347a4bb1716e48a27a01b","3242dc9cae174272b7832b13c5cbd7f3","c2c316cbb1d84e5daf6eaae6198ce327","5c1ed27cb61f4c528439177902142c16","b8ae4688334947abbdc800cfb12ce2de","23a7694490ff4e48aa9b9e84c5b3eed7","d440c2c66a05472aa91533dec8d55291","1f0e89b7fce7466991cb7cdddca227ad","88ed28f105f24dccb6a79a82c2307511","c0abb82712b24bba86d4157e1f808ae9","225067431398426bb24352b8222b76d9","0dfc0eceea84402b84929f446cf845c4","59b3d82644804198878c767ed418e8df","acaea09850b6496da8d7dba5e19b85f1","cea610e123784c3f8700bd8f2507b32e","1efe37c4d4af44c08f57dc3f0e556a64","a35e28d9075c4ec0a66488d426e87208","67c1f3d339514e48a19ad8e8ec186ba0","89748838c40e4169a0460777d18cb9af","671926b782c5411b8740f0ec5c8fdacc","fc7bbe67316149b1a0c5491448875588","38f385ed5ddf4bf0ab4fa79268dfa96a","c74c1ca77e8145269c4a057504acca50","b3c68f94de8c4237805f183a1f356f92","9c29bbb44ab84ef8907c1b80512ca591","3fffc34d1bdc44db8c06a023d26b2f02","42adbd27a7bd49c385a0d58b833f7154","7d001688a98c4c3cba13b0091452184a","86e20fd8fcac4136ab3e3f1815ff9ec9","2be51803c81e42299430bba9cb98bdb5","409ec96032344760be05980b1eb972f9","c9f90de294d941f2a951aaad8a6f7d10","cea52e6d84384c23ba5e454d48c0b1c8","c35d2103ebb64539884edcb111e83556","0c6c496bad0046b08977e7d4a1f979d0","f5cbf7a5d5d049c9a30ea5391d19e6d7","041c1c2c385f439b9975cc5a73758f7b","d3d4fb94563c46df8e13ee78dc6957a9","b4802db5e3f540608795aa6393159201","1b102b8c2d484f529dcdcb5b7a5a058c","29f1e97852634d70878e86e92e8096a4","56cc2e17506d4a93af48a3f47595dfde","e0169761bfb54da990a144ef63ada248","e6eb8ad47044414f936c44b75fdb5c2e","5505a2a9351d40579ba1577352efab60","49b2d89cb90c445a87d98d4410a1ce5c","c5148a331c324ae48e40d1aa37ce6dd5","bb74d1a827014de89a3adde9dea3b60e","8e5d55c80c204b9aa1e5e551f41b1c9c","b0c634c3affe448998ba5d19ea0bf49a","77904bf74a2340f09bc9c59ca8d82bdf","9c5b8a7d920d4adcb775c27540f80a91","e42adafaed124a4a8ba1551451fb20b7","de998f33582046e4b6c114024fab5682","83a4d85fb138464a985006ddf96efead","8db9c65e49134f2896f859576eddadfe","761448a6a3584487bd4a0eef06122ce7","4d46c83672584647ab676514936de69f","3456d453a5a84bf4bdfd6ae8a75c7f4e","ada8af032996408d978fb5a87790c421","56c0699771894c5c83b7d5f4a1d4e2b2","7b502efce2a749f391703c9fda995817"]},"executionInfo":{"elapsed":10463289,"status":"ok","timestamp":1691265057407,"user":{"displayName":"sweta simran","userId":"15795161217929105177"},"user_tz":-60},"id":"q6dZguykI8x-","outputId":"ac967a72-8f7b-4026-9bd4-99f78fb706d6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting nlpaug\n","  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/410.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/410.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m337.9/410.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.22.4)\n","Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.5.3)\n","Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.27.1)\n","Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (4.6.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.12.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (1.16.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.65.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.11.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2022.7.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2023.7.22)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.4)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.4.1)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n","Installing collected packages: nlpaug\n","Successfully installed nlpaug-1.1.11\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b3514618fe784696b1f92d3b0a2eba45","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b32fea3637774c6ea5a28ca4833606dd","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cbdab9b8ebc64392b6f20859d78060a4","version_major":2,"version_minor":0},"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0faca49748a646b29484e57ee882fe7a","version_major":2,"version_minor":0},"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"85785c406ebe4760afc670f7948842e2","version_major":2,"version_minor":0},"text/plain":["Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8d7822942e784eee999f80f963426de8","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/23.3k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3700b4a06c8643749c3fab13457c9126","version_major":2,"version_minor":0},"text/plain":["Downloading metadata:   0%|          | 0.00/21.0k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1f0e89b7fce7466991cb7cdddca227ad","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/32.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"89748838c40e4169a0460777d18cb9af","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/511k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2be51803c81e42299430bba9cb98bdb5","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/5532 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"29f1e97852634d70878e86e92e8096a4","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/1607 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9c5b8a7d920d4adcb775c27540f80a91","version_major":2,"version_minor":0},"text/plain":["Generating validation split:   0%|          | 0/2275 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Augmented Text: [\"18. 10 avc / h. 264 notice : if the software is used to make video calls ( i ) between a personal smartphone and a device that is not a personal computer with ( ii ) between devices that are not family computers, the avc / h. 264 technology may be used to facilitate video functionality in which case the following notice applies : the avc video functionality in this product remains reserved under any avc patent portfolio license for the personal and non - commercial use of a consumer to ( i ) encode video through compliance with the avc standard ( ` ` avc video'' ) and / or ( ii ) decode avc video that was encoded by a consumer engaged in a personal nor non - commercial activity and / or was obtained from a television provider licensed to provide avc video.\"]\n","Label: []\n","==================================================\n","Augmented Text: [\"don't post content that contains anything that, in under armour's original determination, appears objectionable or ban any other person from officially actively modifying the site, or that directly expose under armour members our athletes to any harm or liability of some kind.\"]\n","Label: []\n","==================================================\n","Augmented Text: [\"if you do n'oh agree to the new contract, you must continue using the term, shut your microsoft account and / her registered account while, if you are a parent or guardian, help your minor child close him or her microsoft atm or skype account.\"]\n","Label: []\n","==================================================\n","Augmented Text: ['in some countries regulations are regulations on the content on use of some software, advertising and / or related websites.']\n","Label: []\n","==================================================\n","Augmented Text: ['the company has provide information to you on any further claim, offense, criminal proceeding.']\n","Label: []\n","==================================================\n","Augmented Text: ['so reserve your right for change this user agreement through time to life without notice.']\n","Label: [2]\n","==================================================\n","Augmented Text: ['see note where blizzard is entitled to and could control any such illegal activities.']\n","Label: []\n","==================================================\n","Augmented Text: ['connections to third sector services or resources']\n","Label: []\n","==================================================\n","Augmented Text: ['spotify administrators promote reasonable efforts to maintain a spotify service running.']\n","Label: []\n","==================================================\n","Augmented Text: [\"critics are not arguing over these small companies'content, business environments or monitoring systems, or for how i collect, use or preserve the information they get from clients.\"]\n","Label: []\n","==================================================\n"]}],"source":["!pip install nlpaug\n","import random\n","import nlpaug.augmenter.word as naw\n","from datasets import load_dataset\n","\n","# Initialize the augmentation object\n","aug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"substitute\")\n","dataset = load_dataset('lex_glue', 'unfair_tos')\n","\n","# Get the training data\n","train_data = dataset['train']\n","\n","# Augment the data\n","augmented_texts = []\n","augmented_labels = []\n","\n","for example in train_data:\n","    text = example['text']\n","    label = example['labels']\n","\n","    augmented_text = aug.augment(text)\n","    augmented_texts.append(augmented_text)\n","    augmented_labels.append(label)\n","\n","# Combine original and augmented data\n","combined_data = list(zip(augmented_texts, augmented_labels))\n","random.shuffle(combined_data)\n","augmented_texts, augmented_labels = zip(*combined_data)\n","\n","# Print some augmented examples\n","for text, label in zip(augmented_texts[:10], augmented_labels[:10]):\n","    print(\"Augmented Text:\", text)\n","    print(\"Label:\", label)\n","    print(\"=\" * 50)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":234},"executionInfo":{"elapsed":321,"status":"error","timestamp":1691287823656,"user":{"displayName":"sweta simran","userId":"15795161217929105177"},"user_tz":-60},"id":"XDeOpC08nBDm","outputId":"ec121975-a7cd-488f-9206-3958b5621af7"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-14d1d286ede6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmented_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugmented_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Augmented Text:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Label:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'augmented_texts' is not defined"]}],"source":["for text, label in zip(augmented_texts[:10], augmented_labels[:10]):\n","    print(\"Augmented Text:\", text)\n","    print(\"Label:\", label)\n","    print(\"=\" * 50)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":610,"referenced_widgets":["a417062dc88a4d97a41b408c626417db","1bdab01b1ba64194a83d2f8ca10e24bf","a528e54b36e240e19e843dbaeb53c94a","fa2b279f40164f1f9de2d3f7cfec4472","4e109b2bd06d445ab4794b48dedb4f6d","027c2bbdca934000980c12afe16d24e4","72629b334b1649ebbd44d997044698aa","e82cb59e81f84f1ebeb500a1cd4192fd","bc7a132b6e7e490388a999a505782b10","686a423d59c34d7789815d5be405bae9","38b40dafc599424a91b5b465d76f7f21","e0de6a433af24f2fa8995f0035feeb29","1fa3b8dd78d9437799087ab4bb1bccd4","70749376b1b4496d9b54a5e011c0b50c","279d2edac05b4dad830b87c6d0977599","7d2d97335fae456483df4b0912ade350","aecfb08bc7b7419ea420ca0846370479","10c1bb9fe3854f8892e9173216681029","6eefe08a488a4bd3993a9fa0da8c1b90","3e966e1d707741b1863c662906485518","ef4504de431c4dad8dcec37a960ccd06","8f0eaf06d9d74f1083d14d8df3f50068","225244aa22314ea283d13b2458c9bb12","8da073d7f08442d6a5f49a6e11f4abb3","9c962a45969a44df91db0fdc0ef4b518","415a42aea42140608562076acc4ac4e5","1f43ca31bae84fc99cb4e13722b5fe1f","70cfb9cdca9c4d1e94808f6beb438530","45916c76ecef4a4188dfb041da3942ec","64a7d7b5be79439698d9e9530c573e0f","5aca35719125455a89005f383d86a3ba","9bf0b071fca649c8ae734244f2bddbb7","8e6c9c8d64c44a4c9cc02d8d8a33ea6a","c7427839bf174edeba2b97302e9e0744","b57cac2a661c4ca59cf3363c70c2915d","19b03633a6fc44b0b55e053bcd0e3c65","f5a4de5788024c2d8ea62c2d0eadbfeb","79eff3c76cf54d6ab44ff5a6b8afd0dc","60cd536e9a25432e85eeea404a11202c","b63646922d23402f97f8e1c315f164c5","63e07ebaf6af41f7b8631514740421e1","5ced662924fe49d9af2af3729eb5ed1e","df7f8b8230dc4313b24f88f8bfee5222","b3311519f56e41d492a4e9ef039f2560","7fcd159bd65842c59ba86477d663bc68","36568abb55b949f2b3d1b2ee3537ed43","7a7abf4eaa884c34ab85b0e91fe37f07","5b9ca847e912494cbdba608f5523143a","32de876342ae4f71873de6e6e2f965e8","1e29619a90cd48b18ada7d5cce2bbd39","b12b0976b05145e2b7fa4c937ad7a024","e4737d5039084a46bca644028fad843e","b9dfdd8934904f2bb3fdb11522641705","3fa4a88f3f9541e591949a4d0be4d6bb","4304e4222989448c95cd142d29826394","3e317e239dae4bbe86be9c42c8619089","7124e48443ae4f67a082fb5d971c08a1","0a250f49e430498881c9f2cd0e695854","4a7742845306497daca38a3c38ef2e9e","5e4d09e8821d4754adf3e9cabd0746d6","6ac734f9711c4906b1144f63987a7582","1b06caffeff94a909dc1c2a29bb99d13","c83fc2e6ae784f738cb1ba8979054ea4","c06450233ebf4de88fd9fccda3365086","94ef3fc063f44befaca2a741c6e8bd0e","6d8db85995934c468f0e9a56d52d5873","4932192d7f984fe4b927b3b6eb51b16a","f1cb9844b7b9492fa3feb5616cdb5d6d","b8e4ff5b1bd647cdb5f3fc79d3fd211e","c3a65bed4ffa4bfc854f5054dcab5e71","f92e669990784453bf5854c8b38b13fd","7f6938181e764e739162991a7c6a10ea","90667c8574a04077b5d51e0b70e8908a","6d7ba530350c44babd55091e1172bee3","3803480e26994889b5295db64bfb54f8","a6e5c2cad02348d88005533c82c6ca92","da52333cda2e46b0906ef02729f85968"]},"executionInfo":{"elapsed":16768,"status":"error","timestamp":1691281782995,"user":{"displayName":"sweta simran","userId":"15795161217929105177"},"user_tz":-60},"id":"mqTkIPOtjW-T","outputId":"bd559883-3080-4103-c936-313e658f106d"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["/content\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a417062dc88a4d97a41b408c626417db","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/23.3k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e0de6a433af24f2fa8995f0035feeb29","version_major":2,"version_minor":0},"text/plain":["Downloading metadata:   0%|          | 0.00/21.0k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"225244aa22314ea283d13b2458c9bb12","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/32.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c7427839bf174edeba2b97302e9e0744","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/511k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7fcd159bd65842c59ba86477d663bc68","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/5532 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3e317e239dae4bbe86be9c42c8619089","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/1607 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4932192d7f984fe4b927b3b6eb51b16a","version_major":2,"version_minor":0},"text/plain":["Generating validation split:   0%|          | 0/2275 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"UnboundLocalError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-3ef354c82cde>\u001b[0m in \u001b[0;36m<cell line: 214>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-3ef354c82cde>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0my_train_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mgs_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best Parameters:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'y_train_val' referenced before assignment"]}],"source":["import nltk\n","nltk.download('stopwords')\n","import pandas\n","import torch\n","from sklearn.utils import parallel_backend\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.multiclass import OneVsRestClassifier\n","from sklearn.svm import LinearSVC\n","from sklearn import metrics\n","from sklearn.model_selection import PredefinedSplit\n","from sklearn.preprocessing import MultiLabelBinarizer\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import FunctionTransformer\n","from sklearn.pipeline import FeatureUnion, Pipeline\n","from datasets import load_dataset\n","import logging\n","import os\n","import argparse\n","import random\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.svm import SVC\n","import numpy as np\n","\n","dataset_n_classes = {'ecthr_a': 10, 'ecthr_b': 10, 'scotus': 14, 'eurlex': 100, 'ledgar': 100, 'unfair_tos': 8, 'case_hold': 5}\n","\n","def load_augmented_dataset(dataset, percentage):\n","    split = 'train'\n","    split_data = dataset[split]['text']\n","    sample_size = int(len(split_data) * percentage)\n","    sampled_data = random.sample(split_data, sample_size)\n","    return sampled_data\n","\n","def main():\n","    config = {\n","        'dataset': 'unfair_tos',\n","        'task_type': 'multi_label',\n","        'text_limit': -1\n","    }\n","    n_classes = 100\n","    cwd = os.getcwd()\n","    print(cwd)\n","\n","    if not os.path.exists(f\"logs/{config.get('dataset')}\"):\n","        if not os.path.exists(f'logs'):\n","            os.mkdir(f'logs')\n","        os.mkdir(f\"logs/{config.get('dataset')}\")\n","    handlers = [logging.FileHandler(f\"logs/{config.get('dataset')}_svm.txt\"), logging.StreamHandler()]\n","    logging.basicConfig(handlers=handlers, level=logging.INFO)\n","\n","    def get_text(dataset):\n","        if 'ecthr' in config.get('dataset'):\n","            texts = [' '.join(text) for text in dataset['text']]\n","            return [' '.join(text.split()[:config.get('text_limit')]) for text in texts]\n","        elif config.get('dataset') == 'case_hold':\n","            data = [[context] + endings for context, endings in zip(dataset['context'], dataset['endings'])]\n","            return pd.DataFrame(data=data,\n","                                columns=['context', 'option_1', 'option_2', 'option_3', 'options_4', 'option_5']\n","                                )\n","        else:\n","            return [' '.join(text.split()[:config.get('text_limit')]) for text in dataset['text']]\n","\n","    def get_labels(dataset, mlb=None):\n","        if config.get('task_type') == 'multi_class':\n","            return dataset['label']\n","        else:\n","            return mlb.transform(dataset['labels']).tolist()\n","\n","    def add_zero_class(labels):\n","        augmented_labels = np.zeros((len(labels), len(labels[0]) + 1), dtype=np.int32)\n","        augmented_labels[:, :-1] = labels\n","        augmented_labels[:, -1] = (np.sum(labels, axis=1) == 0).astype('int32')\n","        return augmented_labels\n","\n","    scores = {'micro-f1': [], 'macro-f1': []}\n","    dataset = load_dataset('lex_glue', config.get('dataset'))\n","    from sklearn.model_selection import train_test_split\n","\n","    dataset = load_dataset('lex_glue', config.get('dataset'))\n","\n","    for seed in range(1, 6):\n","        if config.get('task_type') == 'multi_label':\n","            classifier = OneVsRestClassifier(LinearSVC(random_state=seed, max_iter=50000))\n","            parameters = {\n","                'vect__max_features': [10000, 20000, 40000],\n","                'clf__estimator__C': [0.1, 1, 10],\n","                'clf__estimator__loss': ('hinge', 'squared_hinge')\n","            }\n","        elif config.get('dataset') == 'case_hold':\n","            classifier = LinearSVC(random_state=seed, max_iter=50000)\n","            parameters = {\n","                'clf__C': [0.1, 1, 10],\n","                'clf__loss': ('hinge', 'squared_hinge')\n","            }\n","        else:\n","            classifier = LinearSVC(random_state=seed, max_iter=50000)\n","            parameters = {\n","                'vect__max_features': [10000, 20000, 40000],\n","                'clf__C': [0.1, 1, 10],\n","                'clf__loss': ('hinge', 'squared_hinge')\n","            }\n","\n","        if config.get('dataset') == 'case_hold':\n","            text_clf = Pipeline([\n","                ('union', FeatureUnion([('context_tfidf',\n","                                         Pipeline([('extract_field',\n","                                                    FunctionTransformer(lambda x: x['context'], validate=False)),\n","                                                   ('vect', CountVectorizer(stop_words=stopwords.words('english'),\n","                                                                            ngram_range=(1, 3), min_df=5,\n","                                                                            max_features=40000)),\n","                                                   ('tfidf', TfidfTransformer())]))] +\n","                                       [(f'option_{idx}_tfidf',\n","                                         Pipeline([('extract_field',\n","                                                    FunctionTransformer(lambda x: x[f'option_{idx}'], validate=False)),\n","                                                   ('vect', CountVectorizer(stop_words=stopwords.words('english'),\n","                                                                            ngram_range=(1, 3), min_df=5,\n","                                                                            max_features=40000)),\n","                                                   ('tfidf', TfidfTransformer())]))\n","                                        for idx in range(1, 6)]\n","                                       )),\n","                ('clf', classifier)\n","            ])\n","        else:\n","            text_clf = Pipeline([('vect', CountVectorizer(stop_words=stopwords.words('english'),\n","                                                          ngram_range=(1, 3), min_df=5)),\n","                                 ('tfidf', TfidfTransformer()),\n","                                 ('clf', classifier),\n","                                 ])\n","\n","        split_index = [-1] * len(dataset['train']) + [0] * len(dataset['validation'])\n","        val_split = PredefinedSplit(test_fold=split_index)\n","        gs_clf = GridSearchCV(text_clf, parameters, cv=val_split, n_jobs=32, verbose=4, refit=False)\n","        x_train = get_text(dataset['train'])\n","        x_train_series = pd.Series(x_train)\n","        x_val = get_text(dataset['validation'])\n","        x_val_series = pd.Series(x_val)\n","        x_train_val = pd.concat([x_train_series, x_val_series])\n","        if config.get('task_type') == 'multi_label':\n","                mlb = MultiLabelBinarizer(classes=range(n_classes))\n","                mlb.fit(dataset['train']['labels'])\n","        else:\n","                mlb = None\n","                y_train = get_labels(dataset['train'], mlb)\n","                y_val = get_labels(dataset['validation'], mlb)\n","                y_train_val = y_train + y_val\n","\n","\n","        if config.get('dataset') == 'eurlex':\n","            svm_clf = SVC()\n","            param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf'], 'gamma': [0.1, 1, 10]}\n","            batch_size = 1000\n","            total_samples = len(x_train_val)\n","            for i in range(0, total_samples, batch_size):\n","                x_batch = x_train_val[i:i+batch_size]\n","                y_batch = y_train_val[i:i+batch_size]\n","                gs_clf = GridSearchCV(svm_clf, param_grid, cv=5)\n","                gs_clf.fit(x_batch, y_batch)\n","            best_estimator = gs_clf.best_estimator_\n","        else:\n","            x_train = get_text(dataset['train'])\n","            x_train_series = pd.Series(x_train)\n","            x_val = get_text(dataset['validation'])\n","            x_val_series = pd.Series(x_val)\n","            x_train_val = pd.concat([x_train_series, x_val_series])\n","            if config.get('task_type') == 'multi_label':\n","                mlb = MultiLabelBinarizer(classes=range(n_classes))\n","                mlb.fit(dataset['train']['labels'])\n","            else:\n","                mlb = None\n","                y_train = get_labels(dataset['train'], mlb)\n","                y_val = get_labels(dataset['validation'], mlb)\n","                y_train_val = y_train + y_val\n","\n","            gs_clf = gs_clf.fit(x_train_val, y_train_val)\n","\n","        print('Best Parameters:')\n","        for param_name in sorted(parameters.keys()):\n","            print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n","\n","        text_clf.set_params(**gs_clf.best_params_)\n","        gs_clf = text_clf.fit(x_train, y_train)\n","\n","        print(config.get('dataset'))\n","        print('VALIDATION RESULTS:')\n","        y_pred = gs_clf.predict(get_text(dataset['validation']))\n","        y_true = get_labels(dataset[\"validation\"], mlb)\n","        if config.get('task_type') == 'multi_label' and config.get('dataset') != 'eurlex':\n","            y_true = add_zero_class(y_true)\n","            y_pred = add_zero_class(y_pred)\n","        print(f'Accuracy: {metrics.accuracy_score(y_true, y_pred):.1%}')\n","        print(f'Micro-F1: {metrics.f1_score(y_true, y_pred, average=\"micro\") * 100:.1f}')\n","        print(f'Macro-F1: {metrics.f1_score(y_true, y_pred, average=\"macro\") * 100:.1f}')\n","\n","        print('TEST RESULTS:')\n","        y_pred = gs_clf.predict(get_text(dataset['test']))\n","        y_true = get_labels(dataset[\"test\"], mlb)\n","        if config.get('task_type') == 'multi_label' and config.get('dataset') != 'eurlex':\n","            y_true = add_zero_class(y_true)\n","            y_pred = add_zero_class(y_pred)\n","        print(f'Accuracy: {metrics.accuracy_score(y_true, y_pred):.1%}')\n","        print(f'Micro-F1: {metrics.f1_score(y_true, y_pred, average=\"micro\") * 100:.1f}')\n","        print(f'Macro-F1: {metrics.f1_score(y_true, y_pred, average=\"macro\") * 100:.1f}')\n","        scores['micro-f1'].append(metrics.f1_score(y_true, y_pred, average=\"micro\"))\n","        scores['macro-f1'].append(metrics.f1_score(y_true, y_pred, average=\"macro\"))\n","\n","    print('-' * 100)\n","    print(\n","        f'Micro-F1: {np.mean(scores[\"micro-f1\"]) * 100:.1f} +/- {np.std(scores[\"micro-f1\"]) * 100:.1f}\\t'\n","        f'Macro-F1: {np.mean(scores[\"macro-f1\"]) * 100:.1f} +/- {np.std(scores[\"macro-f1\"]) * 100:.1f}\\t'\n","    )\n","\n","\n","if __name__ == '__main__':\n","    main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":232,"status":"ok","timestamp":1689192377762,"user":{"displayName":"sweta simran","userId":"15795161217929105177"},"user_tz":-60},"id":"dYPZuOBN-0RH","outputId":"029c7bf2-9d67-4597-e8af-a1a78b09e3ef"},"outputs":[{"name":"stdout","output_type":"stream","text":["['.config', 'logs', 'sample_data']\n","['case_hold_svm.txt', 'case_hold']\n","\n"]}],"source":["import os\n","contents = os.listdir()\n","print(contents)\n","import os\n","\n","# Get the current working directory\n","current_dir = os.getcwd()\n","\n","# Define the path to the logs directory\n","logs_dir = os.path.join(current_dir, 'logs')\n","\n","# Check if the logs directory exists\n","if os.path.exists(logs_dir):\n","    # Get the list of files and directories inside the logs directory\n","    contents = os.listdir(logs_dir)\n","\n","    # Display the contents\n","    print(contents)\n","else:\n","    print(\"Logs directory does not exist.\")\n","\n","\n","    # Define the path to the case_hold_svm.txt file\n","file_path = os.path.join(logs_dir, 'case_hold_svm.txt')\n","\n","# Check if the file exists\n","if os.path.isfile(file_path):\n","    # Open the file and read its contents\n","    with open(file_path, 'r') as file:\n","        file_contents = file.read()\n","\n","    # Print the contents to the console\n","    print(file_contents)\n","else:\n","    print(\"case_hold_svm.txt does not exist.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":233,"referenced_widgets":["832e584ad396492aa9a460fc3df96eba","62bfea4c265f4a9e9001da9e9d5c606d","6e4ca91a9503420b92a404efd4adf208","02fe8a9a95d449d788b7d5a122897fe0","c12b2bc45104431ab0a3ba09550456f3"]},"executionInfo":{"elapsed":40119,"status":"ok","timestamp":1691289706208,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"},"user_tz":-60},"id":"NnEGoiOoPozx","outputId":"1cae2c5c-6da3-4f7c-8189-a24f5e132b7b"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"832e584ad396492aa9a460fc3df96eba","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"62bfea4c265f4a9e9001da9e9d5c606d","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6e4ca91a9503420b92a404efd4adf208","version_major":2,"version_minor":0},"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"02fe8a9a95d449d788b7d5a122897fe0","version_major":2,"version_minor":0},"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c12b2bc45104431ab0a3ba09550456f3","version_major":2,"version_minor":0},"text/plain":["Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from dataclasses import dataclass\n","from typing import Optional, Tuple\n","\n","import torch\n","import numpy as np\n","from torch import nn\n","from transformers.file_utils import ModelOutput\n","\n","\n","@dataclass\n","class SimpleOutput(ModelOutput):\n","    last_hidden_state: torch.FloatTensor = None\n","    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n","    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n","    attentions: Optional[Tuple[torch.FloatTensor]] = None\n","    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n","\n","\n","def sinusoidal_init(num_embeddings: int, embedding_dim: int):\n","    # keep dim 0 for padding token position encoding zero vector\n","    position_enc = np.array([\n","        [pos / np.power(10000, 2 * i / embedding_dim) for i in range(embedding_dim)]\n","        if pos != 0 else np.zeros(embedding_dim) for pos in range(num_embeddings)])\n","\n","    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2])  # dim 2i\n","    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2])  # dim 2i+1\n","    return torch.from_numpy(position_enc).type(torch.FloatTensor)\n","\n","\n","class HierarchicalBert(nn.Module):\n","\n","    def __init__(self, encoder, max_segments=64, max_segment_length=128):\n","        super(HierarchicalBert, self).__init__()\n","        supported_models = ['bert', 'roberta', 'deberta']\n","        assert encoder.config.model_type in supported_models  # other model types are not supported so far\n","        # Pre-trained segment (token-wise) encoder, e.g., BERT\n","        self.encoder = encoder\n","        # Specs for the segment-wise encoder\n","        self.hidden_size = encoder.config.hidden_size\n","        self.max_segments = max_segments\n","        self.max_segment_length = max_segment_length\n","        # Init sinusoidal positional embeddings\n","        self.seg_pos_embeddings = nn.Embedding(max_segments + 1, encoder.config.hidden_size,\n","                                               padding_idx=0,\n","                                               _weight=sinusoidal_init(max_segments + 1, encoder.config.hidden_size))\n","        # Init segment-wise transformer-based encoder\n","        self.seg_encoder = nn.Transformer(d_model=encoder.config.hidden_size,\n","                                          nhead=encoder.config.num_attention_heads,\n","                                          batch_first=True, dim_feedforward=encoder.config.intermediate_size,\n","                                          activation=encoder.config.hidden_act,\n","                                          dropout=encoder.config.hidden_dropout_prob,\n","                                          layer_norm_eps=encoder.config.layer_norm_eps,\n","                                          num_encoder_layers=2, num_decoder_layers=0).encoder\n","\n","    def forward(self,\n","                input_ids=None,\n","                attention_mask=None,\n","                token_type_ids=None,\n","                position_ids=None,\n","                head_mask=None,\n","                inputs_embeds=None,\n","                labels=None,\n","                output_attentions=None,\n","                output_hidden_states=None,\n","                return_dict=None,\n","                ):\n","        # Hypothetical Example\n","        # Batch of 4 documents: (batch_size, n_segments, max_segment_length) --> (4, 64, 128)\n","        # BERT-BASE encoder: 768 hidden units\n","\n","        # Squash samples and segments into a single axis (batch_size * n_segments, max_segment_length) --> (256, 128)\n","        input_ids_reshape = input_ids.contiguous().view(-1, input_ids.size(-1))\n","        attention_mask_reshape = attention_mask.contiguous().view(-1, attention_mask.size(-1))\n","        if token_type_ids is not None:\n","            token_type_ids_reshape = token_type_ids.contiguous().view(-1, token_type_ids.size(-1))\n","        else:\n","            token_type_ids_reshape = None\n","\n","        # Encode segments with BERT --> (256, 128, 768)\n","        encoder_outputs = self.encoder(input_ids=input_ids_reshape,\n","                                       attention_mask=attention_mask_reshape,\n","                                       token_type_ids=token_type_ids_reshape)[0]\n","\n","        # Reshape back to (batch_size, n_segments, max_segment_length, output_size) --> (4, 64, 128, 768)\n","        encoder_outputs = encoder_outputs.contiguous().view(input_ids.size(0), self.max_segments,\n","                                                            self.max_segment_length,\n","                                                            self.hidden_size)\n","\n","        # Gather CLS outputs per segment --> (4, 64, 768)\n","        encoder_outputs = encoder_outputs[:, :, 0]\n","\n","        # Infer real segments, i.e., mask paddings\n","        seg_mask = (torch.sum(input_ids, 2) != 0).to(input_ids.dtype)\n","        # Infer and collect segment positional embeddings\n","        seg_positions = torch.arange(1, self.max_segments + 1).to(input_ids.device) * seg_mask\n","        # Add segment positional embeddings to segment inputs\n","        encoder_outputs += self.seg_pos_embeddings(seg_positions)\n","\n","        # Encode segments with segment-wise transformer\n","        seg_encoder_outputs = self.seg_encoder(encoder_outputs)\n","\n","        # Collect document representation\n","        outputs, _ = torch.max(seg_encoder_outputs, 1)\n","\n","        return SimpleOutput(last_hidden_state=outputs, hidden_states=outputs)\n","\n","\n","if __name__ == \"__main__\":\n","    from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n","    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","\n","    # Use as a stand-alone encoder\n","    bert = AutoModel.from_pretrained('bert-base-uncased')\n","    model = HierarchicalBert(encoder=bert, max_segments=64, max_segment_length=128)\n","\n","    fake_inputs = {'input_ids': [], 'attention_mask': [], 'token_type_ids': []}\n","    for i in range(4):\n","        # Tokenize segment\n","        temp_inputs = tokenizer(['dog ' * 126] * 64)\n","        fake_inputs['input_ids'].append(temp_inputs['input_ids'])\n","        fake_inputs['attention_mask'].append(temp_inputs['attention_mask'])\n","        fake_inputs['token_type_ids'].append(temp_inputs['token_type_ids'])\n","\n","    fake_inputs['input_ids'] = torch.as_tensor(fake_inputs['input_ids'])\n","    fake_inputs['attention_mask'] = torch.as_tensor(fake_inputs['attention_mask'])\n","    fake_inputs['token_type_ids'] = torch.as_tensor(fake_inputs['token_type_ids'])\n","\n","    output = model(fake_inputs['input_ids'], fake_inputs['attention_mask'], fake_inputs['token_type_ids'])\n","\n","    # 4 document representations of 768 features are expected\n","    assert output[0].shape == torch.Size([4, 768])\n","\n","    # Use with HuggingFace AutoModelForSequenceClassification and Trainer API\n","\n","    # Init Classifier\n","    model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=10)\n","    # Replace flat BERT encoder with hierarchical BERT encoder\n","    model.bert = HierarchicalBert(encoder=model.bert, max_segments=64, max_segment_length=128)\n","    output = model(fake_inputs['input_ids'], fake_inputs['attention_mask'], fake_inputs['token_type_ids'])\n","\n","    # 4 document outputs with 10 (num_labels) logits are expected\n","    assert output.logits.shape == torch.Size([4, 10])\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UwfLuG75E1-t"},"source":["# New section"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CK_d3Nu_EKY_"},"outputs":[],"source":["import torch\n","from torch import nn\n","from transformers import DebertaPreTrainedModel, DebertaModel\n","from transformers.modeling_outputs import SequenceClassifierOutput, MultipleChoiceModelOutput\n","from transformers.activations import ACT2FN\n","\n","\n","class ContextPooler(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.dense = nn.Linear(config.pooler_hidden_size, config.pooler_hidden_size)\n","        self.dropout = StableDropout(config.pooler_dropout)\n","        self.config = config\n","\n","    def forward(self, hidden_states):\n","        # We \"pool\" the model by simply taking the hidden state corresponding\n","        # to the first token.\n","\n","        context_token = hidden_states[:, 0]\n","        context_token = self.dropout(context_token)\n","        pooled_output = self.dense(context_token)\n","        pooled_output = ACT2FN[self.config.pooler_hidden_act](pooled_output)\n","        return pooled_output\n","\n","    @property\n","    def output_dim(self):\n","        return self.config.hidden_size\n","\n","\n","class DropoutContext(object):\n","    def __init__(self):\n","        self.dropout = 0\n","        self.mask = None\n","        self.scale = 1\n","        self.reuse_mask = True\n","\n","\n","def get_mask(input, local_context):\n","    if not isinstance(local_context, DropoutContext):\n","        dropout = local_context\n","        mask = None\n","    else:\n","        dropout = local_context.dropout\n","        dropout *= local_context.scale\n","        mask = local_context.mask if local_context.reuse_mask else None\n","\n","    if dropout > 0 and mask is None:\n","        mask = (1 - torch.empty_like(input).bernoulli_(1 - dropout)).bool()\n","\n","    if isinstance(local_context, DropoutContext):\n","        if local_context.mask is None:\n","            local_context.mask = mask\n","\n","    return mask, dropout\n","\n","\n","class XDropout(torch.autograd.Function):\n","    \"\"\"Optimized dropout function to save computation and memory by using mask operation instead of multiplication.\"\"\"\n","\n","    @staticmethod\n","    def forward(ctx, input, local_ctx):\n","        mask, dropout = get_mask(input, local_ctx)\n","        ctx.scale = 1.0 / (1 - dropout)\n","        if dropout > 0:\n","            ctx.save_for_backward(mask)\n","            return input.masked_fill(mask, 0) * ctx.scale\n","        else:\n","            return input\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        if ctx.scale > 1:\n","            (mask,) = ctx.saved_tensors\n","            return grad_output.masked_fill(mask, 0) * ctx.scale, None\n","        else:\n","            return grad_output, None\n","\n","\n","class StableDropout(nn.Module):\n","    \"\"\"\n","    Optimized dropout module for stabilizing the training\n","\n","    Args:\n","        drop_prob (float): the dropout probabilities\n","    \"\"\"\n","\n","    def __init__(self, drop_prob):\n","        super().__init__()\n","        self.drop_prob = drop_prob\n","        self.count = 0\n","        self.context_stack = None\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Call the module\n","\n","        Args:\n","            x (:obj:`torch.tensor`): The input tensor to apply dropout\n","        \"\"\"\n","        if self.training and self.drop_prob > 0:\n","            return XDropout.apply(x, self.get_context())\n","        return x\n","\n","    def clear_context(self):\n","        self.count = 0\n","        self.context_stack = None\n","\n","    def init_context(self, reuse_mask=True, scale=1):\n","        if self.context_stack is None:\n","            self.context_stack = []\n","        self.count = 0\n","        for c in self.context_stack:\n","            c.reuse_mask = reuse_mask\n","            c.scale = scale\n","\n","    def get_context(self):\n","        if self.context_stack is not None:\n","            if self.count >= len(self.context_stack):\n","                self.context_stack.append(DropoutContext())\n","            ctx = self.context_stack[self.count]\n","            ctx.dropout = self.drop_prob\n","            self.count += 1\n","            return ctx\n","        else:\n","            return self.drop_prob\n","\n","\n","class DebertaForSequenceClassification(DebertaPreTrainedModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","\n","        num_labels = getattr(config, \"num_labels\", 2)\n","        self.num_labels = num_labels\n","\n","        self.deberta = DebertaModel(config)\n","\n","        self.classifier = nn.Linear(config.hidden_size, num_labels)\n","        drop_out = getattr(config, \"cls_dropout\", None)\n","        drop_out = self.config.hidden_dropout_prob if drop_out is None else drop_out\n","        self.dropout = nn.Dropout(drop_out)\n","\n","        self.init_weights()\n","\n","    def get_input_embeddings(self):\n","        return self.deberta.get_input_embeddings()\n","\n","    def set_input_embeddings(self, new_embeddings):\n","        self.deberta.set_input_embeddings(new_embeddings)\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","        r\"\"\"\n","        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n","            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n","            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n","            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.deberta(\n","            input_ids,\n","            token_type_ids=token_type_ids,\n","            attention_mask=attention_mask,\n","            position_ids=position_ids,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        pooled_output = self.dropout(outputs[1])\n","        logits = self.classifier(pooled_output)\n","\n","        loss = None\n","        if labels is not None:\n","            if self.num_labels == 1:\n","                # regression task\n","                loss_fn = nn.MSELoss()\n","                logits = logits.view(-1).to(labels.dtype)\n","                loss = loss_fn(logits, labels.view(-1))\n","            elif labels.dim() == 1 or labels.size(-1) == 1:\n","                label_index = (labels >= 0).nonzero()\n","                labels = labels.long()\n","                if label_index.size(0) > 0:\n","                    labeled_logits = torch.gather(logits, 0, label_index.expand(label_index.size(0), logits.size(1)))\n","                    labels = torch.gather(labels, 0, label_index.view(-1))\n","                    loss_fct = nn.CrossEntropyLoss()\n","                    loss = loss_fct(labeled_logits.view(-1, self.num_labels).float(), labels.view(-1))\n","                else:\n","                    loss = torch.tensor(0).to(logits)\n","            else:\n","                log_softmax = nn.LogSoftmax(-1)\n","                loss = -((log_softmax(logits) * labels).sum(-1)).mean()\n","        if not return_dict:\n","            output = (logits,) + outputs[1:]\n","            return ((loss,) + output) if loss is not None else output\n","        else:\n","            return SequenceClassifierOutput(\n","                loss=loss,\n","                logits=logits,\n","                hidden_states=outputs.hidden_states,\n","                attentions=outputs.attentions,\n","            )\n","\n","\n","class DebertaForMultipleChoice(DebertaPreTrainedModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","\n","        self.deberta = DebertaModel(config)\n","        self.pooler = ContextPooler(config)\n","        output_dim = self.pooler.output_dim\n","        drop_out = getattr(config, \"cls_dropout\", None)\n","        drop_out = self.config.hidden_dropout_prob if drop_out is None else drop_out\n","        self.dropout = StableDropout(drop_out)\n","        self.classifier = nn.Linear(output_dim, 1)\n","\n","        self.init_weights()\n","\n","    def forward(\n","            self,\n","            input_ids=None,\n","            attention_mask=None,\n","            token_type_ids=None,\n","            position_ids=None,\n","            inputs_embeds=None,\n","            labels=None,\n","            output_attentions=None,\n","            output_hidden_states=None,\n","            return_dict=None,\n","    ):\n","        r\"\"\"\n","        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n","            Labels for computing the multiple choice classification loss. Indices should be in ``[0, ...,\n","            num_choices-1]`` where :obj:`num_choices` is the size of the second dimension of the input tensors. (See\n","            :obj:`input_ids` above)\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n","\n","        input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n","        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n","        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n","        position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n","        inputs_embeds = (\n","            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n","            if inputs_embeds is not None\n","            else None\n","        )\n","\n","        outputs = self.deberta(\n","            input_ids,\n","            token_type_ids=token_type_ids,\n","            attention_mask=attention_mask,\n","            position_ids=position_ids,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        encoder_layer = outputs[0]\n","        pooled_output = self.pooler(encoder_layer)\n","\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","        reshaped_logits = logits.view(-1, num_choices)\n","\n","        loss = None\n","        if labels is not None:\n","            loss_fct = nn.CrossEntropyLoss()\n","            loss = loss_fct(reshaped_logits, labels)\n","\n","        if not return_dict:\n","            output = (reshaped_logits,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return MultipleChoiceModelOutput(\n","            loss=loss,\n","            logits=reshaped_logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A_yGVjk8SyRy"},"outputs":[],"source":["!pip install transformers[torch] accelerate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":242},"executionInfo":{"elapsed":720,"status":"error","timestamp":1690293876155,"user":{"displayName":"sweta simran","userId":"15795161217929105177"},"user_tz":-60},"id":"sNTGc_6-XZZZ","outputId":"e4d84166-17f2-4daa-8417-3e9a1ef41bdf"},"outputs":[{"ename":"FileNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-bea564035939>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mall_acts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"current-acts.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mactlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'current-acts.txt'"]}],"source":["from datasets import load_dataset\n","dataset_dict = load_dataset(\"lex_glue\", runtime_args('scotus','bert'))\n","#print(dataset)\n","#Divide into train,dev,test\n","\n","from sklearn.model_selection import train_test_split\n","\n","#data_list = list(dataset_dict.items())\n","\n","train_set_dict = dataset_dict['train'].data\n","test_set_dict = dataset_dict['test'].data\n","validation_set_dict = dataset_dict['validation'].data\n","print(train_set_dict[0])"]},{"cell_type":"markdown","metadata":{"id":"DDbII0MLUh7C"},"source":[]},{"cell_type":"markdown","metadata":{"id":"_f-jPsud4fPg"},"source":["# New section"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":574,"referenced_widgets":["b43474552fbd45c98ffa444b052e7a84","2610a6e4ab1b48fa85ad3fa76fcf7f99","d0d7b32a4d184a7abb1107b2e1f6afda","589cf887c9f247e5a953f617a03a9b0c","bf72b0307f9941cc8a74b99f2408e740","9935f7c01e1449b698e39ae775250ace","0f78340249be49328ea94f7c22cd95ab","ac370f028c814f38a26f2a9dfb939104","13edfeef3f8e4f09ae25e07efe3e7f29","0908082874724b6681d8af1a10f4aa57","32230d53ae9a41d78084f537e97fbc87"]},"executionInfo":{"elapsed":23781,"status":"error","timestamp":1690132817936,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"},"user_tz":-60},"id":"yk79bF6wGXPJ","outputId":"44035f8c-3b2f-497b-e26e-0da342654494"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n"]},{"name":"stdout","output_type":"stream","text":["40683995648\n","40683995648\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:datasets.builder:Found cached dataset lex_glue (/root/.cache/huggingface/datasets/lex_glue/scotus/1.0.0/8a66420941bf6e77a7ddd4da4d3bfb7ba88ef48c1d55302a568ac650a095ca3a)\n","WARNING:datasets.builder:Found cached dataset lex_glue (/root/.cache/huggingface/datasets/lex_glue/scotus/1.0.0/8a66420941bf6e77a7ddd4da4d3bfb7ba88ef48c1d55302a568ac650a095ca3a)\n","WARNING:datasets.builder:Found cached dataset lex_glue (/root/.cache/huggingface/datasets/lex_glue/scotus/1.0.0/8a66420941bf6e77a7ddd4da4d3bfb7ba88ef48c1d55302a568ac650a095ca3a)\n","[WARNING|modeling_utils.py:3331] 2023-07-23 17:19:59,229 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/lex_glue/scotus/1.0.0/8a66420941bf6e77a7ddd4da4d3bfb7ba88ef48c1d55302a568ac650a095ca3a/cache-a13d480e11c88252.arrow\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/lex_glue/scotus/1.0.0/8a66420941bf6e77a7ddd4da4d3bfb7ba88ef48c1d55302a568ac650a095ca3a/cache-4cd48fe3b0d30b7d.arrow\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b43474552fbd45c98ffa444b052e7a84","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on prediction dataset:   0%|          | 0/1400 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"OutOfMemoryError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-02bf43b6080b>\u001b[0m in \u001b[0;36m<cell line: 507>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0mmetric_for_best_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"macro-f1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m     )\n\u001b[0;32m--> 576\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-7-02bf43b6080b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(training_args)\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlast_checkpoint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         max_train_samples = (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m         )\n\u001b[0;32m-> 1539\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1540\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1809\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2654\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2677\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2678\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2679\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2680\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-cea87a31f5a5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         outputs = self.deberta(\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-1ad808171449>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# Encode segments with BERT --> (256, 128, 768)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         encoder_outputs = self.encoder(input_ids=input_ids_reshape,\n\u001b[0m\u001b[1;32m     81\u001b[0m                                        \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask_reshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                                        token_type_ids=token_type_ids_reshape)[0]\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    981\u001b[0m         )\n\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    984\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    474\u001b[0m                 )\n\u001b[1;32m    475\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m                 hidden_states = layer_module(\n\u001b[0m\u001b[1;32m    477\u001b[0m                     \u001b[0mnext_kv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     ):\n\u001b[0;32m--> 383\u001b[0;31m         attention_output = self.attention(\n\u001b[0m\u001b[1;32m    384\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     ):\n\u001b[0;32m--> 316\u001b[0;31m         self_output = self.self(\n\u001b[0m\u001b[1;32m    317\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0mquery_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_layer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_bias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m         \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_layer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_bias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[0mrel_att\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 39.56 GiB total capacity; 37.89 GiB already allocated; 8.56 MiB free; 37.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["# coding=utf-8\n","\"\"\" Finetuning models on SCOTUS (e.g. Bert, RoBERTa, LEGAL-BERT).\"\"\"\n","\n","import logging\n","import os\n","import random\n","import re\n","import sys\n","from dataclasses import dataclass, field\n","from typing import Optional\n","\n","import datasets\n","from datasets import load_dataset\n","from sklearn.metrics import f1_score\n","\n","import numpy as np\n","from torch import nn\n","import glob\n","import shutil\n","import torch\n","import transformers\n","from transformers import (\n","    Trainer,\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    DataCollatorWithPadding,\n","    EvalPrediction,\n","    TrainingArguments,\n","    default_data_collator,\n","    set_seed,\n","    EarlyStoppingCallback,\n",")\n","from transformers.trainer_utils import get_last_checkpoint\n","from transformers.utils import check_min_version\n","from transformers.utils.versions import require_version\n","#from models.deberta import DebertaForSequenceClassification\n","\n","# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n","check_min_version(\"4.9.0\")\n","\n","require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/text-classification/requirements.txt\")\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","#desired_max_split_size_mb = 100\n","\n","from transformers import AutoModel, AutoTokenizer\n","\n","# First, load the tokenizer and pre-trained BERT model\n","tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","bert_model = AutoModel.from_pretrained('bert-base-uncased')\n","\n","# Then, create an instance of HierarchicalBert\n","max_segments = 64\n","max_segment_length = 128\n","HierarchicalBertObj = HierarchicalBert(encoder=bert_model, max_segments=max_segments, max_segment_length=max_segment_length)\n","# Set the environment variable PYTORCH_CUDA_ALLOC_CONF with the desired value\n","#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb=256,512,1024\"\n","torch.cuda.empty_cache()\n","print(torch.cuda.memory_allocated())  # Memory allocated on GPU 0\n","print(torch.cuda.max_memory_allocated())  # Peak memory allocated on GPU 0\n","#max_split_size_mb = torch.cuda.memory._get_max_memory_allocated() / (1024.0 * 1024.0)\n","\n","#print(\"max_split_size_mb:\", max_split_size_mb)\n","\n","\n","@dataclass\n","class DataTrainingArguments:\n","    \"\"\"\n","    Arguments pertaining to what data we are going to input our model for training and eval.\n","\n","    Using `HfArgumentParser` we can turn this class\n","    into argparse arguments to be able to specify them on\n","    the command line.\n","    \"\"\"\n","\n","    max_seq_length: Optional[int] = field(\n","        default=128,\n","        metadata={\n","            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n","                    \"than this will be truncated, sequences shorter will be padded.\"\n","        },\n","    )\n","    max_segments: Optional[int] = field(\n","        default=64,\n","        metadata={\n","            \"help\": \"The maximum number of segments (paragraphs) to be considered. Sequences longer \"\n","                    \"than this will be truncated, sequences shorter will be padded.\"\n","        },\n","    )\n","    max_seg_length: Optional[int] = field(\n","        default=128,\n","        metadata={\n","            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n","                    \"than this will be truncated, sequences shorter will be padded.\"\n","        },\n","    )\n","    overwrite_cache: bool = field(\n","        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n","    )\n","    pad_to_max_length: bool = field(\n","        default=True,\n","        metadata={\n","            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n","            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n","        },\n","    )\n","    max_train_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_eval_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_predict_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    server_ip: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n","    server_port: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n","\n","\n","@dataclass\n","class ModelArguments:\n","    \"\"\"\n","    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n","    \"\"\"\n","\n","    model_name_or_path: str = field(\n","        default=None, metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n","    )\n","    hierarchical: bool = field(\n","        default=True, metadata={\"help\": \"Whether to use a hierarchical variant or not\"}\n","    )\n","    config_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n","    )\n","    tokenizer_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n","    )\n","    cache_dir: Optional[str] = field(\n","        default=None,\n","        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n","    )\n","    do_lower_case: Optional[bool] = field(\n","        default=True,\n","        metadata={\"help\": \"arg to indicate if tokenizer should do lower case in AutoTokenizer.from_pretrained()\"},\n","    )\n","    use_fast_tokenizer: bool = field(\n","        default=True,\n","        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n","    )\n","    model_revision: str = field(\n","        default=\"main\",\n","        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n","    )\n","    use_auth_token: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n","            \"with private models).\"\n","        },\n","    )\n","\n","\n","def main(training_args):\n","    # Set default values for arguments\n","    model_args = ModelArguments(\n","        model_name_or_path=\"microsoft/deberta-base\",\n","        hierarchical=True,\n","        do_lower_case=True,\n","        use_fast_tokenizer=True,\n","    )\n","    data_args = DataTrainingArguments(\n","        max_seq_length=128,\n","        max_segments=64,\n","        max_seg_length=128,\n","        overwrite_cache=False,\n","        pad_to_max_length=True,\n","    )\n","\n","\n","    # Fix boolean parameter\n","    if model_args.do_lower_case == 'False' or not model_args.do_lower_case:\n","        model_args.do_lower_case = False\n","    else:\n","        model_args.do_lower_case = True\n","\n","    if model_args.hierarchical == 'False' or not model_args.hierarchical:\n","        model_args.hierarchical = False\n","    else:\n","        model_args.hierarchical = True\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        handlers=[logging.StreamHandler(sys.stdout)],\n","    )\n","\n","    log_level = training_args.get_process_log_level()\n","    logger.setLevel(log_level)\n","    datasets.utils.logging.set_verbosity(log_level)\n","    transformers.utils.logging.set_verbosity(log_level)\n","    transformers.utils.logging.enable_default_handler()\n","    transformers.utils.logging.enable_explicit_format()\n","\n","    # Log on each process the small summary:\n","    logger.warning(\n","        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n","        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n","    )\n","    logger.info(f\"Training/evaluation parameters {training_args}\")\n","\n","    # Detecting last checkpoint.\n","    last_checkpoint = None\n","    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n","        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n","        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n","            raise ValueError(\n","                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n","                \"Use --overwrite_output_dir to overcome.\"\n","            )\n","        elif last_checkpoint is not None:\n","            logger.info(\n","                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n","                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n","            )\n","\n","    # Set seed before initializing model.\n","    set_seed(training_args.seed)\n","\n","    # In distributed training, the load_dataset function guarantees that only one local process can concurrently\n","    # download the dataset.\n","    # Downloading and loading eurlex dataset from the hub.\n","    if training_args.do_train:\n","        train_dataset = load_dataset(\"lex_glue\", \"scotus\", split=\"train\", cache_dir=model_args.cache_dir)\n","\n","    if training_args.do_eval:\n","        eval_dataset = load_dataset(\"lex_glue\", \"scotus\", split=\"validation\", cache_dir=model_args.cache_dir)\n","\n","    if training_args.do_predict:\n","        predict_dataset = load_dataset(\"lex_glue\", \"scotus\", split=\"test\", cache_dir=model_args.cache_dir)\n","\n","    # Labels\n","    label_list = list(range(14))\n","    num_labels = len(label_list)\n","\n","    # Load pretrained model and tokenizer\n","    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n","    # download model & vocab.\n","    config = AutoConfig.from_pretrained(\n","        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n","        num_labels=num_labels,\n","        finetuning_task=\"scotus\",\n","        cache_dir=model_args.cache_dir,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n","        do_lower_case=model_args.do_lower_case,\n","        cache_dir=model_args.cache_dir,\n","        use_fast=model_args.use_fast_tokenizer,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","    if config.model_type == 'deberta' and model_args.hierarchical:\n","        model = DebertaForSequenceClassification.from_pretrained(\n","            model_args.model_name_or_path,\n","            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","            config=config,\n","            cache_dir=model_args.cache_dir,\n","            revision=model_args.model_revision,\n","            use_auth_token=True if model_args.use_auth_token else None,\n","        )\n","    else:\n","        model = AutoModelForSequenceClassification.from_pretrained(\n","            model_args.model_name_or_path,\n","            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","            config=config,\n","            cache_dir=model_args.cache_dir,\n","            revision=model_args.model_revision,\n","            use_auth_token=True if model_args.use_auth_token else None,\n","        )\n","    if model_args.hierarchical:\n","        # Hack the classifier encoder to use hierarchical BERT\n","        if config.model_type in ['bert', 'deberta']:\n","            if config.model_type == 'bert':\n","                segment_encoder = model.bert\n","            else:\n","                segment_encoder = model.deberta\n","            model_encoder = HierarchicalBert(encoder=segment_encoder,\n","                                             max_segments=data_args.max_segments,\n","                                             max_segment_length=data_args.max_seg_length)\n","            if config.model_type == 'bert':\n","                model.bert = model_encoder\n","            elif config.model_type == 'deberta':\n","                model.deberta = model_encoder\n","            else:\n","                raise NotImplementedError(f\"{config.model_type} is no supported yet!\")\n","        elif config.model_type == 'roberta':\n","            model_encoder = HierarchicalBert(encoder=model.roberta, max_segments=data_args.max_segments,\n","                                             max_segment_length=data_args.max_seg_length)\n","            model.roberta = model_encoder\n","            # Build a new classification layer, as well\n","            dense = nn.Linear(config.hidden_size, config.hidden_size)\n","            dense.load_state_dict(model.classifier.dense.state_dict())  # load weights\n","            dropout = nn.Dropout(config.hidden_dropout_prob).to(model.device)\n","            out_proj = nn.Linear(config.hidden_size, config.num_labels).to(model.device)\n","            out_proj.load_state_dict(model.classifier.out_proj.state_dict())  # load weights\n","            model.classifier = nn.Sequential(dense, dropout, out_proj).to(model.device)\n","        elif config.model_type in ['longformer', 'big_bird']:\n","            pass\n","        else:\n","            raise NotImplementedError(f\"{config.model_type} is no supported yet!\")\n","\n","    # Preprocessing the datasets\n","    # Padding strategy\n","    if data_args.pad_to_max_length:\n","        padding = \"max_length\"\n","    else:\n","        # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n","        padding = False\n","\n","    def preprocess_function(examples):\n","        # Tokenize the texts\n","        if model_args.hierarchical:\n","            case_template = [[0] * data_args.max_seq_length]\n","            if config.model_type == 'roberta':\n","                batch = {'input_ids': [], 'attention_mask': []}\n","                for doc in examples['text']:\n","                    doc = re.split('\\n{2,}', doc)\n","                    doc_encodings = tokenizer(doc[:data_args.max_segments], padding=padding,\n","                                              max_length=data_args.max_seg_length, truncation=True)\n","                    batch['input_ids'].append(doc_encodings['input_ids'] + case_template * (\n","                            data_args.max_segments - len(doc_encodings['input_ids'])))\n","                    batch['attention_mask'].append(doc_encodings['attention_mask'] + case_template * (\n","                            data_args.max_segments - len(doc_encodings['attention_mask'])))\n","            else:\n","                batch = {'input_ids': [], 'attention_mask': [], 'token_type_ids': []}\n","                for doc in examples['text']:\n","                    doc = re.split('\\n{2,}', doc)\n","                    doc_encodings = tokenizer(doc[:data_args.max_segments], padding=padding,\n","                                              max_length=data_args.max_seg_length, truncation=True)\n","                    batch['input_ids'].append(doc_encodings['input_ids'] + case_template * (\n","                                data_args.max_segments - len(doc_encodings['input_ids'])))\n","                    batch['attention_mask'].append(doc_encodings['attention_mask'] + case_template * (\n","                                data_args.max_segments - len(doc_encodings['attention_mask'])))\n","                    batch['token_type_ids'].append(doc_encodings['token_type_ids'] + case_template * (\n","                                data_args.max_segments - len(doc_encodings['token_type_ids'])))\n","        elif config.model_type in ['longformer', 'big_bird']:\n","            cases = []\n","            max_position_embeddings = config.max_position_embeddings - 2 if config.model_type == 'longformer' \\\n","                else config.max_position_embeddings\n","            for doc in examples['text']:\n","                doc = re.split('\\n{2,}', doc)\n","                cases.append(f' {tokenizer.sep_token} '.join([' '.join(paragraph.split()[:data_args.max_seg_length])\n","                                                              for paragraph in doc[:data_args.max_segments]]))\n","            batch = tokenizer(cases, padding=padding, max_length=max_position_embeddings, truncation=True)\n","            if config.model_type == 'longformer':\n","                global_attention_mask = np.zeros((len(cases), max_position_embeddings), dtype=np.int32)\n","                # global attention on cls token\n","                global_attention_mask[:, 0] = 1\n","                batch['global_attention_mask'] = list(global_attention_mask)\n","        else:\n","            batch = tokenizer(examples['text'], padding=padding, max_length=512, truncation=True)\n","\n","        batch[\"label\"] = [label_list.index(labels) for labels in examples[\"label\"]]\n","\n","        return batch\n","\n","    if training_args.do_train:\n","        if data_args.max_train_samples is not None:\n","            train_dataset = train_dataset.select(range(data_args.max_train_samples))\n","        with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n","            train_dataset = train_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on train dataset\",\n","            )\n","        # Log a few random samples from the training set:\n","        for index in random.sample(range(len(train_dataset)), 3):\n","            logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n","\n","    if training_args.do_eval:\n","        if data_args.max_eval_samples is not None:\n","            eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))\n","        with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n","            eval_dataset = eval_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on validation dataset\",\n","            )\n","\n","    if training_args.do_predict:\n","        if data_args.max_predict_samples is not None:\n","            predict_dataset = predict_dataset.select(range(data_args.max_predict_samples))\n","        with training_args.main_process_first(desc=\"prediction dataset map pre-processing\"):\n","            predict_dataset = predict_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on prediction dataset\",\n","            )\n","\n","    # You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n","    # predictions and label_ids field) and has to return a dictionary string to float.\n","    def compute_metrics(p: EvalPrediction):\n","        logits = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","        preds = np.argmax(logits, axis=1)\n","        macro_f1 = f1_score(y_true=p.label_ids, y_pred=preds, average='macro', zero_division=0)\n","        micro_f1 = f1_score(y_true=p.label_ids, y_pred=preds, average='micro', zero_division=0)\n","        return {'macro-f1': macro_f1, 'micro-f1': micro_f1}\n","\n","    # Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n","    if data_args.pad_to_max_length:\n","        data_collator = default_data_collator\n","    elif training_args.fp16:\n","        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n","    else:\n","        data_collator = None\n","\n","    # Initialize our Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset if training_args.do_train else None,\n","        eval_dataset=eval_dataset if training_args.do_eval else None,\n","        compute_metrics=compute_metrics,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n","    )\n","\n","    # Training\n","    if training_args.do_train:\n","        checkpoint = None\n","        if training_args.resume_from_checkpoint is not None:\n","            checkpoint = training_args.resume_from_checkpoint\n","        elif last_checkpoint is not None:\n","            checkpoint = last_checkpoint\n","        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n","        metrics = train_result.metrics\n","        max_train_samples = (\n","            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n","        )\n","        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n","\n","        trainer.save_model()  # Saves the tokenizer too for easy upload\n","\n","        trainer.log_metrics(\"train\", metrics)\n","        trainer.save_metrics(\"train\", metrics)\n","        trainer.save_state()\n","\n","    # Evaluation\n","    if training_args.do_eval:\n","        logger.info(\"*** Evaluate ***\")\n","        metrics = trainer.evaluate(eval_dataset=eval_dataset)\n","\n","        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n","        metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n","\n","        trainer.log_metrics(\"eval\", metrics)\n","        trainer.save_metrics(\"eval\", metrics)\n","\n","    # Prediction\n","    if training_args.do_predict:\n","        logger.info(\"*** Predict ***\")\n","        predictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix=\"predict\")\n","\n","        max_predict_samples = (\n","            data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)\n","        )\n","        metrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n","\n","        trainer.log_metrics(\"predict\", metrics)\n","        trainer.save_metrics(\"predict\", metrics)\n","\n","        output_predict_file = os.path.join(training_args.output_dir, \"test_predictions.csv\")\n","        if trainer.is_world_process_zero():\n","            with open(output_predict_file, \"w\") as writer:\n","                for index, pred_list in enumerate(predictions[0]):\n","                    pred_line = '\\t'.join([f'{pred:.5f}' for pred in pred_list])\n","                    writer.write(f\"{index}\\t{pred_line}\\n\")\n","\n","    # Clean up checkpoints\n","    checkpoints = [filepath for filepath in glob.glob(f'{training_args.output_dir}/*/') if '/checkpoint' in filepath]\n","    for checkpoint in checkpoints:\n","        shutil.rmtree(checkpoint)\n","\n","\n","if __name__ == \"__main__\":\n","    #main()\n","\n","#For training\n","\n","    training_args = TrainingArguments(\n","        do_train = True,\n","        do_eval = False,\n","        output_dir=os.getcwd(),\n","        overwrite_output_dir=True,\n","        num_train_epochs=1,\n","        per_device_train_batch_size=4,\n","        per_device_eval_batch_size=4,\n","        save_steps=500,\n","        save_total_limit=2,\n","        fp16=False,\n","        logging_dir=\"./logs\",\n","        logging_steps=100,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=500,\n","        logging_first_step=False,\n","        load_best_model_at_end = True,\n","        metric_for_best_model=\"macro-f1\",\n","    )\n","    #main(training_args)\n","\n","# For Validation\n","    training_args = TrainingArguments(\n","        do_train = False,\n","        do_eval = True,\n","        output_dir=os.getcwd(),\n","        overwrite_output_dir=True,\n","        num_train_epochs=1,\n","        per_device_train_batch_size=4,\n","        per_device_eval_batch_size=4,\n","        save_steps=500,\n","        save_total_limit=2,\n","        fp16=False,\n","        logging_dir=\"./logs\",\n","        logging_steps=100,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=500,\n","        logging_first_step=False,\n","        load_best_model_at_end = True,\n","        metric_for_best_model=\"macro-f1\",\n","    )\n","    #main(training_args)\n","\n","    # For Evaluation\n","    training_args = TrainingArguments(\n","        do_train = True,\n","        do_eval = True,\n","        do_predict = True,\n","        output_dir=os.getcwd(),\n","        overwrite_output_dir=True,\n","        num_train_epochs=10,\n","        per_device_train_batch_size=4,\n","        per_device_eval_batch_size=4,\n","        save_steps=500,\n","        save_total_limit=2,\n","        fp16=False,\n","        logging_dir=\"./logs\",\n","        logging_steps=100,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=500,\n","        logging_first_step=False,\n","        load_best_model_at_end = True,\n","        metric_for_best_model=\"macro-f1\",\n","    )\n","    main(training_args)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["6618ed16ad594465b8bc0e8461aa2b17","27ec412a7c5846f9818845ec565debca","75ea624f3eca40d4a67c22159d91192b","8e82d08aa8524a0a943f74476dc90761","d7c118d5c20a43d2bfa9365cfb942647","a1ed7c5603864952857fa2ba61f1e98d","b68ca0f1bee542468e76cf56a376f917","d91104ff98b644faa25c542998ac220c","c8143640340c48ecb4ce76161f7c2fbe","5fd322e7eae04b8087925ea8a3a8e1f7","fc3b1e2e37e5406fb64e4f8de1131c5e","c4e0f9bf6f4a45d088ff320e445b2ccf","1c4da1e6fa8c46f0b3810d6fd3694e46","197e5648712d497a90efa1f03cfab4a9","beeaacfd14b4462e8233a75ada298c11"]},"executionInfo":{"elapsed":6539310,"status":"ok","timestamp":1691296247288,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"},"user_tz":-60},"id":"Oya_-Oy1j_Z5","outputId":"df768312-2e7e-4a2c-9e46-8f78d25d8af3"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6618ed16ad594465b8bc0e8461aa2b17","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/23.3k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"27ec412a7c5846f9818845ec565debca","version_major":2,"version_minor":0},"text/plain":["Downloading metadata:   0%|          | 0.00/21.0k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"75ea624f3eca40d4a67c22159d91192b","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/32.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8e82d08aa8524a0a943f74476dc90761","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/32.9M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d7c118d5c20a43d2bfa9365cfb942647","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/9000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a1ed7c5603864952857fa2ba61f1e98d","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/1000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b68ca0f1bee542468e76cf56a376f917","version_major":2,"version_minor":0},"text/plain":["Generating validation split:   0%|          | 0/1000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d91104ff98b644faa25c542998ac220c","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c8143640340c48ecb4ce76161f7c2fbe","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5fd322e7eae04b8087925ea8a3a8e1f7","version_major":2,"version_minor":0},"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/222k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fc3b1e2e37e5406fb64e4f8de1131c5e","version_major":2,"version_minor":0},"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c4e0f9bf6f4a45d088ff320e445b2ccf","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[WARNING|modeling_utils.py:3331] 2023-08-07 04:26:30,218 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1c4da1e6fa8c46f0b3810d6fd3694e46","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on train dataset:   0%|          | 0/9000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"197e5648712d497a90efa1f03cfab4a9","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on validation dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"beeaacfd14b4462e8233a75ada298c11","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on prediction dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='687' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 687/2250 15:06 < 34:27, 0.76 it/s, Epoch 0.30/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro-f1</th>\n","      <th>Micro-f1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.240500</td>\n","      <td>0.270479</td>\n","      <td>0.135584</td>\n","      <td>0.313827</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["#!/usr/bin/env python\n","# coding=utf-8\n","\"\"\" Finetuning models on the ECtHR dataset (e.g. Bert, RoBERTa, LEGAL-BERT).\"\"\"\n","\n","import logging\n","import os\n","import random\n","import sys\n","from dataclasses import dataclass, field\n","from typing import Optional\n","\n","import datasets\n","import numpy as np\n","from datasets import load_dataset\n","from sklearn.metrics import f1_score\n","#from trainer import MultilabelTrainer\n","from scipy.special import expit\n","from torch import nn\n","import glob\n","import shutil\n","import torch\n","torch.cuda.empty_cache()\n","import transformers\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    DataCollatorWithPadding,\n","    EvalPrediction,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    default_data_collator,\n","    set_seed,\n","    EarlyStoppingCallback,\n",")\n","from transformers.trainer_utils import get_last_checkpoint\n","from transformers.utils import check_min_version\n","from transformers.utils.versions import require_version\n","#from models.hierbert import HierarchicalBert\n","#from models.deberta import DebertaForSequenceClassification\n","\n","\n","# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n","check_min_version(\"4.9.0\")\n","\n","require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/text-classification/requirements.txt\")\n","\n","logger = logging.getLogger(__name__)\n","\n","from transformers import AutoModel, AutoTokenizer\n","\n","# First, load the tokenizer and pre-trained BERT model\n","tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","bert_model = AutoModel.from_pretrained('bert-base-uncased')\n","\n","# Then, create an instance of HierarchicalBert\n","max_segments = 64\n","max_segment_length = 128\n","HierarchicalBertObj = HierarchicalBert(encoder=bert_model, max_segments=max_segments, max_segment_length=max_segment_length)\n","#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb=256,512,1024\"\n","\n","@dataclass\n","class DataTrainingArguments:\n","    \"\"\"\n","    Arguments pertaining to what data we are going to input our model for training and eval.\n","\n","    Using `HfArgumentParser` we can turn this class\n","    into argparse arguments to be able to specify them on\n","    the command line.\n","    \"\"\"\n","\n","    max_seq_length: Optional[int] = field(\n","        default=4096,\n","        metadata={\n","            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n","            \"than this will be truncated, sequences shorter will be padded.\"\n","        },\n","    )\n","    max_segments: Optional[int] = field(\n","        default=64,\n","        metadata={\n","            \"help\": \"The maximum number of segments (paragraphs) to be considered. Sequences longer \"\n","                    \"than this will be truncated, sequences shorter will be padded.\"\n","        },\n","    )\n","    max_seg_length: Optional[int] = field(\n","        default=128,\n","        metadata={\n","            \"help\": \"The maximum segment (paragraph) length to be considered. Segments longer \"\n","                    \"than this will be truncated, sequences shorter will be padded.\"\n","        },\n","    )\n","    overwrite_cache: bool = field(\n","        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n","    )\n","    pad_to_max_length: bool = field(\n","        default=True,\n","        metadata={\n","            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n","            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n","        },\n","    )\n","    max_train_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_eval_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_predict_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    task: Optional[str] = field(\n","        default='ecthr_b',\n","        metadata={\n","            \"help\": \"Define downstream task\"\n","        },\n","    )\n","    server_ip: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n","    server_port: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n","\n","\n","@dataclass\n","class ModelArguments:\n","    \"\"\"\n","    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n","    \"\"\"\n","\n","    model_name_or_path: str = field(\n","        default=None, metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n","    )\n","    hierarchical: bool = field(\n","        default=True, metadata={\"help\": \"Whether to use a hierarchical variant or not\"}\n","    )\n","    config_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n","    )\n","    tokenizer_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n","    )\n","    cache_dir: Optional[str] = field(\n","        default=None,\n","        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n","    )\n","    do_lower_case: Optional[bool] = field(\n","        default=True,\n","        metadata={\"help\": \"arg to indicate if tokenizer should do lower case in AutoTokenizer.from_pretrained()\"},\n","    )\n","    use_fast_tokenizer: bool = field(\n","        default=True,\n","        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n","    )\n","    model_revision: str = field(\n","        default=\"main\",\n","        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n","    )\n","    use_auth_token: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n","            \"with private models).\"\n","        },\n","    )\n","\n","\n","def main(training_args):\n","    # See all possible arguments in src/transformers/training_args.py\n","    # or by passing the --help flag to this script.\n","    # We now keep distinct sets of args, for a cleaner separation of concerns.\n","\n","    model_args = ModelArguments(\n","        model_name_or_path=\"nlpaueb/legal-bert-base-uncased\",\n","        hierarchical=True,\n","        do_lower_case=True,\n","        use_fast_tokenizer=True,\n","    )\n","    data_args = DataTrainingArguments(\n","        max_seq_length=128,\n","        max_segments=64,\n","        max_seg_length=128,\n","        overwrite_cache=False,\n","        pad_to_max_length=True,\n","    )\n","\n","\n","    # Fix boolean parameter\n","    if model_args.do_lower_case == 'False' or not model_args.do_lower_case:\n","        model_args.do_lower_case = False\n","    else:\n","        model_args.do_lower_case = True\n","\n","    if model_args.hierarchical == 'False' or not model_args.hierarchical:\n","        model_args.hierarchical = False\n","    else:\n","        model_args.hierarchical = True\n","\n","    # Setup distant debugging if needed\n","    if data_args.server_ip and data_args.server_port:\n","        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n","        import ptvsd\n","\n","        print(\"Waiting for debugger attach\")\n","        ptvsd.enable_attach(address=(data_args.server_ip, data_args.server_port), redirect_output=True)\n","        ptvsd.wait_for_attach()\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        handlers=[logging.StreamHandler(sys.stdout)],\n","    )\n","\n","    log_level = training_args.get_process_log_level()\n","    logger.setLevel(log_level)\n","    datasets.utils.logging.set_verbosity(log_level)\n","    transformers.utils.logging.set_verbosity(log_level)\n","    transformers.utils.logging.enable_default_handler()\n","    transformers.utils.logging.enable_explicit_format()\n","\n","    # Log on each process the small summary:\n","    logger.warning(\n","        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n","        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n","    )\n","    logger.info(f\"Training/evaluation parameters {training_args}\")\n","\n","    # Detecting last checkpoint.\n","    last_checkpoint = None\n","    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n","        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n","        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n","            raise ValueError(\n","                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n","                \"Use --overwrite_output_dir to overcome.\"\n","            )\n","        elif last_checkpoint is not None:\n","            logger.info(\n","                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n","                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n","            )\n","\n","    # Set seed before initializing model.\n","    set_seed(training_args.seed)\n","\n","    # In distributed training, the load_dataset function guarantees that only one local process can concurrently\n","    # download the dataset.\n","    # Downloading and loading eurlex dataset from the hub.\n","    if training_args.do_train:\n","        train_dataset = load_dataset(\"lex_glue\", name=data_args.task, split=\"train\", data_dir='data', cache_dir=model_args.cache_dir)\n","\n","    if training_args.do_eval:\n","        eval_dataset = load_dataset(\"lex_glue\", name=data_args.task, split=\"validation\", data_dir='data', cache_dir=model_args.cache_dir)\n","\n","    if training_args.do_predict:\n","        predict_dataset = load_dataset(\"lex_glue\", name=data_args.task, split=\"test\", data_dir='data', cache_dir=model_args.cache_dir)\n","\n","    # Labels\n","    label_list = list(range(10))\n","    num_labels = len(label_list)\n","\n","    # Load pretrained model and tokenizer\n","    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n","    # download model & vocab.\n","    config = AutoConfig.from_pretrained(\n","        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n","        num_labels=num_labels,\n","        finetuning_task=f\"{data_args.task}\",\n","        cache_dir=model_args.cache_dir,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n","        do_lower_case=model_args.do_lower_case,\n","        cache_dir=model_args.cache_dir,\n","        use_fast=model_args.use_fast_tokenizer,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","    if config.model_type == 'deberta' and model_args.hierarchical:\n","        model = DebertaForSequenceClassification.from_pretrained(\n","            model_args.model_name_or_path,\n","            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","            config=config,\n","            cache_dir=model_args.cache_dir,\n","            revision=model_args.model_revision,\n","            use_auth_token=True if model_args.use_auth_token else None,\n","        )\n","    else:\n","        model = AutoModelForSequenceClassification.from_pretrained(\n","            model_args.model_name_or_path,\n","            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","            config=config,\n","            cache_dir=model_args.cache_dir,\n","            revision=model_args.model_revision,\n","            use_auth_token=True if model_args.use_auth_token else None,\n","        )\n","\n","    if model_args.hierarchical:\n","        # Hack the classifier encoder to use hierarchical BERT\n","        if config.model_type in ['bert', 'deberta']:\n","            if config.model_type == 'bert':\n","                segment_encoder = model.bert\n","            else:\n","                segment_encoder = model.deberta\n","            model_encoder = HierarchicalBert(encoder=segment_encoder,\n","                                             max_segments=data_args.max_segments,\n","                                             max_segment_length=data_args.max_seg_length)\n","            if config.model_type == 'bert':\n","                model.bert = model_encoder\n","            elif config.model_type == 'deberta':\n","                model.deberta = model_encoder\n","            else:\n","                raise NotImplementedError(f\"{config.model_type} is no supported yet!\")\n","        elif config.model_type == 'roberta':\n","            model_encoder = HierarchicalBert(encoder=model.roberta, max_segments=data_args.max_segments,\n","                                             max_segment_length=data_args.max_seg_length)\n","            model.roberta = model_encoder\n","            # Build a new classification layer, as well\n","            dense = nn.Linear(config.hidden_size, config.hidden_size)\n","            dense.load_state_dict(model.classifier.dense.state_dict())  # load weights\n","            dropout = nn.Dropout(config.hidden_dropout_prob).to(model.device)\n","            out_proj = nn.Linear(config.hidden_size, config.num_labels).to(model.device)\n","            out_proj.load_state_dict(model.classifier.out_proj.state_dict())  # load weights\n","            model.classifier = nn.Sequential(dense, dropout, out_proj).to(model.device)\n","        elif config.model_type in ['longformer', 'big_bird']:\n","            pass\n","        else:\n","            raise NotImplementedError(f\"{config.model_type} is no supported yet!\")\n","\n","    # Preprocessing the datasets\n","    # Padding strategy\n","    if data_args.pad_to_max_length:\n","        padding = \"max_length\"\n","    else:\n","        # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n","        padding = False\n","\n","    def preprocess_function(examples):\n","        # Tokenize the texts\n","        if model_args.hierarchical:\n","            case_template = [[0] * data_args.max_seg_length]\n","            if config.model_type == 'roberta':\n","                batch = {'input_ids': [], 'attention_mask': []}\n","                for case in examples['text']:\n","                    case_encodings = tokenizer(case[:data_args.max_segments], padding=padding,\n","                                               max_length=data_args.max_seg_length, truncation=True)\n","                    batch['input_ids'].append(case_encodings['input_ids'] + case_template * (\n","                                data_args.max_segments - len(case_encodings['input_ids'])))\n","                    batch['attention_mask'].append(case_encodings['attention_mask'] + case_template * (\n","                                data_args.max_segments - len(case_encodings['attention_mask'])))\n","            else:\n","                batch = {'input_ids': [], 'attention_mask': [], 'token_type_ids': []}\n","                for case in examples['text']:\n","                    case_encodings = tokenizer(case[:data_args.max_segments], padding=padding,\n","                                               max_length=data_args.max_seg_length, truncation=True)\n","                    batch['input_ids'].append(case_encodings['input_ids'] + case_template * (\n","                            data_args.max_segments - len(case_encodings['input_ids'])))\n","                    batch['attention_mask'].append(case_encodings['attention_mask'] + case_template * (\n","                            data_args.max_segments - len(case_encodings['attention_mask'])))\n","                    batch['token_type_ids'].append(case_encodings['token_type_ids'] + case_template * (\n","                            data_args.max_segments - len(case_encodings['token_type_ids'])))\n","        elif config.model_type in ['longformer', 'big_bird']:\n","            cases = []\n","            max_position_embeddings = config.max_position_embeddings - 2 if config.model_type == 'longformer' \\\n","                else config.max_position_embeddings\n","            for case in examples['text']:\n","                cases.append(f' {tokenizer.sep_token} '.join(\n","                    [' '.join(fact.split()[:data_args.max_seg_length]) for fact in case[:data_args.max_segments]]))\n","            batch = tokenizer(cases, padding=padding, max_length=max_position_embeddings, truncation=True)\n","            if config.model_type == 'longformer':\n","                global_attention_mask = np.zeros((len(cases), max_position_embeddings), dtype=np.int32)\n","                # global attention on cls token\n","                global_attention_mask[:, 0] = 1\n","                batch['global_attention_mask'] = list(global_attention_mask)\n","        else:\n","            cases = []\n","            for case in examples['text']:\n","                cases.append(f'\\n'.join(case))\n","            batch = tokenizer(cases, padding=padding, max_length=512, truncation=True)\n","\n","        batch[\"labels\"] = [[1 if label in labels else 0 for label in label_list] for labels in examples[\"labels\"]]\n","\n","        return batch\n","\n","    if training_args.do_train:\n","        if data_args.max_train_samples is not None:\n","            train_dataset = train_dataset.select(range(data_args.max_train_samples))\n","        with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n","            train_dataset = train_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on train dataset\",\n","            )\n","        # Log a few random samples from the training set:\n","        for index in random.sample(range(len(train_dataset)), 3):\n","            logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n","\n","    if training_args.do_eval:\n","        if data_args.max_eval_samples is not None:\n","            eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))\n","        with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n","            eval_dataset = eval_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on validation dataset\",\n","            )\n","\n","    if training_args.do_predict:\n","        if data_args.max_predict_samples is not None:\n","            predict_dataset = predict_dataset.select(range(data_args.max_predict_samples))\n","        with training_args.main_process_first(desc=\"prediction dataset map pre-processing\"):\n","            predict_dataset = predict_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on prediction dataset\",\n","            )\n","\n","    # You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n","    # predictions and label_ids field) and has to return a dictionary string to float.\n","    def compute_metrics(p: EvalPrediction):\n","        # Fix gold labels\n","        y_true = np.zeros((p.label_ids.shape[0], p.label_ids.shape[1] + 1), dtype=np.int32)\n","        y_true[:, :-1] = p.label_ids\n","        y_true[:, -1] = (np.sum(p.label_ids, axis=1) == 0).astype('int32')\n","        # Fix predictions\n","        logits = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","        preds = (expit(logits) > 0.5).astype('int32')\n","        y_pred = np.zeros((p.label_ids.shape[0], p.label_ids.shape[1] + 1), dtype=np.int32)\n","        y_pred[:, :-1] = preds\n","        y_pred[:, -1] = (np.sum(preds, axis=1) == 0).astype('int32')\n","        # Compute scores\n","        macro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='macro', zero_division=0)\n","        micro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='micro', zero_division=0)\n","        return {'macro-f1': macro_f1, 'micro-f1': micro_f1}\n","\n","    # Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n","    if data_args.pad_to_max_length:\n","        data_collator = default_data_collator\n","    elif training_args.fp16:\n","        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n","    else:\n","        data_collator = None\n","\n","    # Initialize our Trainer\n","    trainer = MultilabelTrainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset if training_args.do_train else None,\n","        eval_dataset=eval_dataset if training_args.do_eval else None,\n","        compute_metrics=compute_metrics,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n","    )\n","\n","    # Training\n","    if training_args.do_train:\n","        checkpoint = None\n","        if training_args.resume_from_checkpoint is not None:\n","            checkpoint = training_args.resume_from_checkpoint\n","        elif last_checkpoint is not None:\n","            checkpoint = last_checkpoint\n","        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n","        metrics = train_result.metrics\n","        max_train_samples = (\n","            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n","        )\n","        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n","\n","        trainer.save_model()  # Saves the tokenizer too for easy upload\n","\n","        trainer.log_metrics(\"train\", metrics)\n","        trainer.save_metrics(\"train\", metrics)\n","        trainer.save_state()\n","\n","    # Evaluation\n","    if training_args.do_eval:\n","        logger.info(\"*** Evaluate ***\")\n","        metrics = trainer.evaluate(eval_dataset=eval_dataset)\n","\n","        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n","        metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n","\n","        trainer.log_metrics(\"eval\", metrics)\n","        trainer.save_metrics(\"eval\", metrics)\n","\n","    # Prediction\n","    if training_args.do_predict:\n","        logger.info(\"*** Predict ***\")\n","        predictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix=\"predict\")\n","\n","        max_predict_samples = (\n","            data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)\n","        )\n","        metrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n","\n","        trainer.log_metrics(\"predict\", metrics)\n","        trainer.save_metrics(\"predict\", metrics)\n","\n","        output_predict_file = os.path.join(training_args.output_dir, \"test_predictions.csv\")\n","        if trainer.is_world_process_zero():\n","            with open(output_predict_file, \"w\") as writer:\n","                for index, pred_list in enumerate(predictions[0]):\n","                    pred_line = '\\t'.join([f'{pred:.5f}' for pred in pred_list])\n","                    writer.write(f\"{index}\\t{pred_line}\\n\")\n","\n","    # Clean up checkpoints\n","    checkpoints = [filepath for filepath in glob.glob(f'{training_args.output_dir}/*/') if '/checkpoint' in filepath]\n","    for checkpoint in checkpoints:\n","        shutil.rmtree(checkpoint)\n","\n","\n","if __name__ == \"__main__\":\n","    #For training\n","\n","    training_args = TrainingArguments(\n","        do_train = True,\n","        output_dir=os.getcwd(),\n","        overwrite_output_dir=True,\n","        num_train_epochs=1,\n","        per_device_train_batch_size=8,\n","        save_steps=500,\n","        save_total_limit=2,\n","        fp16=False,\n","        logging_dir=\"./logs\",\n","        logging_steps=100,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=500,\n","        logging_first_step=False,\n","        load_best_model_at_end = True,\n","        metric_for_best_model=\"macro-f1\",\n","    )\n","    #main(training_args)\n","\n","# For Validation\n","    training_args = TrainingArguments(\n","        do_train = False,\n","        do_eval = True,\n","        output_dir=os.getcwd(),\n","        overwrite_output_dir=True,\n","        num_train_epochs=1,\n","        per_device_train_batch_size=8,\n","        save_steps=500,\n","        save_total_limit=2,\n","        fp16=False,\n","        logging_dir=\"./logs\",\n","        logging_steps=100,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=500,\n","        logging_first_step=False,\n","        load_best_model_at_end = True,\n","        metric_for_best_model=\"macro-f1\",\n","    )\n","    #main(training_args)\n","\n","    # For Evaluation\n","    training_args = TrainingArguments(\n","        do_train = True,\n","        do_eval = True,\n","        do_predict = True,\n","        output_dir=os.getcwd(),\n","        overwrite_output_dir=True,\n","        num_train_epochs=2,\n","        per_device_train_batch_size=4,\n","        per_device_eval_batch_size=4,\n","        save_steps=500,\n","        save_total_limit=2,\n","        fp16=False,\n","        logging_dir=\"./logs\",\n","        logging_steps=100,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=500,\n","        logging_first_step=False,\n","        load_best_model_at_end = True,\n","        metric_for_best_model=\"micro-f1\",\n","    )\n","    main(training_args)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":94499,"status":"ok","timestamp":1691287617920,"user":{"displayName":"sweta simran","userId":"15795161217929105177"},"user_tz":-60},"id":"Y8h6id2VWwVh","outputId":"3e14b856-554f-4f0a-f2ac-9d42d4b34d6a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.99\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Collecting transformers\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.22.4)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n","Collecting datasets\n","  Downloading datasets-2.14.3-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.1/519.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, dill, multiprocess, datasets\n","Successfully installed datasets-2.14.3 dill-0.3.7 multiprocess-0.70.15 xxhash-3.3.0\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.10.1)\n","Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.22.4)\n","Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.31.0)\n","Collecting accelerate\n","  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.3.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.65.0)\n","Requirement already satisfied: torch!=1.12.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (16.0.6)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.21.0\n","\u001b[31mERROR: Could not find a version that satisfies the requirement transformers-cli (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for transformers-cli\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["\n","!pip install sentencepiece\n","! pip install torch\n","! pip install transformers\n","! pip install scikit-learn\n","! pip install tqdm\n","! pip install numpy\n","! pip install datasets\n","! pip install nltk\n","import nltk\n","nltk.download('stopwords')\n","! pip install scipy\n","! pip install transformers[torch] accelerate\n","#! pip install transformers-cli\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bdnQAIs3nI7Q","outputId":"12a45f9a-3b4f-470c-a8a8-c9fb65c3fa90"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: nlpaug in /usr/local/lib/python3.10/dist-packages (1.1.11)\n","Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.22.4)\n","Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.5.3)\n","Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.27.1)\n","Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (4.6.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.12.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (1.16.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.65.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.11.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2022.7.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2023.7.22)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.4)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.4.1)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n"]}],"source":["!pip install nlpaug\n","import random\n","import nlpaug.augmenter.word as naw\n","from datasets import load_dataset\n","\n","# Initialize the augmentation object\n","aug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"substitute\")\n","dataset = load_dataset('lex_glue', 'unfair_tos')\n","\n","# Get the training data\n","train_data = dataset['train']\n","\n","# Define batch size\n","batch_size = 32\n","\n","# Augment the data\n","augmented_texts = []\n","augmented_labels = []\n","\n","for i in range(0, len(train_data), batch_size):\n","    batch = train_data[i:i+batch_size]\n","    batch_texts = batch['text']  # Access the 'text' key of each batch element\n","    batch_labels = batch['labels']  # Access the 'labels' key of each batch element\n","\n","    augmented_batch = aug.augment(batch_texts)\n","    augmented_texts.extend(augmented_batch)\n","    augmented_labels.extend(batch_labels)\n","\n","# Combine original and augmented data\n","combined_data = list(zip(augmented_texts, augmented_labels))\n","random.shuffle(combined_data)\n","augmented_texts, augmented_labels = zip(*combined_data)\n","\n","# Print some augmented examples\n","for text, label in zip(augmented_texts[:10], augmented_labels[:10]):\n","    print(\"Augmented Text:\", text)\n","    print(\"Label:\", label)\n","    print(\"=\" * 50)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["4c64063c79174c7d9c4c6add7d641ec3","b6b7be8c86e04cad97e037990f5372d8","da93f3aaa34b427a8fd9e73dd85ab047","c772bdb98003478cae852ed85ca61887","b4c49dd9aa4f45da9badcf1f807f4ca4","c5f64a6498714d4987944334ef8e80b1","c021b3af3e384c109ace73099cf06332","dd8952e4fd87410f85a9470f0279f568","bae6efb7df4e488bbf7471c2f4838dc0","94c9e97efd6c4f628dbf73b443e1e2c9","2c38d4790dd649ec8827203406ab4f2b"]},"executionInfo":{"elapsed":126568,"status":"error","timestamp":1691296773440,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"},"user_tz":-60},"id":"e5jfn3XXorDj","outputId":"ee0ce934-45ff-440b-df72-6f58e4918b87"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n","[WARNING|modeling_utils.py:3331] 2023-08-06 04:37:38,398 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4c64063c79174c7d9c4c6add7d641ec3","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on validation dataset:   0%|          | 0/2275 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1384' max='1384' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1384/1384 01:44, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro-f1</th>\n","      <th>Micro-f1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.076400</td>\n","      <td>0.070431</td>\n","      <td>0.105195</td>\n","      <td>0.895163</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.067400</td>\n","      <td>0.054441</td>\n","      <td>0.105195</td>\n","      <td>0.895163</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["***** train metrics *****\n","  epoch                    =        2.0\n","  total_flos               =   677820GF\n","  train_loss               =     0.0779\n","  train_runtime            = 0:01:44.13\n","  train_samples            =       5532\n","  train_samples_per_second =    106.249\n","  train_steps_per_second   =     13.291\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["***** eval metrics *****\n","  epoch                   =        2.0\n","  eval_loss               =     0.0704\n","  eval_macro-f1           =     0.1052\n","  eval_micro-f1           =     0.8952\n","  eval_runtime            = 0:00:05.10\n","  eval_samples            =       2275\n","  eval_samples_per_second =    445.227\n","  eval_steps_per_second   =     55.776\n","***** predict metrics *****\n","  predict_loss               =     0.0734\n","  predict_macro-f1           =     0.1048\n","  predict_micro-f1           =     0.8891\n","  predict_runtime            = 0:00:03.91\n","  predict_samples            =       1607\n","  predict_samples_per_second =    410.974\n","  predict_steps_per_second   =     51.404\n"]},{"ename":"TypeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-c5c7ed9ba68f>\u001b[0m in \u001b[0;36m<cell line: 417>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mmetric_for_best_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"macro-f1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     )\n\u001b[0;32m--> 439\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-6-c5c7ed9ba68f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(training_args)\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_predict_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_list\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                     \u001b[0mpred_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\t'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'{pred:.5f}'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m                     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{index}\\t{pred_line}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"]}],"source":["#!/usr/bin/env python\n","# coding=utf-8\n","\"\"\" Finetuning models on UNFAIR-ToC (e.g. Bert, RoBERTa, LEGAL-BERT).\"\"\"\n","\n","import logging\n","import os\n","import random\n","import sys\n","from dataclasses import dataclass, field\n","from typing import Optional\n","\n","import datasets\n","from datasets import load_dataset\n","from sklearn.metrics import f1_score\n","#from trainer import MultilabelTrainer\n","from scipy.special import expit\n","import glob\n","import shutil\n","import numpy as np\n","\n","import transformers\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    DataCollatorWithPadding,\n","    EvalPrediction,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    default_data_collator,\n","    set_seed,\n","    EarlyStoppingCallback,\n",")\n","from transformers.trainer_utils import get_last_checkpoint\n","from transformers.utils import check_min_version\n","from transformers.utils.versions import require_version\n","\n","\n","# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n","check_min_version(\"4.9.0\")\n","\n","require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/text-classification/requirements.txt\")\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","@dataclass\n","class DataTrainingArguments:\n","    \"\"\"\n","    Arguments pertaining to what data we are going to input our model for training and eval.\n","\n","    Using `HfArgumentParser` we can turn this class\n","    into argparse arguments to be able to specify them on\n","    the command line.\n","    \"\"\"\n","\n","    max_seq_length: Optional[int] = field(\n","        default=128,\n","        metadata={\n","            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n","            \"than this will be truncated, sequences shorter will be padded.\"\n","        },\n","    )\n","    overwrite_cache: bool = field(\n","        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n","    )\n","    pad_to_max_length: bool = field(\n","        default=True,\n","        metadata={\n","            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n","            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n","        },\n","    )\n","    max_train_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_eval_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_predict_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    server_ip: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n","    server_port: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n","\n","\n","@dataclass\n","class ModelArguments:\n","    \"\"\"\n","    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n","    \"\"\"\n","\n","    model_name_or_path: str = field(\n","        default=None, metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n","    )\n","    config_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n","    )\n","    tokenizer_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n","    )\n","    cache_dir: Optional[str] = field(\n","        default=None,\n","        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n","    )\n","    do_lower_case: Optional[bool] = field(\n","        default=True,\n","        metadata={\"help\": \"arg to indicate if tokenizer should do lower case in AutoTokenizer.from_pretrained()\"},\n","    )\n","    use_fast_tokenizer: bool = field(\n","        default=True,\n","        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n","    )\n","    model_revision: str = field(\n","        default=\"main\",\n","        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n","    )\n","    use_auth_token: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n","            \"with private models).\"\n","        },\n","    )\n","\n","\n","def main(training_args):\n","    # See all possible arguments in src/transformers/training_args.py\n","    # or by passing the --help flag to this script.\n","    # We now keep distinct sets of args, for a cleaner separation of concerns.\n","\n","    model_args = ModelArguments(\n","        model_name_or_path=\"nlpaueb/legal-bert-base-uncased\",\n","        #hierarchical=True,\n","        do_lower_case=True,\n","        use_fast_tokenizer=True,\n","    )\n","    data_args = DataTrainingArguments(\n","        max_seq_length=128,\n","        #max_segments=64,\n","        #max_seg_length=128,\n","        overwrite_cache=False,\n","        pad_to_max_length=True,\n","    )\n","\n","\n","    # Setup distant debugging if needed\n","    if data_args.server_ip and data_args.server_port:\n","        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n","        import ptvsd\n","\n","        print(\"Waiting for debugger attach\")\n","        ptvsd.enable_attach(address=(data_args.server_ip, data_args.server_port), redirect_output=True)\n","        ptvsd.wait_for_attach()\n","\n","    # Fix boolean parameter\n","    if model_args.do_lower_case == 'False' or not model_args.do_lower_case:\n","        model_args.do_lower_case = False\n","        'Tokenizer do_lower_case False'\n","    else:\n","        model_args.do_lower_case = True\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        handlers=[logging.StreamHandler(sys.stdout)],\n","    )\n","\n","    log_level = training_args.get_process_log_level()\n","    logger.setLevel(log_level)\n","    datasets.utils.logging.set_verbosity(log_level)\n","    transformers.utils.logging.set_verbosity(log_level)\n","    transformers.utils.logging.enable_default_handler()\n","    transformers.utils.logging.enable_explicit_format()\n","\n","    # Log on each process the small summary:\n","    logger.warning(\n","        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n","        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n","    )\n","    logger.info(f\"Training/evaluation parameters {training_args}\")\n","\n","    # Detecting last checkpoint.\n","    last_checkpoint = None\n","    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n","        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n","        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n","            raise ValueError(\n","                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n","                \"Use --overwrite_output_dir to overcome.\"\n","            )\n","        elif last_checkpoint is not None:\n","            logger.info(\n","                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n","                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n","            )\n","\n","    # Set seed before initializing model.\n","    set_seed(training_args.seed)\n","\n","    # In distributed training, the load_dataset function guarantees that only one local process can concurrently\n","    # download the dataset.\n","    # Downloading and loading eurlex dataset from the hub.\n","    if training_args.do_train:\n","        train_dataset = load_dataset(\"lex_glue\", \"unfair_tos\", split=\"train\", data_dir='data', cache_dir=model_args.cache_dir)\n","\n","    if training_args.do_eval:\n","        eval_dataset = load_dataset(\"lex_glue\", \"unfair_tos\", split=\"validation\", data_dir='data', cache_dir=model_args.cache_dir)\n","\n","    if training_args.do_predict:\n","        predict_dataset = load_dataset(\"lex_glue\", \"unfair_tos\", split=\"test\", data_dir='data', cache_dir=model_args.cache_dir)\n","\n","    # Labels\n","    label_list = list(range(8))\n","    num_labels = len(label_list)\n","\n","    # Load pretrained model and tokenizer\n","    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n","    # download model & vocab.\n","    config = AutoConfig.from_pretrained(\n","        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n","        num_labels=num_labels,\n","        finetuning_task=\"unfair_toc\",\n","        cache_dir=model_args.cache_dir,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","\n","    if config.model_type == 'big_bird':\n","        config.attention_type = 'original_full'\n","\n","    if config.model_type == 'longformer':\n","        config.attention_window = [128] * config.num_hidden_layers\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n","        do_lower_case=model_args.do_lower_case,\n","        cache_dir=model_args.cache_dir,\n","        use_fast=model_args.use_fast_tokenizer,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        model_args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","        config=config,\n","        cache_dir=model_args.cache_dir,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","\n","    # Preprocessing the datasets\n","    # Padding strategy\n","    if data_args.pad_to_max_length:\n","        padding = \"max_length\"\n","    else:\n","        # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n","        padding = False\n","\n","    def preprocess_function(examples):\n","        # Tokenize the texts\n","        batch = tokenizer(\n","            examples[\"text\"],\n","            padding=padding,\n","            max_length=data_args.max_seq_length,\n","            truncation=True,\n","        )\n","        batch[\"labels\"] = [[1 if label in labels else 0 for label in label_list] for labels in\n","                              examples[\"labels\"]]\n","\n","        return batch\n","\n","    if training_args.do_train:\n","        if data_args.max_train_samples is not None:\n","            train_dataset = train_dataset.select(range(data_args.max_train_samples))\n","        with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n","            train_dataset = train_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on train dataset\",\n","            )\n","        # Log a few random samples from the training set:\n","        for index in random.sample(range(len(train_dataset)), 3):\n","            logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n","\n","    if training_args.do_eval:\n","        if data_args.max_eval_samples is not None:\n","            eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))\n","        with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n","            eval_dataset = eval_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on validation dataset\",\n","            )\n","\n","    if training_args.do_predict:\n","        if data_args.max_predict_samples is not None:\n","            predict_dataset = predict_dataset.select(range(data_args.max_predict_samples))\n","        with training_args.main_process_first(desc=\"prediction dataset map pre-processing\"):\n","            predict_dataset = predict_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on prediction dataset\",\n","            )\n","\n","    # You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n","    # predictions and label_ids field) and has to return a dictionary string to float.\n","    def compute_metrics(p: EvalPrediction):\n","        # Fix gold labels\n","        y_true = np.zeros((p.label_ids.shape[0], p.label_ids.shape[1] + 1), dtype=np.int32)\n","        y_true[:, :-1] = p.label_ids\n","        y_true[:, -1] = (np.sum(p.label_ids, axis=1) == 0).astype('int32')\n","        # Fix predictions\n","        logits = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","        preds = (expit(logits) > 0.5).astype('int32')\n","        y_pred = np.zeros((p.label_ids.shape[0], p.label_ids.shape[1] + 1), dtype=np.int32)\n","        y_pred[:, :-1] = preds\n","        y_pred[:, -1] = (np.sum(preds, axis=1) == 0).astype('int32')\n","        # Compute scores\n","        macro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='macro', zero_division=0)\n","        micro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='micro', zero_division=0)\n","        return {'macro-f1': macro_f1, 'micro-f1': micro_f1}\n","\n","    # Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n","    if data_args.pad_to_max_length:\n","        data_collator = default_data_collator\n","    elif training_args.fp16:\n","        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n","    else:\n","        data_collator = None\n","\n","    # Initialize our Trainer\n","    trainer = MultilabelTrainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset if training_args.do_train else None,\n","        eval_dataset=eval_dataset if training_args.do_eval else None,\n","        compute_metrics=compute_metrics,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n","    )\n","\n","    # Training\n","    if training_args.do_train:\n","        checkpoint = None\n","        if training_args.resume_from_checkpoint is not None:\n","            checkpoint = training_args.resume_from_checkpoint\n","        elif last_checkpoint is not None:\n","            checkpoint = last_checkpoint\n","        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n","        metrics = train_result.metrics\n","        max_train_samples = (\n","            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n","        )\n","        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n","\n","        trainer.save_model()  # Saves the tokenizer too for easy upload\n","\n","        trainer.log_metrics(\"train\", metrics)\n","        trainer.save_metrics(\"train\", metrics)\n","        trainer.save_state()\n","\n","    # Evaluation\n","    if training_args.do_eval:\n","        logger.info(\"*** Evaluate ***\")\n","        metrics = trainer.evaluate(eval_dataset=eval_dataset)\n","\n","        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n","        metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n","\n","        trainer.log_metrics(\"eval\", metrics)\n","        trainer.save_metrics(\"eval\", metrics)\n","\n","    # Prediction\n","    if training_args.do_predict:\n","        logger.info(\"*** Predict ***\")\n","        predictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix=\"predict\")\n","\n","        max_predict_samples = (\n","            data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)\n","        )\n","        metrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n","\n","        trainer.log_metrics(\"predict\", metrics)\n","        trainer.save_metrics(\"predict\", metrics)\n","\n","        output_predict_file = os.path.join(training_args.output_dir, \"test_predictions.csv\")\n","        if trainer.is_world_process_zero():\n","            with open(output_predict_file, \"w\") as writer:\n","                for index, pred_list in enumerate(predictions[0]):\n","                    pred_line = '\\t'.join([f'{pred:.5f}' for pred in pred_list.tolist()])\n","                    writer.write(f\"{index}\\t{pred_line}\\n\")\n","\n","    # Clean up checkpoints\n","    checkpoints = [filepath for filepath in glob.glob(f'{training_args.output_dir}/*/') if '/checkpoint' in filepath]\n","    for checkpoint in checkpoints:\n","        shutil.rmtree(checkpoint)\n","\n","\n","if __name__ == \"__main__\":\n"," #    For Evaluation\n","    training_args = TrainingArguments(\n","        do_train = True,\n","        do_eval = True,\n","        do_predict = True,\n","        output_dir=os.getcwd(),\n","        overwrite_output_dir=True,\n","        num_train_epochs=2,\n","        per_device_train_batch_size=8,\n","        per_device_eval_batch_size=8,\n","        save_steps=500,\n","        save_total_limit=2,\n","        fp16=False,\n","        logging_dir=\"./logs\",\n","        logging_steps=200,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=500,\n","        logging_first_step=False,\n","        load_best_model_at_end = True,\n","        metric_for_best_model=\"macro-f1\",\n","    )\n","    main(training_args)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5228,"status":"ok","timestamp":1689896740296,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"},"user_tz":-60},"id":"SLZDIELksbwn","outputId":"6239368b-e03c-456a-8cec-e88758d3117a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"]}],"source":["pip install --upgrade transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["50dfdba2a16a4468a776f38a4a959b22","6bd7b209bb3a4674af2fcf6acf059fbe","ce68de4bc52a4d009526da735fa04a8b","0e0637a70b5c4b91adee1abf57367543","c951de5e84044ec08ad22b9f867e8cd9","f4809f95c8f541a48bdeddeb7f2c681c","78ad33dc3d0e4b76a744c408338119d9","d7734d35f5d948d1b96e4b3420af92f2","e36087148ed14dde8cad0ba372d6a958","bae25b13f9174004ba576e137b21e180","842b34b9117644a9bf16d0a68e2f13d1","479fd428bfb64c91b8c09dab4cb5aab3","c55afeb8d8c9476d8e1f0750b5b9c634","bf518457f4494d6bb59f4ed33ee9f19e","a3d71a12da20417d96c075c29a85a3ba","ad2e01a5a3fb4bafb0dbb585c3f4c2aa","fb44a253e49f48d9a4c3cc25af56679c","10046d513ebf4c9782fa1f4926406fda","2e07411366c94542b1bc877705d0b2c0","c55eacacaf7649a891adcd0c0242453e","b8d0a24a93bb41b48cbe9c03dc2134a8","ba3238bf9336424dabffeca80282ecf9","018e017133cc48fea26244709c3ce738","f2f88b76ef6549c0bf5bb673960add6a","65465f712c7d44d48ef13ca54c7a0bd6","ee2773b8e9cd4c39af6a7f10e3ae4f48","1cf9071d1cd8474a88d0cfbfadcf3cf6","1ea84f047b0f4a0083fb61a4869edd09","1e07598ff0c847f0938a44a9b0c4ef0e","22854f47b3ad4b2ea6af0a0c44af6af5","39db27f176ef4c51941d63acf4cd5829","e003feb1cbdd4ea8b92f1d397fe5b052","795eb28905d145038df3c7b896e9d1a8","958537996c2340168f02de3a73c0e034","e13ac8b55c2b46c6b9b089269d550ebc","57654d55a3a34ff486ee1af57f9e78e6","59bc23c52b64456a8e6f7f56ec377c32","c94f0926e23043f9b8a71309b92019f2","07f57c4aa65a4ee0adac45bf2b94ebc2","ae12c82bb38142a5b000cffc5c1f87ea","8c3fecfa10bf4280aed8c9dcb93f94b3","1380dd34fabe46b696cae4ac9130bb8e","cec4f7284e424a17a0a97d4d59754f86","936d34e02e9245e0bc479a43550e8958","2608744b99164200ba18ba625dc9c303","bd1aa50b342e4b338d5053ee0ee4985b","be83be31ea7f47f4a3e2e830b7d64dc7","5b59b5f1a1b148df93d6d91e6e52da68","d959e20713a04777addce249dfd18a06","daac365702144c1bb550e4d98f0aa500","3b48afacc0834191b31963b71c333583","093daac294574bf5b036b2d9b30e3b18","d5b3fab0ca424d58a2bf27127be12403","504a6042fbfb4f11bd5df9d928056460","263a01d58cd842d0a13e986ab8f37f85","e712427370224a6498457db89aec1d23","5e5e6d22764d4cd8a18e7367038c37a7","c82a86d204e74813bd3f1fd37e8ab70e","ac479cbb932645bfa3b4527b38f0e0a2","fd6fd0d418b14bde98f2590a3806843b","430ea0c945ef445c8d508bcfc0fd8482","e7d0d3942fbc41a79bc9ab75db47d1f7","e97b9131dbe9438994a89fa2b3c5c976","db389b4b36514e8882272dd3e852c08e","aae1dc15b15f4a19903d5221e178a8dd","296111eaaab2460e8aa0515eb103fe82","f66b64b9d9d9455397e14144435215f3","d21080b841624469a3c1f902a3761cb9","ade01a070eb742e68e5f673c1cd93f02","ff9a18eaa3d0460ba54072515f5a43ca","d3f8781634744768bc5be832a0678282","ef7d31e3ad28466fb2801fb577e77008","5979352ca84d49e8a2550af96ea06dbe","89fb254c313844fd8b9408326c6778ab","4ae19ac474044a859e515303c8252387","5d93ec15abcc4df6b67ce8f72e884ddf","ad21dd22325a4d6393fb1d4a2472235f"]},"executionInfo":{"elapsed":1539978,"status":"ok","timestamp":1691298355937,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"},"user_tz":-60},"id":"X03FjIupraS-","outputId":"053c1b09-755e-4a80-9d58-9692c9fe09cf"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"50dfdba2a16a4468a776f38a4a959b22","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/16.3M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"479fd428bfb64c91b8c09dab4cb5aab3","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/60000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"018e017133cc48fea26244709c3ce738","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"958537996c2340168f02de3a73c0e034","version_major":2,"version_minor":0},"text/plain":["Generating validation split:   0%|          | 0/10000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[WARNING|modeling_utils.py:3331] 2023-08-06 04:40:39,326 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2608744b99164200ba18ba625dc9c303","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on train dataset:   0%|          | 0/60000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e712427370224a6498457db89aec1d23","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on validation dataset:   0%|          | 0/10000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f66b64b9d9d9455397e14144435215f3","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on prediction dataset:   0%|          | 0/10000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11000' max='30000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11000/30000 23:47 < 41:05, 7.71 it/s, Epoch 0/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro-f1</th>\n","      <th>Micro-f1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>2.536300</td>\n","      <td>1.861240</td>\n","      <td>0.437121</td>\n","      <td>0.672700</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.398700</td>\n","      <td>1.268317</td>\n","      <td>0.488014</td>\n","      <td>0.716600</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>1.278100</td>\n","      <td>1.088620</td>\n","      <td>0.547837</td>\n","      <td>0.751600</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>1.092500</td>\n","      <td>1.016976</td>\n","      <td>0.579902</td>\n","      <td>0.765300</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>1.148800</td>\n","      <td>1.033687</td>\n","      <td>0.594855</td>\n","      <td>0.758500</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>1.020400</td>\n","      <td>0.973116</td>\n","      <td>0.601465</td>\n","      <td>0.767900</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.950400</td>\n","      <td>0.906729</td>\n","      <td>0.605360</td>\n","      <td>0.769000</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.949600</td>\n","      <td>0.931866</td>\n","      <td>0.625732</td>\n","      <td>0.782800</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.877400</td>\n","      <td>0.912036</td>\n","      <td>0.613990</td>\n","      <td>0.780100</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.971700</td>\n","      <td>0.894923</td>\n","      <td>0.633270</td>\n","      <td>0.785500</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.888100</td>\n","      <td>0.870812</td>\n","      <td>0.645050</td>\n","      <td>0.793400</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.773300</td>\n","      <td>0.882204</td>\n","      <td>0.655270</td>\n","      <td>0.796800</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>0.912000</td>\n","      <td>0.836033</td>\n","      <td>0.649910</td>\n","      <td>0.800000</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>0.953600</td>\n","      <td>0.838219</td>\n","      <td>0.671407</td>\n","      <td>0.804500</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>0.958300</td>\n","      <td>0.828471</td>\n","      <td>0.674714</td>\n","      <td>0.802600</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>0.826800</td>\n","      <td>0.820714</td>\n","      <td>0.683327</td>\n","      <td>0.809000</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>0.718100</td>\n","      <td>0.862273</td>\n","      <td>0.678627</td>\n","      <td>0.808600</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>0.851200</td>\n","      <td>0.812551</td>\n","      <td>0.693762</td>\n","      <td>0.812100</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>0.763500</td>\n","      <td>0.757328</td>\n","      <td>0.711841</td>\n","      <td>0.823700</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>0.708500</td>\n","      <td>0.791261</td>\n","      <td>0.709061</td>\n","      <td>0.822300</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>0.807900</td>\n","      <td>0.760132</td>\n","      <td>0.702612</td>\n","      <td>0.821400</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>0.790600</td>\n","      <td>0.806991</td>\n","      <td>0.697738</td>\n","      <td>0.814700</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["***** train metrics *****\n","  epoch                    =       0.73\n","  total_flos               =  2697825GF\n","  train_loss               =     1.0322\n","  train_runtime            = 0:23:47.29\n","  train_samples            =      60000\n","  train_samples_per_second =     84.075\n","  train_steps_per_second   =     21.019\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["***** eval metrics *****\n","  epoch                   =       0.73\n","  eval_loss               =     0.7573\n","  eval_macro-f1           =     0.7118\n","  eval_micro-f1           =     0.8237\n","  eval_runtime            = 0:00:35.94\n","  eval_samples            =      10000\n","  eval_samples_per_second =    278.173\n","  eval_steps_per_second   =     69.543\n","***** predict metrics *****\n","  predict_loss               =      0.743\n","  predict_macro-f1           =     0.7082\n","  predict_micro-f1           =     0.8263\n","  predict_runtime            = 0:00:39.53\n","  predict_samples            =      10000\n","  predict_samples_per_second =    252.926\n","  predict_steps_per_second   =     63.231\n"]}],"source":["#!/usr/bin/env python\n","# coding=utf-8\n","\"\"\" Finetuning models on LEDGAR (e.g. Bert, RoBERTa, LEGAL-BERT).\"\"\"\n","\n","import logging\n","import os\n","import random\n","import sys\n","from dataclasses import dataclass, field\n","from typing import Optional\n","\n","import datasets\n","from datasets import load_dataset\n","from sklearn.metrics import f1_score\n","import numpy as np\n","import glob\n","import shutil\n","\n","import transformers\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    DataCollatorWithPadding,\n","    EvalPrediction,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    default_data_collator,\n","    set_seed,\n","    EarlyStoppingCallback,\n","    Trainer\n",")\n","from transformers.trainer_utils import get_last_checkpoint\n","from transformers.utils import check_min_version\n","from transformers.utils.versions import require_version\n","\n","\n","# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n","check_min_version(\"4.9.0\")\n","\n","require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/text-classification/requirements.txt\")\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","@dataclass\n","class DataTrainingArguments:\n","    \"\"\"\n","    Arguments pertaining to what data we are going to input our model for training and eval.\n","\n","    Using `HfArgumentParser` we can turn this class\n","    into argparse arguments to be able to specify them on\n","    the command line.\n","    \"\"\"\n","\n","    max_seq_length: Optional[int] = field(\n","        default=512,\n","        metadata={\n","            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n","            \"than this will be truncated, sequences shorter will be padded.\"\n","        },\n","    )\n","    overwrite_cache: bool = field(\n","        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n","    )\n","    pad_to_max_length: bool = field(\n","        default=True,\n","        metadata={\n","            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n","            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n","        },\n","    )\n","    max_train_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_eval_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_predict_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    server_ip: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n","    server_port: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n","\n","\n","@dataclass\n","class ModelArguments:\n","    \"\"\"\n","    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n","    \"\"\"\n","\n","    model_name_or_path: str = field(\n","        default=None, metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n","    )\n","    config_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n","    )\n","    tokenizer_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n","    )\n","    cache_dir: Optional[str] = field(\n","        default=None,\n","        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n","    )\n","    do_lower_case: Optional[bool] = field(\n","        default=True,\n","        metadata={\"help\": \"arg to indicate if tokenizer should do lower case in AutoTokenizer.from_pretrained()\"},\n","    )\n","    use_fast_tokenizer: bool = field(\n","        default=True,\n","        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n","    )\n","    model_revision: str = field(\n","        default=\"main\",\n","        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n","    )\n","    use_auth_token: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n","            \"with private models).\"\n","        },\n","    )\n","\n","\n","def main(training_args):\n","    # See all possible arguments in src/transformers/training_args.py\n","    # or by passing the --help flag to this script.\n","    # We now keep distinct sets of args, for a cleaner separation of concerns.\n","\n","    model_args = ModelArguments(\n","        model_name_or_path=\"nlpaueb/legal-bert-base-uncased\",\n","        #hierarchical=True,\n","        do_lower_case=True,\n","        use_fast_tokenizer=True,\n","    )\n","    data_args = DataTrainingArguments(\n","        max_seq_length=128,\n","        #max_segments=64,\n","        #max_seg_length=128,\n","        overwrite_cache=False,\n","        pad_to_max_length=True,\n","    )\n","\n","    # Setup distant debugging if needed\n","    if data_args.server_ip and data_args.server_port:\n","        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n","        import ptvsd\n","\n","        print(\"Waiting for debugger attach\")\n","        ptvsd.enable_attach(address=(data_args.server_ip, data_args.server_port), redirect_output=True)\n","        ptvsd.wait_for_attach()\n","\n","    # Fix boolean parameter\n","    if model_args.do_lower_case == 'False' or not model_args.do_lower_case:\n","        model_args.do_lower_case = False\n","        'Tokenizer do_lower_case False'\n","    else:\n","        model_args.do_lower_case = True\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        handlers=[logging.StreamHandler(sys.stdout)],\n","    )\n","\n","    log_level = training_args.get_process_log_level()\n","    logger.setLevel(log_level)\n","    datasets.utils.logging.set_verbosity(log_level)\n","    transformers.utils.logging.set_verbosity(log_level)\n","    transformers.utils.logging.enable_default_handler()\n","    transformers.utils.logging.enable_explicit_format()\n","\n","    # Log on each process the small summary:\n","    logger.warning(\n","        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n","        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n","    )\n","    logger.info(f\"Training/evaluation parameters {training_args}\")\n","\n","    # Detecting last checkpoint.\n","    last_checkpoint = None\n","    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n","        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n","        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n","            raise ValueError(\n","                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n","                \"Use --overwrite_output_dir to overcome.\"\n","            )\n","        elif last_checkpoint is not None:\n","            logger.info(\n","                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n","                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n","            )\n","\n","    # Set seed before initializing model.\n","    set_seed(training_args.seed)\n","\n","    # In distributed training, the load_dataset function guarantees that only one local process can concurrently\n","    # download the dataset.\n","    # Downloading and loading eurlex dataset from the hub.\n","    if training_args.do_train:\n","        train_dataset = load_dataset(\"lex_glue\", \"ledgar\", split=\"train\", cache_dir=model_args.cache_dir)\n","\n","    if training_args.do_eval:\n","        eval_dataset = load_dataset(\"lex_glue\", \"ledgar\", split=\"validation\", cache_dir=model_args.cache_dir)\n","\n","    if training_args.do_predict:\n","        predict_dataset = load_dataset(\"lex_glue\", \"ledgar\", split=\"test\", cache_dir=model_args.cache_dir)\n","\n","    # Labels\n","    label_list = list(range(100))\n","    num_labels = len(label_list)\n","\n","    # Load pretrained model and tokenizer\n","    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n","    # download model & vocab.\n","    config = AutoConfig.from_pretrained(\n","        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n","        num_labels=num_labels,\n","        finetuning_task=\"eurlex\",\n","        cache_dir=model_args.cache_dir,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","\n","    if config.model_type == 'big_bird':\n","        config.attention_type = 'original_full'\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n","        do_lower_case=model_args.do_lower_case,\n","        cache_dir=model_args.cache_dir,\n","        use_fast=model_args.use_fast_tokenizer,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        model_args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","        config=config,\n","        cache_dir=model_args.cache_dir,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","\n","    # Preprocessing the datasets\n","    # Padding strategy\n","    if data_args.pad_to_max_length:\n","        padding = \"max_length\"\n","    else:\n","        # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n","        padding = False\n","\n","    def preprocess_function(examples):\n","        # Tokenize the texts\n","        batch = tokenizer(\n","            examples[\"text\"],\n","            padding=padding,\n","            max_length=data_args.max_seq_length,\n","            truncation=True,\n","        )\n","        batch[\"label\"] = [label_list.index(label) for label in examples[\"label\"]]\n","\n","        return batch\n","\n","    if training_args.do_train:\n","        if data_args.max_train_samples is not None:\n","            train_dataset = train_dataset.select(range(data_args.max_train_samples))\n","        with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n","            train_dataset = train_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on train dataset\",\n","            )\n","        # Log a few random samples from the training set:\n","        for index in random.sample(range(len(train_dataset)), 3):\n","            logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n","\n","    if training_args.do_eval:\n","        if data_args.max_eval_samples is not None:\n","            eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))\n","        with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n","            eval_dataset = eval_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on validation dataset\",\n","            )\n","\n","    if training_args.do_predict:\n","        if data_args.max_predict_samples is not None:\n","            predict_dataset = predict_dataset.select(range(data_args.max_predict_samples))\n","        with training_args.main_process_first(desc=\"prediction dataset map pre-processing\"):\n","            predict_dataset = predict_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on prediction dataset\",\n","            )\n","\n","    # You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n","    # predictions and label_ids field) and has to return a dictionary string to float.\n","    def compute_metrics(p: EvalPrediction):\n","        logits = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","        preds = np.argmax(logits, axis=1)\n","        macro_f1 = f1_score(y_true=p.label_ids, y_pred=preds, average='macro', zero_division=0)\n","        micro_f1 = f1_score(y_true=p.label_ids, y_pred=preds, average='micro', zero_division=0)\n","        return {'macro-f1': macro_f1, 'micro-f1': micro_f1}\n","\n","    # Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n","    if data_args.pad_to_max_length:\n","        data_collator = default_data_collator\n","    elif training_args.fp16:\n","        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n","    else:\n","        data_collator = None\n","\n","    # Initialize our Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        #train_dataset=train_dataset if training_args.do_train else None,\n","        eval_dataset=eval_dataset if training_args.do_eval else None,\n","        #eval_dataset=eval_dataset if training_args.do_eval else None,\n","        compute_metrics=compute_metrics,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n","    )\n","\n","    # Training\n","    if training_args.do_train:\n","        checkpoint = None\n","        if training_args.resume_from_checkpoint is not None:\n","            checkpoint = training_args.resume_from_checkpoint\n","        elif last_checkpoint is not None:\n","            checkpoint = last_checkpoint\n","        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n","        metrics = train_result.metrics\n","        max_train_samples = (\n","            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n","        )\n","        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n","\n","        trainer.save_model()  # Saves the tokenizer too for easy upload\n","\n","        trainer.log_metrics(\"train\", metrics)\n","        trainer.save_metrics(\"train\", metrics)\n","        trainer.save_state()\n","\n","    # Evaluation\n","    if training_args.do_eval:\n","        logger.info(\"*** Evaluate ***\")\n","        metrics = trainer.evaluate(eval_dataset=eval_dataset)\n","\n","        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n","        metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n","\n","        trainer.log_metrics(\"eval\", metrics)\n","        trainer.save_metrics(\"eval\", metrics)\n","\n","    # Prediction\n","    if training_args.do_predict:\n","        logger.info(\"*** Predict ***\")\n","        predictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix=\"predict\")\n","\n","        max_predict_samples = (\n","            data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)\n","        )\n","        metrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n","\n","        trainer.log_metrics(\"predict\", metrics)\n","        trainer.save_metrics(\"predict\", metrics)\n","\n","        output_predict_file = os.path.join(training_args.output_dir, \"test_predictions.csv\")\n","        if trainer.is_world_process_zero():\n","            with open(output_predict_file, \"w\") as writer:\n","                for index, pred_list in enumerate(predictions):\n","                    pred_line = '\\t'.join([f'{pred:.5f}' for pred in pred_list])\n","                    writer.write(f\"{index}\\t{pred_line}\\n\")\n","\n","\n","    # Clean up checkpoints\n","    checkpoints = [filepath for filepath in glob.glob(f'{training_args.output_dir}/*/') if '/checkpoint' in filepath]\n","    for checkpoint in checkpoints:\n","        shutil.rmtree(checkpoint)\n","\n","\n","if __name__ == \"__main__\":\n"," #    For Evaluation\n","    training_args = TrainingArguments(\n","        do_train = True,\n","        do_eval = True,\n","        do_predict = True,\n","        output_dir=os.getcwd(),\n","        overwrite_output_dir=True,\n","        num_train_epochs=2,\n","        per_device_train_batch_size=4,\n","        per_device_eval_batch_size=4,\n","        save_steps=500,\n","        save_total_limit=2,\n","        fp16=False,\n","        logging_dir=\"./logs\",\n","        logging_steps=200,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=500,\n","        logging_first_step=False,\n","        load_best_model_at_end = True,\n","        metric_for_best_model=\"macro-f1\",\n","    )\n","    main(training_args)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4EdBFu5YZs6V"},"outputs":[],"source":["import logging\n","import os\n","from dataclasses import dataclass\n","from enum import Enum\n","from typing import List, Optional\n","\n","import tqdm\n","import re\n","\n","from filelock import FileLock\n","from transformers import PreTrainedTokenizer, is_tf_available, is_torch_available\n","import datasets\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","@dataclass(frozen=True)\n","class InputFeatures:\n","    \"\"\"\n","    A single set of features of data.\n","    Property names are the same names as the corresponding inputs to a model.\n","    \"\"\"\n","\n","    input_ids: List[List[int]]\n","    attention_mask: Optional[List[List[int]]]\n","    token_type_ids: Optional[List[List[int]]]\n","    label: Optional[int]\n","\n","\n","class Split(Enum):\n","    train = \"train\"\n","    dev = \"dev\"\n","    test = \"test\"\n","\n","\n","if is_torch_available():\n","    import torch\n","    from torch.utils.data.dataset import Dataset\n","\n","    class MultipleChoiceDataset(Dataset):\n","        \"\"\"\n","        PyTorch multiple choice dataset class\n","        \"\"\"\n","\n","        features: List[InputFeatures]\n","\n","        def __init__(\n","            self,\n","            tokenizer: PreTrainedTokenizer,\n","            task: str,\n","            max_seq_length: Optional[int] = None,\n","            overwrite_cache=False,\n","            mode: Split = Split.train,\n","        ):\n","            dataset = datasets.load_dataset('lex_glue', task)\n","            tokenizer_name = re.sub('[^a-z]+', ' ', tokenizer.name_or_path).title().replace(' ', '')\n","            cached_features_file = os.path.join(\n","                '.cache',\n","                task,\n","                \"cached_{}_{}_{}_{}\".format(\n","                    mode.value,\n","                    tokenizer_name,\n","                    str(max_seq_length),\n","                    task,\n","                ),\n","            )\n","\n","            # Make sure only the first process in distributed training processes the dataset,\n","            # and the others will use the cache.\n","            lock_path = cached_features_file + \".lock\"\n","            if not os.path.exists(os.path.join('.cache', task)):\n","                if not os.path.exists('.cache'):\n","                    os.mkdir('.cache')\n","                os.mkdir(os.path.join('.cache', task))\n","            with FileLock(lock_path):\n","\n","                if os.path.exists(cached_features_file) and not overwrite_cache:\n","                    logger.info(f\"Loading features from cached file {cached_features_file}\")\n","                    self.features = torch.load(cached_features_file)\n","                else:\n","                    logger.info(f\"Creating features from dataset file at {task}\")\n","                    if mode == Split.dev:\n","                        examples = dataset['validation']\n","                    elif mode == Split.test:\n","                        examples = dataset['test']\n","                    elif mode == Split.train:\n","                        examples = dataset['train']\n","                    logger.info(\"Training examples: %s\", len(examples))\n","                    self.features = convert_examples_to_features(\n","                        examples,\n","                        max_seq_length,\n","                        tokenizer,\n","                    )\n","                    logger.info(\"Saving features into cached file %s\", cached_features_file)\n","                    torch.save(self.features, cached_features_file)\n","\n","        def __len__(self):\n","            return len(self.features)\n","\n","        def __getitem__(self, i) -> InputFeatures:\n","            return self.features[i]\n","\n","\n","if is_tf_available():\n","    import tensorflow as tf\n","\n","    class TFMultipleChoiceDataset:\n","        \"\"\"\n","        TensorFlow multiple choice dataset class\n","        \"\"\"\n","\n","        features: List[InputFeatures]\n","\n","        def __init__(\n","            self,\n","            tokenizer: PreTrainedTokenizer,\n","            task: str,\n","            max_seq_length: Optional[int] = 256,\n","            overwrite_cache=False,\n","            mode: Split = Split.train,\n","        ):\n","            dataset = datasets.load_dataset('lex_glue')\n","\n","            logger.info(f\"Creating features from dataset file at {task}\")\n","            if mode == Split.dev:\n","                examples = dataset['validation']\n","            elif mode == Split.test:\n","                examples = dataset['test']\n","            else:\n","                examples = dataset['train']\n","            logger.info(f\"{mode.name.title()} examples: %s\", len(examples))\n","\n","            self.features = convert_examples_to_features(\n","                examples,\n","                max_seq_length,\n","                tokenizer,\n","            )\n","\n","            def gen():\n","                for (ex_index, ex) in tqdm.tqdm(enumerate(self.features), desc=\"convert examples to features\"):\n","                    if ex_index % 10000 == 0:\n","                        logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n","\n","                    yield (\n","                        {\n","                            \"input_ids\": ex.input_ids,\n","                            \"attention_mask\": ex.attention_mask,\n","                            \"token_type_ids\": ex.token_type_ids,\n","                        },\n","                        ex.label,\n","                    )\n","\n","            self.dataset = tf.data.Dataset.from_generator(\n","                gen,\n","                (\n","                    {\n","                        \"input_ids\": tf.int32,\n","                        \"attention_mask\": tf.int32,\n","                        \"token_type_ids\": tf.int32,\n","                    },\n","                    tf.int64,\n","                ),\n","                (\n","                    {\n","                        \"input_ids\": tf.TensorShape([None, None]),\n","                        \"attention_mask\": tf.TensorShape([None, None]),\n","                        \"token_type_ids\": tf.TensorShape([None, None]),\n","                    },\n","                    tf.TensorShape([]),\n","                ),\n","            )\n","\n","        def get_dataset(self):\n","            self.dataset = self.dataset.apply(tf.data.experimental.assert_cardinality(len(self.features)))\n","\n","            return self.dataset\n","\n","        def __len__(self):\n","            return len(self.features)\n","\n","        def __getitem__(self, i) -> InputFeatures:\n","            return self.features[i]\n","\n","\n","def convert_examples_to_features(\n","    examples: datasets.Dataset,\n","    max_length: int,\n","    tokenizer: PreTrainedTokenizer,\n",") -> List[InputFeatures]:\n","    \"\"\"\n","    Loads a data file into a list of `InputFeatures`\n","    \"\"\"\n","    features = []\n","    for (ex_index, example) in tqdm.tqdm(enumerate(examples), desc=\"convert examples to features\"):\n","        if ex_index % 10000 == 0:\n","            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n","        choices_inputs = []\n","        for ending_idx, ending in enumerate(example['endings']):\n","            context = example['context']\n","            inputs = tokenizer(\n","                context,\n","                ending,\n","                add_special_tokens=True,\n","                max_length=max_length,\n","                padding=\"max_length\",\n","                truncation=True,\n","            )\n","\n","            choices_inputs.append(inputs)\n","\n","        label = example['label']\n","\n","        input_ids = [x[\"input_ids\"] for x in choices_inputs]\n","        attention_mask = (\n","            [x[\"attention_mask\"] for x in choices_inputs] if \"attention_mask\" in choices_inputs[0] else None\n","        )\n","        token_type_ids = (\n","            [x[\"token_type_ids\"] for x in choices_inputs] if \"token_type_ids\" in choices_inputs[0] else None\n","        )\n","\n","        features.append(\n","            InputFeatures(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                token_type_ids=token_type_ids,\n","                label=label,\n","            )\n","        )\n","\n","    for f in features[:2]:\n","        logger.info(\"*** Example ***\")\n","        logger.info(\"feature: %s\" % f)\n","\n","    return features\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"9KWqMmxCS2TN","outputId":"8b86bcfd-3705-4f8f-f333-e8189573365c"},"outputs":[{"name":"stderr","output_type":"stream","text":["[INFO|training_args.py:1299] 2023-07-28 17:35:42,149 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n","[INFO|training_args.py:1713] 2023-07-28 17:35:42,151 >> PyTorch: setting up devices\n","[INFO|training_args.py:1439] 2023-07-28 17:35:42,155 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False\n","[INFO|configuration_utils.py:712] 2023-07-28 17:35:42,212 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-28 17:35:42,213 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"finetuning_task\": \"case_hold\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|configuration_utils.py:712] 2023-07-28 17:35:42,267 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-28 17:35:42,269 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 17:35:42,272 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/vocab.txt\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 17:35:42,273 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer.json\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 17:35:42,276 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 17:35:42,277 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 17:35:42,278 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer_config.json\n","[INFO|configuration_utils.py:712] 2023-07-28 17:35:42,281 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-28 17:35:42,282 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|modeling_utils.py:2603] 2023-07-28 17:35:42,324 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/model.safetensors\n","[INFO|modeling_utils.py:3319] 2023-07-28 17:35:43,291 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMultipleChoice: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:3331] 2023-07-28 17:35:43,292 >> Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","convert examples to features: 45000it [05:14, 143.18it/s]\n","convert examples to features: 3900it [00:28, 134.99it/s]\n","convert examples to features: 3600it [00:26, 133.78it/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1500: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","[INFO|trainer.py:1686] 2023-07-28 17:42:42,331 >> ***** Running training *****\n","[INFO|trainer.py:1687] 2023-07-28 17:42:42,332 >>   Num examples = 45,000\n","[INFO|trainer.py:1688] 2023-07-28 17:42:42,333 >>   Num Epochs = 1\n","[INFO|trainer.py:1689] 2023-07-28 17:42:42,334 >>   Instantaneous batch size per device = 2\n","[INFO|trainer.py:1692] 2023-07-28 17:42:42,337 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n","[INFO|trainer.py:1693] 2023-07-28 17:42:42,338 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1694] 2023-07-28 17:42:42,340 >>   Total optimization steps = 22,500\n","[INFO|trainer.py:1695] 2023-07-28 17:42:42,342 >>   Number of trainable parameters = 108,311,041\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6500' max='22500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 6500/22500 21:48 < 53:41, 4.97 it/s, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro-f1</th>\n","      <th>Micro-f1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>1.249000</td>\n","      <td>1.175207</td>\n","      <td>0.537423</td>\n","      <td>0.537436</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.182800</td>\n","      <td>1.127146</td>\n","      <td>0.555552</td>\n","      <td>0.555641</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>1.067400</td>\n","      <td>1.097465</td>\n","      <td>0.571301</td>\n","      <td>0.571282</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>1.074200</td>\n","      <td>1.060058</td>\n","      <td>0.569259</td>\n","      <td>0.569487</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>1.074900</td>\n","      <td>1.102692</td>\n","      <td>0.579966</td>\n","      <td>0.580000</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>1.136200</td>\n","      <td>1.097008</td>\n","      <td>0.584679</td>\n","      <td>0.584615</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>1.123000</td>\n","      <td>1.120891</td>\n","      <td>0.583792</td>\n","      <td>0.583846</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>1.132700</td>\n","      <td>1.059442</td>\n","      <td>0.596483</td>\n","      <td>0.596667</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>1.061800</td>\n","      <td>1.065231</td>\n","      <td>0.598838</td>\n","      <td>0.598974</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>1.138100</td>\n","      <td>1.048116</td>\n","      <td>0.599958</td>\n","      <td>0.600000</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>1.020800</td>\n","      <td>1.055843</td>\n","      <td>0.594582</td>\n","      <td>0.594615</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>1.031900</td>\n","      <td>1.089456</td>\n","      <td>0.597879</td>\n","      <td>0.597949</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>1.106000</td>\n","      <td>1.062823</td>\n","      <td>0.590022</td>\n","      <td>0.590000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[INFO|trainer.py:3081] 2023-07-28 17:43:26,719 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 17:43:26,721 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 17:43:26,722 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 17:44:12,644 >> Saving model checkpoint to /content/checkpoint-500\n","[INFO|configuration_utils.py:458] 2023-07-28 17:44:12,647 >> Configuration saved in /content/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 17:44:15,484 >> Model weights saved in /content/checkpoint-500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 17:44:24,225 >> Deleting older checkpoint [/content/checkpoint-3000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 17:45:10,041 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 17:45:10,042 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 17:45:10,044 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 17:45:56,121 >> Saving model checkpoint to /content/checkpoint-1000\n","[INFO|configuration_utils.py:458] 2023-07-28 17:45:56,124 >> Configuration saved in /content/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 17:46:02,047 >> Model weights saved in /content/checkpoint-1000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 17:46:08,898 >> Deleting older checkpoint [/content/checkpoint-3500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 17:46:54,646 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 17:46:54,647 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 17:46:54,650 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 17:47:40,734 >> Saving model checkpoint to /content/checkpoint-1500\n","[INFO|configuration_utils.py:458] 2023-07-28 17:47:40,737 >> Configuration saved in /content/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 17:47:41,843 >> Model weights saved in /content/checkpoint-1500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 17:47:45,629 >> Deleting older checkpoint [/content/checkpoint-500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 17:48:31,297 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 17:48:31,298 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 17:48:31,299 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 17:49:17,361 >> Saving model checkpoint to /content/checkpoint-2000\n","[INFO|configuration_utils.py:458] 2023-07-28 17:49:17,364 >> Configuration saved in /content/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 17:49:19,616 >> Model weights saved in /content/checkpoint-2000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 17:49:23,355 >> Deleting older checkpoint [/content/checkpoint-1000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 17:50:08,945 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 17:50:08,946 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 17:50:08,948 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 17:50:54,934 >> Saving model checkpoint to /content/checkpoint-2500\n","[INFO|configuration_utils.py:458] 2023-07-28 17:50:54,937 >> Configuration saved in /content/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 17:50:56,384 >> Model weights saved in /content/checkpoint-2500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 17:51:05,991 >> Deleting older checkpoint [/content/checkpoint-1500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 17:51:51,644 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 17:51:51,646 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 17:51:51,647 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 17:52:37,643 >> Saving model checkpoint to /content/checkpoint-3000\n","[INFO|configuration_utils.py:458] 2023-07-28 17:52:37,646 >> Configuration saved in /content/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 17:52:43,543 >> Model weights saved in /content/checkpoint-3000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 17:52:49,080 >> Deleting older checkpoint [/content/checkpoint-2000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 17:53:34,688 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 17:53:34,689 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 17:53:34,692 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 17:54:20,682 >> Saving model checkpoint to /content/checkpoint-3500\n","[INFO|configuration_utils.py:458] 2023-07-28 17:54:20,685 >> Configuration saved in /content/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 17:54:26,391 >> Model weights saved in /content/checkpoint-3500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 17:54:29,697 >> Deleting older checkpoint [/content/checkpoint-2500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 17:55:15,277 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 17:55:15,279 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 17:55:15,280 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 17:56:01,311 >> Saving model checkpoint to /content/checkpoint-4000\n","[INFO|configuration_utils.py:458] 2023-07-28 17:56:01,314 >> Configuration saved in /content/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 17:56:07,243 >> Model weights saved in /content/checkpoint-4000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 17:56:10,507 >> Deleting older checkpoint [/content/checkpoint-3000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 17:56:56,046 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 17:56:56,048 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 17:56:56,049 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 17:57:42,105 >> Saving model checkpoint to /content/checkpoint-4500\n","[INFO|configuration_utils.py:458] 2023-07-28 17:57:42,108 >> Configuration saved in /content/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 17:57:45,618 >> Model weights saved in /content/checkpoint-4500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 17:57:50,224 >> Deleting older checkpoint [/content/checkpoint-3500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 17:58:35,710 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 17:58:35,712 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 17:58:35,713 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 17:59:21,679 >> Saving model checkpoint to /content/checkpoint-5000\n","[INFO|configuration_utils.py:458] 2023-07-28 17:59:21,683 >> Configuration saved in /content/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 17:59:26,138 >> Model weights saved in /content/checkpoint-5000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 17:59:29,792 >> Deleting older checkpoint [/content/checkpoint-4000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 18:00:15,118 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:00:15,119 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:00:15,120 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 18:01:01,040 >> Saving model checkpoint to /content/checkpoint-5500\n","[INFO|configuration_utils.py:458] 2023-07-28 18:01:01,044 >> Configuration saved in /content/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:01:06,949 >> Model weights saved in /content/checkpoint-5500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 18:01:10,172 >> Deleting older checkpoint [/content/checkpoint-4500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 18:01:55,603 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:01:55,605 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:01:55,607 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 18:02:41,841 >> Saving model checkpoint to /content/checkpoint-6000\n","[INFO|configuration_utils.py:458] 2023-07-28 18:02:41,844 >> Configuration saved in /content/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:02:45,576 >> Model weights saved in /content/checkpoint-6000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 18:02:49,964 >> Deleting older checkpoint [/content/checkpoint-5500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 18:03:35,972 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:03:35,973 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:03:35,974 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 18:04:22,072 >> Saving model checkpoint to /content/checkpoint-6500\n","[INFO|configuration_utils.py:458] 2023-07-28 18:04:22,075 >> Configuration saved in /content/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:04:26,381 >> Model weights saved in /content/checkpoint-6500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 18:04:30,190 >> Deleting older checkpoint [/content/checkpoint-6000] due to args.save_total_limit\n","[INFO|trainer.py:1934] 2023-07-28 18:04:30,390 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2093] 2023-07-28 18:04:30,392 >> Loading best model from /content/checkpoint-5000 (score: 0.5999575421636008).\n","[INFO|trainer.py:2807] 2023-07-28 18:04:30,790 >> Saving model checkpoint to /content\n","[INFO|configuration_utils.py:458] 2023-07-28 18:04:30,794 >> Configuration saved in /content/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:04:33,146 >> Model weights saved in /content/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2210] 2023-07-28 18:04:33,151 >> tokenizer config file saved in /content/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2217] 2023-07-28 18:04:33,153 >> Special tokens file saved in /content/special_tokens_map.json\n","[INFO|trainer.py:3081] 2023-07-28 18:04:33,255 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:04:33,256 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:04:33,258 >>   Batch size = 2\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[INFO|trainer.py:3081] 2023-07-28 18:05:19,086 >> ***** Running Prediction *****\n","[INFO|trainer.py:3083] 2023-07-28 18:05:19,087 >>   Num examples = 3600\n","[INFO|trainer.py:3086] 2023-07-28 18:05:19,088 >>   Batch size = 2\n"]},{"name":"stdout","output_type":"stream","text":["***** eval metrics *****\n","  epoch                   =       0.29\n","  eval_loss               =     1.0481\n","  eval_macro-f1           =        0.6\n","  eval_micro-f1           =        0.6\n","  eval_runtime            = 0:00:45.81\n","  eval_samples            =       3900\n","  eval_samples_per_second =     85.125\n","  eval_steps_per_second   =     42.563\n","*** Evaluate ***\n","Best Hyperparameters: {'learning_rate': 1e-05, 'num_train_epochs': 1, 'per_device_eval_batch_size': 2, 'per_device_train_batch_size': 2}\n","Best Validation Macro-F1: 0.5999575421636008\n","***** predict metrics *****\n","  predict_loss               =     1.0839\n","  predict_macro-f1           =     0.5741\n","  predict_micro-f1           =     0.5742\n","  predict_runtime            = 0:00:46.28\n","  predict_samples            =       3600\n","  predict_samples_per_second =     77.782\n","  predict_steps_per_second   =     38.891\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False\n","[INFO|configuration_utils.py:712] 2023-07-28 18:06:06,247 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-28 18:06:06,249 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"finetuning_task\": \"case_hold\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|configuration_utils.py:712] 2023-07-28 18:06:06,295 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-28 18:06:06,297 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 18:06:06,300 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/vocab.txt\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 18:06:06,301 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer.json\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 18:06:06,303 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 18:06:06,304 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 18:06:06,306 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer_config.json\n","[INFO|configuration_utils.py:712] 2023-07-28 18:06:06,308 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-28 18:06:06,310 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|modeling_utils.py:2603] 2023-07-28 18:06:06,406 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/model.safetensors\n","[INFO|modeling_utils.py:3319] 2023-07-28 18:06:07,305 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMultipleChoice: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:3331] 2023-07-28 18:06:07,309 >> Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1500: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","[INFO|trainer.py:1686] 2023-07-28 18:06:27,529 >> ***** Running training *****\n","[INFO|trainer.py:1687] 2023-07-28 18:06:27,531 >>   Num examples = 45,000\n","[INFO|trainer.py:1688] 2023-07-28 18:06:27,532 >>   Num Epochs = 1\n","[INFO|trainer.py:1689] 2023-07-28 18:06:27,533 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:1692] 2023-07-28 18:06:27,534 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n","[INFO|trainer.py:1693] 2023-07-28 18:06:27,535 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1694] 2023-07-28 18:06:27,537 >>   Total optimization steps = 11,250\n","[INFO|trainer.py:1695] 2023-07-28 18:06:27,539 >>   Number of trainable parameters = 108,311,041\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6500' max='11250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 6500/11250 27:45 < 20:17, 3.90 it/s, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro-f1</th>\n","      <th>Micro-f1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>1.231400</td>\n","      <td>1.122095</td>\n","      <td>0.556678</td>\n","      <td>0.556667</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.112800</td>\n","      <td>1.076632</td>\n","      <td>0.576361</td>\n","      <td>0.576410</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>1.097000</td>\n","      <td>1.029327</td>\n","      <td>0.589759</td>\n","      <td>0.589744</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>1.047000</td>\n","      <td>1.013927</td>\n","      <td>0.597693</td>\n","      <td>0.597692</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>1.058500</td>\n","      <td>0.997230</td>\n","      <td>0.602222</td>\n","      <td>0.602308</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>1.010500</td>\n","      <td>1.015857</td>\n","      <td>0.599233</td>\n","      <td>0.599231</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>1.048800</td>\n","      <td>0.992870</td>\n","      <td>0.603630</td>\n","      <td>0.603590</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>1.037400</td>\n","      <td>1.004387</td>\n","      <td>0.608152</td>\n","      <td>0.608205</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>1.061100</td>\n","      <td>0.992145</td>\n","      <td>0.611446</td>\n","      <td>0.611538</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>1.021100</td>\n","      <td>0.976924</td>\n","      <td>0.613862</td>\n","      <td>0.613846</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>1.038400</td>\n","      <td>0.962572</td>\n","      <td>0.609157</td>\n","      <td>0.609231</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>1.091900</td>\n","      <td>0.961803</td>\n","      <td>0.603783</td>\n","      <td>0.603846</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>0.969500</td>\n","      <td>0.966806</td>\n","      <td>0.611136</td>\n","      <td>0.611282</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[INFO|trainer.py:3081] 2023-07-28 18:07:41,225 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:07:41,226 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:07:41,228 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 18:08:27,247 >> Saving model checkpoint to /content/checkpoint-500\n","[INFO|configuration_utils.py:458] 2023-07-28 18:08:27,250 >> Configuration saved in /content/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:08:33,257 >> Model weights saved in /content/checkpoint-500/pytorch_model.bin\n","[INFO|trainer.py:3081] 2023-07-28 18:09:54,259 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:09:54,260 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:09:54,263 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 18:10:40,312 >> Saving model checkpoint to /content/checkpoint-1000\n","[INFO|configuration_utils.py:458] 2023-07-28 18:10:40,315 >> Configuration saved in /content/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:10:45,913 >> Model weights saved in /content/checkpoint-1000/pytorch_model.bin\n","[INFO|trainer.py:3081] 2023-07-28 18:12:03,196 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:12:03,198 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:12:03,200 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 18:12:49,608 >> Saving model checkpoint to /content/checkpoint-1500\n","[INFO|configuration_utils.py:458] 2023-07-28 18:12:49,610 >> Configuration saved in /content/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:12:55,648 >> Model weights saved in /content/checkpoint-1500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 18:12:58,893 >> Deleting older checkpoint [/content/checkpoint-500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 18:14:13,064 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:14:13,065 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:14:13,066 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 18:14:59,159 >> Saving model checkpoint to /content/checkpoint-2000\n","[INFO|configuration_utils.py:458] 2023-07-28 18:14:59,163 >> Configuration saved in /content/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:15:00,279 >> Model weights saved in /content/checkpoint-2000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 18:15:03,738 >> Deleting older checkpoint [/content/checkpoint-1000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 18:16:17,843 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:16:17,845 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:16:17,846 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 18:17:04,055 >> Saving model checkpoint to /content/checkpoint-2500\n","[INFO|configuration_utils.py:458] 2023-07-28 18:17:04,058 >> Configuration saved in /content/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:17:05,216 >> Model weights saved in /content/checkpoint-2500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 18:17:08,659 >> Deleting older checkpoint [/content/checkpoint-1500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 18:18:22,951 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:18:22,953 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:18:22,954 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 18:19:09,136 >> Saving model checkpoint to /content/checkpoint-3000\n","[INFO|configuration_utils.py:458] 2023-07-28 18:19:09,139 >> Configuration saved in /content/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:19:14,995 >> Model weights saved in /content/checkpoint-3000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 18:19:18,389 >> Deleting older checkpoint [/content/checkpoint-2000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 18:20:32,636 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:20:32,637 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:20:32,638 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 18:21:18,803 >> Saving model checkpoint to /content/checkpoint-3500\n","[INFO|configuration_utils.py:458] 2023-07-28 18:21:18,806 >> Configuration saved in /content/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:21:19,914 >> Model weights saved in /content/checkpoint-3500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 18:21:23,360 >> Deleting older checkpoint [/content/checkpoint-2500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 18:22:37,598 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:22:37,599 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:22:37,601 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 18:23:23,745 >> Saving model checkpoint to /content/checkpoint-4000\n","[INFO|configuration_utils.py:458] 2023-07-28 18:23:23,749 >> Configuration saved in /content/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:23:24,872 >> Model weights saved in /content/checkpoint-4000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 18:23:28,436 >> Deleting older checkpoint [/content/checkpoint-3000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 18:24:42,593 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:24:42,594 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:24:42,597 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 18:25:28,714 >> Saving model checkpoint to /content/checkpoint-4500\n","[INFO|configuration_utils.py:458] 2023-07-28 18:25:28,717 >> Configuration saved in /content/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:25:34,713 >> Model weights saved in /content/checkpoint-4500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 18:25:37,934 >> Deleting older checkpoint [/content/checkpoint-3500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 18:26:52,146 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:26:52,147 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:26:52,148 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 18:27:38,279 >> Saving model checkpoint to /content/checkpoint-5000\n","[INFO|configuration_utils.py:458] 2023-07-28 18:27:38,282 >> Configuration saved in /content/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:27:40,687 >> Model weights saved in /content/checkpoint-5000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 18:27:44,346 >> Deleting older checkpoint [/content/checkpoint-4000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 18:28:58,567 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:28:58,568 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:28:58,569 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 18:29:44,828 >> Saving model checkpoint to /content/checkpoint-5500\n","[INFO|configuration_utils.py:458] 2023-07-28 18:29:44,832 >> Configuration saved in /content/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:29:50,853 >> Model weights saved in /content/checkpoint-5500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 18:29:54,049 >> Deleting older checkpoint [/content/checkpoint-4500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 18:31:08,290 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:31:08,291 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:31:08,294 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 18:31:54,483 >> Saving model checkpoint to /content/checkpoint-6000\n","[INFO|configuration_utils.py:458] 2023-07-28 18:31:54,486 >> Configuration saved in /content/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:31:56,419 >> Model weights saved in /content/checkpoint-6000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 18:32:02,816 >> Deleting older checkpoint [/content/checkpoint-5500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 18:33:17,001 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:33:17,002 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:33:17,003 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 18:34:03,247 >> Saving model checkpoint to /content/checkpoint-6500\n","[INFO|configuration_utils.py:458] 2023-07-28 18:34:03,249 >> Configuration saved in /content/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:34:09,224 >> Model weights saved in /content/checkpoint-6500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 18:34:12,467 >> Deleting older checkpoint [/content/checkpoint-6000] due to args.save_total_limit\n","[INFO|trainer.py:1934] 2023-07-28 18:34:12,736 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2093] 2023-07-28 18:34:12,738 >> Loading best model from /content/checkpoint-5000 (score: 0.6138615413816193).\n","[INFO|trainer.py:2807] 2023-07-28 18:34:13,112 >> Saving model checkpoint to /content\n","[INFO|configuration_utils.py:458] 2023-07-28 18:34:13,116 >> Configuration saved in /content/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:34:14,233 >> Model weights saved in /content/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2210] 2023-07-28 18:34:14,237 >> tokenizer config file saved in /content/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2217] 2023-07-28 18:34:14,238 >> Special tokens file saved in /content/special_tokens_map.json\n","[INFO|trainer.py:3081] 2023-07-28 18:34:14,307 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:34:14,308 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:34:14,309 >>   Batch size = 2\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[INFO|trainer.py:3081] 2023-07-28 18:35:00,261 >> ***** Running Prediction *****\n","[INFO|trainer.py:3083] 2023-07-28 18:35:00,262 >>   Num examples = 3600\n","[INFO|trainer.py:3086] 2023-07-28 18:35:00,263 >>   Batch size = 2\n"]},{"name":"stdout","output_type":"stream","text":["***** eval metrics *****\n","  epoch                   =       0.58\n","  eval_loss               =     0.9769\n","  eval_macro-f1           =     0.6139\n","  eval_micro-f1           =     0.6138\n","  eval_runtime            = 0:00:45.94\n","  eval_samples            =       3900\n","  eval_samples_per_second =     84.893\n","  eval_steps_per_second   =     42.446\n","*** Evaluate ***\n","Best Hyperparameters: {'learning_rate': 1e-05, 'num_train_epochs': 1, 'per_device_eval_batch_size': 2, 'per_device_train_batch_size': 4}\n","Best Validation Macro-F1: 0.6138615413816193\n","***** predict metrics *****\n","  predict_loss               =     1.0152\n","  predict_macro-f1           =     0.5833\n","  predict_micro-f1           =     0.5833\n","  predict_runtime            = 0:00:46.27\n","  predict_samples            =       3600\n","  predict_samples_per_second =       77.8\n","  predict_steps_per_second   =       38.9\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False\n","[INFO|configuration_utils.py:712] 2023-07-28 18:35:47,315 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-28 18:35:47,319 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"finetuning_task\": \"case_hold\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|configuration_utils.py:712] 2023-07-28 18:35:47,367 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-28 18:35:47,369 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 18:35:47,373 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/vocab.txt\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 18:35:47,374 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer.json\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 18:35:47,375 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 18:35:47,376 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 18:35:47,377 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer_config.json\n","[INFO|configuration_utils.py:712] 2023-07-28 18:35:47,382 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-28 18:35:47,385 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|modeling_utils.py:2603] 2023-07-28 18:35:47,468 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/model.safetensors\n","[INFO|modeling_utils.py:3319] 2023-07-28 18:35:48,356 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMultipleChoice: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:3331] 2023-07-28 18:35:48,357 >> Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1500: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","[INFO|trainer.py:1686] 2023-07-28 18:36:09,538 >> ***** Running training *****\n","[INFO|trainer.py:1687] 2023-07-28 18:36:09,539 >>   Num examples = 45,000\n","[INFO|trainer.py:1688] 2023-07-28 18:36:09,540 >>   Num Epochs = 1\n","[INFO|trainer.py:1689] 2023-07-28 18:36:09,542 >>   Instantaneous batch size per device = 2\n","[INFO|trainer.py:1692] 2023-07-28 18:36:09,543 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n","[INFO|trainer.py:1693] 2023-07-28 18:36:09,544 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1694] 2023-07-28 18:36:09,545 >>   Total optimization steps = 22,500\n","[INFO|trainer.py:1695] 2023-07-28 18:36:09,549 >>   Number of trainable parameters = 108,311,041\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6500' max='22500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 6500/22500 21:01 < 51:45, 5.15 it/s, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro-f1</th>\n","      <th>Micro-f1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>1.249000</td>\n","      <td>1.175207</td>\n","      <td>0.537423</td>\n","      <td>0.537436</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.182800</td>\n","      <td>1.127146</td>\n","      <td>0.555552</td>\n","      <td>0.555641</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>1.067400</td>\n","      <td>1.097464</td>\n","      <td>0.571301</td>\n","      <td>0.571282</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>1.074200</td>\n","      <td>1.060058</td>\n","      <td>0.569259</td>\n","      <td>0.569487</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>1.074900</td>\n","      <td>1.102692</td>\n","      <td>0.579966</td>\n","      <td>0.580000</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>1.136200</td>\n","      <td>1.097008</td>\n","      <td>0.584679</td>\n","      <td>0.584615</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>1.123000</td>\n","      <td>1.120891</td>\n","      <td>0.583792</td>\n","      <td>0.583846</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>1.132700</td>\n","      <td>1.059442</td>\n","      <td>0.596483</td>\n","      <td>0.596667</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>1.061800</td>\n","      <td>1.065231</td>\n","      <td>0.598838</td>\n","      <td>0.598974</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>1.138100</td>\n","      <td>1.048116</td>\n","      <td>0.599958</td>\n","      <td>0.600000</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>1.020800</td>\n","      <td>1.055843</td>\n","      <td>0.594582</td>\n","      <td>0.594615</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>1.031900</td>\n","      <td>1.089457</td>\n","      <td>0.597879</td>\n","      <td>0.597949</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>1.106000</td>\n","      <td>1.062823</td>\n","      <td>0.590022</td>\n","      <td>0.590000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[INFO|trainer.py:3081] 2023-07-28 18:36:53,856 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:36:53,857 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:36:53,858 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 18:37:36,138 >> Saving model checkpoint to /content/checkpoint-500\n","[INFO|configuration_utils.py:458] 2023-07-28 18:37:36,143 >> Configuration saved in /content/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:37:42,158 >> Model weights saved in /content/checkpoint-500/pytorch_model.bin\n","[INFO|trainer.py:3081] 2023-07-28 18:38:31,367 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:38:31,368 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:38:31,371 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 18:39:13,728 >> Saving model checkpoint to /content/checkpoint-1000\n","[INFO|configuration_utils.py:458] 2023-07-28 18:39:13,732 >> Configuration saved in /content/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:39:14,786 >> Model weights saved in /content/checkpoint-1000/pytorch_model.bin\n","[INFO|trainer.py:3081] 2023-07-28 18:40:10,795 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:40:10,796 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:40:10,800 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 18:40:53,207 >> Saving model checkpoint to /content/checkpoint-1500\n","[INFO|configuration_utils.py:458] 2023-07-28 18:40:53,211 >> Configuration saved in /content/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:40:58,252 >> Model weights saved in /content/checkpoint-1500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 18:41:01,673 >> Deleting older checkpoint [/content/checkpoint-500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 18:41:47,822 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:41:47,823 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:41:47,830 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 18:42:30,101 >> Saving model checkpoint to /content/checkpoint-2000\n","[INFO|configuration_utils.py:458] 2023-07-28 18:42:30,105 >> Configuration saved in /content/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:42:31,229 >> Model weights saved in /content/checkpoint-2000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 18:42:34,682 >> Deleting older checkpoint [/content/checkpoint-1000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 18:43:20,624 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:43:20,625 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:43:20,627 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 18:44:02,902 >> Saving model checkpoint to /content/checkpoint-2500\n","[INFO|configuration_utils.py:458] 2023-07-28 18:44:02,905 >> Configuration saved in /content/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:44:04,017 >> Model weights saved in /content/checkpoint-2500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 18:44:07,465 >> Deleting older checkpoint [/content/checkpoint-1500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 18:44:53,676 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:44:53,677 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:44:53,679 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 18:45:36,070 >> Saving model checkpoint to /content/checkpoint-3000\n","[INFO|configuration_utils.py:458] 2023-07-28 18:45:36,074 >> Configuration saved in /content/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:45:37,199 >> Model weights saved in /content/checkpoint-3000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 18:45:40,648 >> Deleting older checkpoint [/content/checkpoint-2000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 18:46:26,637 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:46:26,639 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:46:26,642 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 18:47:09,051 >> Saving model checkpoint to /content/checkpoint-3500\n","[INFO|configuration_utils.py:458] 2023-07-28 18:47:09,054 >> Configuration saved in /content/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:47:15,165 >> Model weights saved in /content/checkpoint-3500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 18:47:18,433 >> Deleting older checkpoint [/content/checkpoint-2500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 18:48:04,461 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:48:04,463 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:48:04,464 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 18:48:46,969 >> Saving model checkpoint to /content/checkpoint-4000\n","[INFO|configuration_utils.py:458] 2023-07-28 18:48:46,973 >> Configuration saved in /content/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:48:53,062 >> Model weights saved in /content/checkpoint-4000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 18:48:56,573 >> Deleting older checkpoint [/content/checkpoint-3000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 18:49:42,662 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:49:42,663 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:49:42,664 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 18:50:25,230 >> Saving model checkpoint to /content/checkpoint-4500\n","[INFO|configuration_utils.py:458] 2023-07-28 18:50:25,234 >> Configuration saved in /content/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:50:31,264 >> Model weights saved in /content/checkpoint-4500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 18:50:34,573 >> Deleting older checkpoint [/content/checkpoint-3500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 18:51:21,223 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:51:21,225 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:51:21,227 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 18:52:03,786 >> Saving model checkpoint to /content/checkpoint-5000\n","[INFO|configuration_utils.py:458] 2023-07-28 18:52:03,789 >> Configuration saved in /content/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:52:09,872 >> Model weights saved in /content/checkpoint-5000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 18:52:13,166 >> Deleting older checkpoint [/content/checkpoint-4000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 18:52:59,921 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:52:59,922 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:52:59,925 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 18:53:42,579 >> Saving model checkpoint to /content/checkpoint-5500\n","[INFO|configuration_utils.py:458] 2023-07-28 18:53:42,583 >> Configuration saved in /content/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:53:47,831 >> Model weights saved in /content/checkpoint-5500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 18:53:51,144 >> Deleting older checkpoint [/content/checkpoint-4500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 18:54:38,031 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:54:38,032 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:54:38,034 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 18:55:20,810 >> Saving model checkpoint to /content/checkpoint-6000\n","[INFO|configuration_utils.py:458] 2023-07-28 18:55:20,813 >> Configuration saved in /content/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:55:26,567 >> Model weights saved in /content/checkpoint-6000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 18:55:29,815 >> Deleting older checkpoint [/content/checkpoint-5500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 18:56:16,593 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:56:16,594 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:56:16,597 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 18:56:59,296 >> Saving model checkpoint to /content/checkpoint-6500\n","[INFO|configuration_utils.py:458] 2023-07-28 18:56:59,299 >> Configuration saved in /content/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:57:05,284 >> Model weights saved in /content/checkpoint-6500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 18:57:08,653 >> Deleting older checkpoint [/content/checkpoint-6000] due to args.save_total_limit\n","[INFO|trainer.py:1934] 2023-07-28 18:57:08,866 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2093] 2023-07-28 18:57:08,869 >> Loading best model from /content/checkpoint-5000 (score: 0.5999575421636008).\n","[INFO|trainer.py:2807] 2023-07-28 18:57:10,800 >> Saving model checkpoint to /content\n","[INFO|configuration_utils.py:458] 2023-07-28 18:57:10,804 >> Configuration saved in /content/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 18:57:12,020 >> Model weights saved in /content/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2210] 2023-07-28 18:57:12,023 >> tokenizer config file saved in /content/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2217] 2023-07-28 18:57:12,026 >> Special tokens file saved in /content/special_tokens_map.json\n","[INFO|trainer.py:3081] 2023-07-28 18:57:12,107 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 18:57:12,108 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 18:57:12,109 >>   Batch size = 4\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[INFO|trainer.py:3081] 2023-07-28 18:57:54,694 >> ***** Running Prediction *****\n","[INFO|trainer.py:3083] 2023-07-28 18:57:54,695 >>   Num examples = 3600\n","[INFO|trainer.py:3086] 2023-07-28 18:57:54,696 >>   Batch size = 4\n"]},{"name":"stdout","output_type":"stream","text":["***** eval metrics *****\n","  epoch                   =       0.29\n","  eval_loss               =     1.0481\n","  eval_macro-f1           =        0.6\n","  eval_micro-f1           =        0.6\n","  eval_runtime            = 0:00:42.57\n","  eval_samples            =       3900\n","  eval_samples_per_second =     91.595\n","  eval_steps_per_second   =     22.899\n","*** Evaluate ***\n","Best Hyperparameters: {'learning_rate': 1e-05, 'num_train_epochs': 1, 'per_device_eval_batch_size': 4, 'per_device_train_batch_size': 2}\n","Best Validation Macro-F1: 0.5999575421636008\n","***** predict metrics *****\n","  predict_loss               =     1.0839\n","  predict_macro-f1           =     0.5741\n","  predict_micro-f1           =     0.5742\n","  predict_runtime            = 0:00:41.55\n","  predict_samples            =       3600\n","  predict_samples_per_second =     86.625\n","  predict_steps_per_second   =     21.656\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False\n","[INFO|configuration_utils.py:712] 2023-07-28 18:58:36,941 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-28 18:58:36,944 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"finetuning_task\": \"case_hold\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|configuration_utils.py:712] 2023-07-28 18:58:36,992 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-28 18:58:36,993 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 18:58:36,996 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/vocab.txt\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 18:58:36,998 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer.json\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 18:58:36,999 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 18:58:37,000 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 18:58:37,001 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer_config.json\n","[INFO|configuration_utils.py:712] 2023-07-28 18:58:37,003 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-28 18:58:37,006 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|modeling_utils.py:2603] 2023-07-28 18:58:37,084 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/model.safetensors\n","[INFO|modeling_utils.py:3319] 2023-07-28 18:58:37,966 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMultipleChoice: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:3331] 2023-07-28 18:58:37,968 >> Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1500: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","[INFO|trainer.py:1686] 2023-07-28 18:59:00,658 >> ***** Running training *****\n","[INFO|trainer.py:1687] 2023-07-28 18:59:00,659 >>   Num examples = 45,000\n","[INFO|trainer.py:1688] 2023-07-28 18:59:00,660 >>   Num Epochs = 1\n","[INFO|trainer.py:1689] 2023-07-28 18:59:00,664 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:1692] 2023-07-28 18:59:00,665 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n","[INFO|trainer.py:1693] 2023-07-28 18:59:00,667 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1694] 2023-07-28 18:59:00,668 >>   Total optimization steps = 11,250\n","[INFO|trainer.py:1695] 2023-07-28 18:59:00,670 >>   Number of trainable parameters = 108,311,041\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6500' max='11250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 6500/11250 27:33 < 20:08, 3.93 it/s, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro-f1</th>\n","      <th>Micro-f1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>1.231400</td>\n","      <td>1.122095</td>\n","      <td>0.556678</td>\n","      <td>0.556667</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.112800</td>\n","      <td>1.076632</td>\n","      <td>0.576361</td>\n","      <td>0.576410</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>1.097000</td>\n","      <td>1.029327</td>\n","      <td>0.589759</td>\n","      <td>0.589744</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>1.047000</td>\n","      <td>1.013927</td>\n","      <td>0.597693</td>\n","      <td>0.597692</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>1.058500</td>\n","      <td>0.997230</td>\n","      <td>0.602222</td>\n","      <td>0.602308</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>1.010500</td>\n","      <td>1.015857</td>\n","      <td>0.599233</td>\n","      <td>0.599231</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>1.048800</td>\n","      <td>0.992870</td>\n","      <td>0.603630</td>\n","      <td>0.603590</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>1.037400</td>\n","      <td>1.004387</td>\n","      <td>0.608152</td>\n","      <td>0.608205</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>1.061100</td>\n","      <td>0.992145</td>\n","      <td>0.611446</td>\n","      <td>0.611538</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>1.021100</td>\n","      <td>0.976924</td>\n","      <td>0.613862</td>\n","      <td>0.613846</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>1.038400</td>\n","      <td>0.962572</td>\n","      <td>0.609157</td>\n","      <td>0.609231</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>1.091900</td>\n","      <td>0.961803</td>\n","      <td>0.603783</td>\n","      <td>0.603846</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>0.969500</td>\n","      <td>0.966806</td>\n","      <td>0.611136</td>\n","      <td>0.611282</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[INFO|trainer.py:3081] 2023-07-28 19:00:14,943 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:00:14,944 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:00:14,947 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 19:00:57,771 >> Saving model checkpoint to /content/checkpoint-500\n","[INFO|configuration_utils.py:458] 2023-07-28 19:00:57,774 >> Configuration saved in /content/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:01:03,889 >> Model weights saved in /content/checkpoint-500/pytorch_model.bin\n","[INFO|trainer.py:3081] 2023-07-28 19:02:21,899 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:02:21,901 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:02:21,903 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 19:03:04,759 >> Saving model checkpoint to /content/checkpoint-1000\n","[INFO|configuration_utils.py:458] 2023-07-28 19:03:04,763 >> Configuration saved in /content/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:03:06,043 >> Model weights saved in /content/checkpoint-1000/pytorch_model.bin\n","[INFO|trainer.py:3081] 2023-07-28 19:04:24,134 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:04:24,136 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:04:24,138 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 19:05:06,978 >> Saving model checkpoint to /content/checkpoint-1500\n","[INFO|configuration_utils.py:458] 2023-07-28 19:05:06,982 >> Configuration saved in /content/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:05:12,144 >> Model weights saved in /content/checkpoint-1500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 19:05:15,782 >> Deleting older checkpoint [/content/checkpoint-500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 19:06:30,567 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:06:30,569 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:06:30,570 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 19:07:13,417 >> Saving model checkpoint to /content/checkpoint-2000\n","[INFO|configuration_utils.py:458] 2023-07-28 19:07:13,421 >> Configuration saved in /content/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:07:18,814 >> Model weights saved in /content/checkpoint-2000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 19:07:22,122 >> Deleting older checkpoint [/content/checkpoint-1000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 19:08:36,917 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:08:36,919 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:08:36,921 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 19:09:19,821 >> Saving model checkpoint to /content/checkpoint-2500\n","[INFO|configuration_utils.py:458] 2023-07-28 19:09:19,824 >> Configuration saved in /content/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:09:26,115 >> Model weights saved in /content/checkpoint-2500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 19:09:29,380 >> Deleting older checkpoint [/content/checkpoint-1500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 19:10:44,160 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:10:44,161 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:10:44,163 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 19:11:27,068 >> Saving model checkpoint to /content/checkpoint-3000\n","[INFO|configuration_utils.py:458] 2023-07-28 19:11:27,071 >> Configuration saved in /content/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:11:33,150 >> Model weights saved in /content/checkpoint-3000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 19:11:36,531 >> Deleting older checkpoint [/content/checkpoint-2000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 19:12:51,385 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:12:51,386 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:12:51,388 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 19:13:34,266 >> Saving model checkpoint to /content/checkpoint-3500\n","[INFO|configuration_utils.py:458] 2023-07-28 19:13:34,270 >> Configuration saved in /content/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:13:40,473 >> Model weights saved in /content/checkpoint-3500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 19:13:43,747 >> Deleting older checkpoint [/content/checkpoint-2500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 19:14:58,716 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:14:58,717 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:14:58,718 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 19:15:41,615 >> Saving model checkpoint to /content/checkpoint-4000\n","[INFO|configuration_utils.py:458] 2023-07-28 19:15:41,618 >> Configuration saved in /content/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:15:47,119 >> Model weights saved in /content/checkpoint-4000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 19:15:50,436 >> Deleting older checkpoint [/content/checkpoint-3000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 19:17:05,289 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:17:05,290 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:17:05,291 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 19:17:48,155 >> Saving model checkpoint to /content/checkpoint-4500\n","[INFO|configuration_utils.py:458] 2023-07-28 19:17:48,159 >> Configuration saved in /content/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:17:54,359 >> Model weights saved in /content/checkpoint-4500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 19:17:59,597 >> Deleting older checkpoint [/content/checkpoint-3500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 19:19:14,456 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:19:14,460 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:19:14,462 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 19:19:57,451 >> Saving model checkpoint to /content/checkpoint-5000\n","[INFO|configuration_utils.py:458] 2023-07-28 19:19:57,455 >> Configuration saved in /content/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:20:03,503 >> Model weights saved in /content/checkpoint-5000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 19:20:06,914 >> Deleting older checkpoint [/content/checkpoint-4000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 19:21:21,870 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:21:21,872 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:21:21,873 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 19:22:04,828 >> Saving model checkpoint to /content/checkpoint-5500\n","[INFO|configuration_utils.py:458] 2023-07-28 19:22:04,833 >> Configuration saved in /content/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:22:11,048 >> Model weights saved in /content/checkpoint-5500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 19:22:14,298 >> Deleting older checkpoint [/content/checkpoint-4500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 19:23:29,353 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:23:29,355 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:23:29,356 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 19:24:12,308 >> Saving model checkpoint to /content/checkpoint-6000\n","[INFO|configuration_utils.py:458] 2023-07-28 19:24:12,312 >> Configuration saved in /content/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:24:17,805 >> Model weights saved in /content/checkpoint-6000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 19:24:21,110 >> Deleting older checkpoint [/content/checkpoint-5500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 19:25:36,311 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:25:36,313 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:25:36,314 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 19:26:19,288 >> Saving model checkpoint to /content/checkpoint-6500\n","[INFO|configuration_utils.py:458] 2023-07-28 19:26:19,292 >> Configuration saved in /content/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:26:25,412 >> Model weights saved in /content/checkpoint-6500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 19:26:32,025 >> Deleting older checkpoint [/content/checkpoint-6000] due to args.save_total_limit\n","[INFO|trainer.py:1934] 2023-07-28 19:26:32,250 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2093] 2023-07-28 19:26:32,251 >> Loading best model from /content/checkpoint-5000 (score: 0.6138615413816193).\n","[INFO|trainer.py:2807] 2023-07-28 19:26:34,389 >> Saving model checkpoint to /content\n","[INFO|configuration_utils.py:458] 2023-07-28 19:26:34,392 >> Configuration saved in /content/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:26:35,668 >> Model weights saved in /content/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2210] 2023-07-28 19:26:35,674 >> tokenizer config file saved in /content/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2217] 2023-07-28 19:26:35,676 >> Special tokens file saved in /content/special_tokens_map.json\n","[INFO|trainer.py:3081] 2023-07-28 19:26:35,791 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:26:35,792 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:26:35,793 >>   Batch size = 4\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[INFO|trainer.py:3081] 2023-07-28 19:27:18,590 >> ***** Running Prediction *****\n","[INFO|trainer.py:3083] 2023-07-28 19:27:18,591 >>   Num examples = 3600\n","[INFO|trainer.py:3086] 2023-07-28 19:27:18,596 >>   Batch size = 4\n"]},{"name":"stdout","output_type":"stream","text":["***** eval metrics *****\n","  epoch                   =       0.58\n","  eval_loss               =     0.9769\n","  eval_macro-f1           =     0.6139\n","  eval_micro-f1           =     0.6138\n","  eval_runtime            = 0:00:42.79\n","  eval_samples            =       3900\n","  eval_samples_per_second =     91.141\n","  eval_steps_per_second   =     22.785\n","*** Evaluate ***\n","Best Hyperparameters: {'learning_rate': 1e-05, 'num_train_epochs': 1, 'per_device_eval_batch_size': 4, 'per_device_train_batch_size': 4}\n","Best Validation Macro-F1: 0.6138615413816193\n","***** predict metrics *****\n","  predict_loss               =     1.0152\n","  predict_macro-f1           =     0.5833\n","  predict_micro-f1           =     0.5833\n","  predict_runtime            = 0:00:41.86\n","  predict_samples            =       3600\n","  predict_samples_per_second =     85.981\n","  predict_steps_per_second   =     21.495\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False\n","[INFO|configuration_utils.py:712] 2023-07-28 19:28:01,120 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-28 19:28:01,123 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"finetuning_task\": \"case_hold\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|configuration_utils.py:712] 2023-07-28 19:28:01,172 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-28 19:28:01,174 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 19:28:01,177 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/vocab.txt\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 19:28:01,179 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer.json\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 19:28:01,180 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 19:28:01,183 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 19:28:01,184 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer_config.json\n","[INFO|configuration_utils.py:712] 2023-07-28 19:28:01,187 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-28 19:28:01,190 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|modeling_utils.py:2603] 2023-07-28 19:28:01,257 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/model.safetensors\n","[INFO|modeling_utils.py:3319] 2023-07-28 19:28:02,224 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMultipleChoice: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:3331] 2023-07-28 19:28:02,226 >> Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1500: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","[INFO|trainer.py:1686] 2023-07-28 19:28:26,433 >> ***** Running training *****\n","[INFO|trainer.py:1687] 2023-07-28 19:28:26,434 >>   Num examples = 45,000\n","[INFO|trainer.py:1688] 2023-07-28 19:28:26,435 >>   Num Epochs = 2\n","[INFO|trainer.py:1689] 2023-07-28 19:28:26,438 >>   Instantaneous batch size per device = 2\n","[INFO|trainer.py:1692] 2023-07-28 19:28:26,440 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n","[INFO|trainer.py:1693] 2023-07-28 19:28:26,442 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1694] 2023-07-28 19:28:26,444 >>   Total optimization steps = 45,000\n","[INFO|trainer.py:1695] 2023-07-28 19:28:26,446 >>   Number of trainable parameters = 108,311,041\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6500' max='45000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 6500/45000 22:32 < 2:13:31, 4.81 it/s, Epoch 0/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro-f1</th>\n","      <th>Micro-f1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>1.250100</td>\n","      <td>1.176715</td>\n","      <td>0.537964</td>\n","      <td>0.537949</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.182900</td>\n","      <td>1.108024</td>\n","      <td>0.557875</td>\n","      <td>0.557949</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>1.061200</td>\n","      <td>1.089585</td>\n","      <td>0.569759</td>\n","      <td>0.569744</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>1.076600</td>\n","      <td>1.072198</td>\n","      <td>0.570840</td>\n","      <td>0.571026</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>1.080500</td>\n","      <td>1.086128</td>\n","      <td>0.575130</td>\n","      <td>0.575128</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>1.145600</td>\n","      <td>1.101332</td>\n","      <td>0.588264</td>\n","      <td>0.588205</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>1.129000</td>\n","      <td>1.124533</td>\n","      <td>0.583514</td>\n","      <td>0.583590</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>1.124000</td>\n","      <td>1.057125</td>\n","      <td>0.592438</td>\n","      <td>0.592564</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>1.072500</td>\n","      <td>1.070488</td>\n","      <td>0.598559</td>\n","      <td>0.598718</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>1.146900</td>\n","      <td>1.032918</td>\n","      <td>0.601713</td>\n","      <td>0.601795</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>1.013800</td>\n","      <td>1.067168</td>\n","      <td>0.592741</td>\n","      <td>0.592821</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>1.036700</td>\n","      <td>1.108096</td>\n","      <td>0.596436</td>\n","      <td>0.596410</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>1.103400</td>\n","      <td>1.059869</td>\n","      <td>0.591796</td>\n","      <td>0.591795</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[INFO|trainer.py:3081] 2023-07-28 19:29:12,578 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:29:12,579 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:29:12,581 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 19:29:59,844 >> Saving model checkpoint to /content/checkpoint-500\n","[INFO|configuration_utils.py:458] 2023-07-28 19:29:59,848 >> Configuration saved in /content/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:30:05,941 >> Model weights saved in /content/checkpoint-500/pytorch_model.bin\n","[INFO|trainer.py:3081] 2023-07-28 19:30:56,790 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:30:56,792 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:30:56,793 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 19:31:44,076 >> Saving model checkpoint to /content/checkpoint-1000\n","[INFO|configuration_utils.py:458] 2023-07-28 19:31:44,081 >> Configuration saved in /content/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:31:45,625 >> Model weights saved in /content/checkpoint-1000/pytorch_model.bin\n","[INFO|trainer.py:3081] 2023-07-28 19:32:39,755 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:32:39,756 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:32:39,757 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 19:33:27,068 >> Saving model checkpoint to /content/checkpoint-1500\n","[INFO|configuration_utils.py:458] 2023-07-28 19:33:27,072 >> Configuration saved in /content/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:33:32,782 >> Model weights saved in /content/checkpoint-1500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 19:33:36,094 >> Deleting older checkpoint [/content/checkpoint-500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 19:34:23,615 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:34:23,617 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:34:23,618 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 19:35:10,947 >> Saving model checkpoint to /content/checkpoint-2000\n","[INFO|configuration_utils.py:458] 2023-07-28 19:35:10,950 >> Configuration saved in /content/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:35:16,591 >> Model weights saved in /content/checkpoint-2000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 19:35:19,899 >> Deleting older checkpoint [/content/checkpoint-1000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 19:36:07,334 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:36:07,336 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:36:07,337 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 19:36:54,724 >> Saving model checkpoint to /content/checkpoint-2500\n","[INFO|configuration_utils.py:458] 2023-07-28 19:36:54,727 >> Configuration saved in /content/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:37:00,405 >> Model weights saved in /content/checkpoint-2500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 19:37:03,705 >> Deleting older checkpoint [/content/checkpoint-1500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 19:37:51,148 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:37:51,151 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:37:51,154 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 19:38:38,549 >> Saving model checkpoint to /content/checkpoint-3000\n","[INFO|configuration_utils.py:458] 2023-07-28 19:38:38,553 >> Configuration saved in /content/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:38:43,434 >> Model weights saved in /content/checkpoint-3000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 19:38:49,924 >> Deleting older checkpoint [/content/checkpoint-2000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 19:39:37,342 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:39:37,343 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:39:37,345 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 19:40:24,734 >> Saving model checkpoint to /content/checkpoint-3500\n","[INFO|configuration_utils.py:458] 2023-07-28 19:40:24,738 >> Configuration saved in /content/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:40:30,393 >> Model weights saved in /content/checkpoint-3500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 19:40:33,733 >> Deleting older checkpoint [/content/checkpoint-2500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 19:41:21,272 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:41:21,274 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:41:21,276 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 19:42:08,702 >> Saving model checkpoint to /content/checkpoint-4000\n","[INFO|configuration_utils.py:458] 2023-07-28 19:42:08,705 >> Configuration saved in /content/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:42:10,064 >> Model weights saved in /content/checkpoint-4000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 19:42:13,512 >> Deleting older checkpoint [/content/checkpoint-3000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 19:43:01,131 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:43:01,132 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:43:01,133 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 19:43:48,589 >> Saving model checkpoint to /content/checkpoint-4500\n","[INFO|configuration_utils.py:458] 2023-07-28 19:43:48,592 >> Configuration saved in /content/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:43:54,740 >> Model weights saved in /content/checkpoint-4500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 19:44:00,414 >> Deleting older checkpoint [/content/checkpoint-3500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 19:44:47,849 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:44:47,851 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:44:47,853 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 19:45:35,322 >> Saving model checkpoint to /content/checkpoint-5000\n","[INFO|configuration_utils.py:458] 2023-07-28 19:45:35,325 >> Configuration saved in /content/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:45:40,201 >> Model weights saved in /content/checkpoint-5000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 19:45:43,653 >> Deleting older checkpoint [/content/checkpoint-4000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 19:46:31,316 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:46:31,317 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:46:31,318 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 19:47:18,744 >> Saving model checkpoint to /content/checkpoint-5500\n","[INFO|configuration_utils.py:458] 2023-07-28 19:47:18,751 >> Configuration saved in /content/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:47:25,020 >> Model weights saved in /content/checkpoint-5500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 19:47:31,066 >> Deleting older checkpoint [/content/checkpoint-4500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 19:48:18,737 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:48:18,738 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:48:18,740 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 19:49:06,205 >> Saving model checkpoint to /content/checkpoint-6000\n","[INFO|configuration_utils.py:458] 2023-07-28 19:49:06,208 >> Configuration saved in /content/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:49:07,576 >> Model weights saved in /content/checkpoint-6000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 19:49:11,031 >> Deleting older checkpoint [/content/checkpoint-5500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 19:49:59,026 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:49:59,027 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:49:59,028 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 19:50:46,536 >> Saving model checkpoint to /content/checkpoint-6500\n","[INFO|configuration_utils.py:458] 2023-07-28 19:50:46,539 >> Configuration saved in /content/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:50:52,704 >> Model weights saved in /content/checkpoint-6500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 19:50:56,557 >> Deleting older checkpoint [/content/checkpoint-6000] due to args.save_total_limit\n","[INFO|trainer.py:1934] 2023-07-28 19:50:56,768 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2093] 2023-07-28 19:50:56,769 >> Loading best model from /content/checkpoint-5000 (score: 0.6017131697028711).\n","[INFO|trainer.py:2807] 2023-07-28 19:50:58,838 >> Saving model checkpoint to /content\n","[INFO|configuration_utils.py:458] 2023-07-28 19:50:58,842 >> Configuration saved in /content/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:51:00,230 >> Model weights saved in /content/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2210] 2023-07-28 19:51:00,237 >> tokenizer config file saved in /content/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2217] 2023-07-28 19:51:00,239 >> Special tokens file saved in /content/special_tokens_map.json\n","[INFO|trainer.py:3081] 2023-07-28 19:51:00,347 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:51:00,349 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:51:00,350 >>   Batch size = 2\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[INFO|trainer.py:3081] 2023-07-28 19:51:47,764 >> ***** Running Prediction *****\n","[INFO|trainer.py:3083] 2023-07-28 19:51:47,765 >>   Num examples = 3600\n","[INFO|trainer.py:3086] 2023-07-28 19:51:47,767 >>   Batch size = 2\n"]},{"name":"stdout","output_type":"stream","text":["***** eval metrics *****\n","  epoch                   =       0.29\n","  eval_loss               =     1.0329\n","  eval_macro-f1           =     0.6017\n","  eval_micro-f1           =     0.6018\n","  eval_runtime            = 0:00:47.41\n","  eval_samples            =       3900\n","  eval_samples_per_second =     82.261\n","  eval_steps_per_second   =      41.13\n","*** Evaluate ***\n","Best Hyperparameters: {'learning_rate': 1e-05, 'num_train_epochs': 2, 'per_device_eval_batch_size': 2, 'per_device_train_batch_size': 2}\n","Best Validation Macro-F1: 0.6017131697028711\n","***** predict metrics *****\n","  predict_loss               =     1.0749\n","  predict_macro-f1           =     0.5753\n","  predict_micro-f1           =     0.5753\n","  predict_runtime            = 0:00:48.42\n","  predict_samples            =       3600\n","  predict_samples_per_second =     74.335\n","  predict_steps_per_second   =     37.167\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False\n","[INFO|configuration_utils.py:712] 2023-07-28 19:52:36,892 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-28 19:52:36,895 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"finetuning_task\": \"case_hold\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|configuration_utils.py:712] 2023-07-28 19:52:36,941 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-28 19:52:36,944 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 19:52:36,948 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/vocab.txt\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 19:52:36,949 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer.json\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 19:52:36,950 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 19:52:36,953 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 19:52:36,955 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer_config.json\n","[INFO|configuration_utils.py:712] 2023-07-28 19:52:36,957 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-28 19:52:36,959 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|modeling_utils.py:2603] 2023-07-28 19:52:37,043 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/model.safetensors\n","[INFO|modeling_utils.py:3319] 2023-07-28 19:52:38,082 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMultipleChoice: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:3331] 2023-07-28 19:52:38,083 >> Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1500: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","[INFO|trainer.py:1686] 2023-07-28 19:53:03,423 >> ***** Running training *****\n","[INFO|trainer.py:1687] 2023-07-28 19:53:03,424 >>   Num examples = 45,000\n","[INFO|trainer.py:1688] 2023-07-28 19:53:03,427 >>   Num Epochs = 2\n","[INFO|trainer.py:1689] 2023-07-28 19:53:03,428 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:1692] 2023-07-28 19:53:03,430 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n","[INFO|trainer.py:1693] 2023-07-28 19:53:03,432 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1694] 2023-07-28 19:53:03,433 >>   Total optimization steps = 22,500\n","[INFO|trainer.py:1695] 2023-07-28 19:53:03,437 >>   Number of trainable parameters = 108,311,041\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3500' max='22500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 3500/22500 15:31 < 1:24:19, 3.76 it/s, Epoch 0/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro-f1</th>\n","      <th>Micro-f1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>1.230800</td>\n","      <td>1.121775</td>\n","      <td>0.556928</td>\n","      <td>0.556923</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.112900</td>\n","      <td>1.074650</td>\n","      <td>0.576387</td>\n","      <td>0.576410</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>1.100300</td>\n","      <td>1.029728</td>\n","      <td>0.590315</td>\n","      <td>0.590256</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>1.050600</td>\n","      <td>1.012806</td>\n","      <td>0.601811</td>\n","      <td>0.601795</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>1.057300</td>\n","      <td>0.998161</td>\n","      <td>0.599428</td>\n","      <td>0.599487</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>1.015500</td>\n","      <td>1.017272</td>\n","      <td>0.601285</td>\n","      <td>0.601282</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>1.046600</td>\n","      <td>0.998847</td>\n","      <td>0.601803</td>\n","      <td>0.601795</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[INFO|trainer.py:3081] 2023-07-28 19:54:18,649 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:54:18,653 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:54:18,655 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 19:55:06,361 >> Saving model checkpoint to /content/checkpoint-500\n","[INFO|configuration_utils.py:458] 2023-07-28 19:55:06,364 >> Configuration saved in /content/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:55:12,513 >> Model weights saved in /content/checkpoint-500/pytorch_model.bin\n","[INFO|trainer.py:3081] 2023-07-28 19:56:32,390 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:56:32,392 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:56:32,395 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 19:57:20,071 >> Saving model checkpoint to /content/checkpoint-1000\n","[INFO|configuration_utils.py:458] 2023-07-28 19:57:20,075 >> Configuration saved in /content/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:57:26,401 >> Model weights saved in /content/checkpoint-1000/pytorch_model.bin\n","[INFO|trainer.py:3081] 2023-07-28 19:58:45,095 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 19:58:45,096 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 19:58:45,099 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 19:59:32,820 >> Saving model checkpoint to /content/checkpoint-1500\n","[INFO|configuration_utils.py:458] 2023-07-28 19:59:32,824 >> Configuration saved in /content/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 19:59:39,359 >> Model weights saved in /content/checkpoint-1500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 19:59:42,722 >> Deleting older checkpoint [/content/checkpoint-500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 20:00:58,250 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 20:00:58,252 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 20:00:58,254 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 20:01:46,037 >> Saving model checkpoint to /content/checkpoint-2000\n","[INFO|configuration_utils.py:458] 2023-07-28 20:01:46,041 >> Configuration saved in /content/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 20:01:52,529 >> Model weights saved in /content/checkpoint-2000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 20:01:55,974 >> Deleting older checkpoint [/content/checkpoint-1000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 20:03:11,567 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 20:03:11,568 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 20:03:11,569 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 20:03:59,427 >> Saving model checkpoint to /content/checkpoint-2500\n","[INFO|configuration_utils.py:458] 2023-07-28 20:03:59,431 >> Configuration saved in /content/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 20:04:05,894 >> Model weights saved in /content/checkpoint-2500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 20:04:09,215 >> Deleting older checkpoint [/content/checkpoint-1500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 20:05:24,653 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 20:05:24,655 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 20:05:24,656 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 20:06:12,511 >> Saving model checkpoint to /content/checkpoint-3000\n","[INFO|configuration_utils.py:458] 2023-07-28 20:06:12,514 >> Configuration saved in /content/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 20:06:13,960 >> Model weights saved in /content/checkpoint-3000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 20:06:20,039 >> Deleting older checkpoint [/content/checkpoint-2500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 20:07:35,795 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 20:07:35,797 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 20:07:35,798 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 20:08:23,680 >> Saving model checkpoint to /content/checkpoint-3500\n","[INFO|configuration_utils.py:458] 2023-07-28 20:08:23,684 >> Configuration saved in /content/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 20:08:29,548 >> Model weights saved in /content/checkpoint-3500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 20:08:32,879 >> Deleting older checkpoint [/content/checkpoint-3000] due to args.save_total_limit\n","[INFO|trainer.py:1934] 2023-07-28 20:08:32,986 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2093] 2023-07-28 20:08:32,988 >> Loading best model from /content/checkpoint-2000 (score: 0.6018114496654995).\n","[INFO|trainer.py:2807] 2023-07-28 20:08:35,077 >> Saving model checkpoint to /content\n","[INFO|configuration_utils.py:458] 2023-07-28 20:08:35,081 >> Configuration saved in /content/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 20:08:36,571 >> Model weights saved in /content/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2210] 2023-07-28 20:08:36,576 >> tokenizer config file saved in /content/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2217] 2023-07-28 20:08:36,579 >> Special tokens file saved in /content/special_tokens_map.json\n","[INFO|trainer.py:3081] 2023-07-28 20:08:36,688 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 20:08:36,690 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 20:08:36,692 >>   Batch size = 2\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[INFO|trainer.py:3081] 2023-07-28 20:09:24,336 >> ***** Running Prediction *****\n","[INFO|trainer.py:3083] 2023-07-28 20:09:24,337 >>   Num examples = 3600\n","[INFO|trainer.py:3086] 2023-07-28 20:09:24,342 >>   Batch size = 2\n"]},{"name":"stdout","output_type":"stream","text":["***** eval metrics *****\n","  epoch                   =       0.31\n","  eval_loss               =     1.0128\n","  eval_macro-f1           =     0.6018\n","  eval_micro-f1           =     0.6018\n","  eval_runtime            = 0:00:47.64\n","  eval_samples            =       3900\n","  eval_samples_per_second =     81.862\n","  eval_steps_per_second   =     40.931\n","*** Evaluate ***\n","Best Hyperparameters: {'learning_rate': 1e-05, 'num_train_epochs': 2, 'per_device_eval_batch_size': 2, 'per_device_train_batch_size': 4}\n","Best Validation Macro-F1: 0.6018114496654995\n","***** predict metrics *****\n","  predict_loss               =     1.0325\n","  predict_macro-f1           =     0.5769\n","  predict_micro-f1           =     0.5769\n","  predict_runtime            = 0:00:48.94\n","  predict_samples            =       3600\n","  predict_samples_per_second =      73.55\n","  predict_steps_per_second   =     36.775\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False\n","[INFO|configuration_utils.py:712] 2023-07-28 20:10:13,898 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-28 20:10:13,900 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"finetuning_task\": \"case_hold\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|configuration_utils.py:712] 2023-07-28 20:10:13,950 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-28 20:10:13,952 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 20:10:13,956 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/vocab.txt\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 20:10:13,957 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer.json\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 20:10:13,960 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 20:10:13,961 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 20:10:13,964 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer_config.json\n","[INFO|configuration_utils.py:712] 2023-07-28 20:10:13,967 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-28 20:10:13,971 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|modeling_utils.py:2603] 2023-07-28 20:10:14,043 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/model.safetensors\n","[INFO|modeling_utils.py:3319] 2023-07-28 20:10:15,007 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMultipleChoice: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:3331] 2023-07-28 20:10:15,008 >> Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1500: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","[INFO|trainer.py:1686] 2023-07-28 20:10:41,154 >> ***** Running training *****\n","[INFO|trainer.py:1687] 2023-07-28 20:10:41,155 >>   Num examples = 45,000\n","[INFO|trainer.py:1688] 2023-07-28 20:10:41,156 >>   Num Epochs = 2\n","[INFO|trainer.py:1689] 2023-07-28 20:10:41,160 >>   Instantaneous batch size per device = 2\n","[INFO|trainer.py:1692] 2023-07-28 20:10:41,162 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n","[INFO|trainer.py:1693] 2023-07-28 20:10:41,163 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1694] 2023-07-28 20:10:41,164 >>   Total optimization steps = 45,000\n","[INFO|trainer.py:1695] 2023-07-28 20:10:41,167 >>   Number of trainable parameters = 108,311,041\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6500' max='45000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 6500/45000 23:04 < 2:16:41, 4.69 it/s, Epoch 0/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro-f1</th>\n","      <th>Micro-f1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>1.250100</td>\n","      <td>1.176715</td>\n","      <td>0.537964</td>\n","      <td>0.537949</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.182900</td>\n","      <td>1.108024</td>\n","      <td>0.557875</td>\n","      <td>0.557949</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>1.061200</td>\n","      <td>1.089585</td>\n","      <td>0.569759</td>\n","      <td>0.569744</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>1.076600</td>\n","      <td>1.072198</td>\n","      <td>0.570840</td>\n","      <td>0.571026</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>1.080500</td>\n","      <td>1.086128</td>\n","      <td>0.575130</td>\n","      <td>0.575128</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>1.145600</td>\n","      <td>1.101332</td>\n","      <td>0.588264</td>\n","      <td>0.588205</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>1.129000</td>\n","      <td>1.124533</td>\n","      <td>0.583514</td>\n","      <td>0.583590</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>1.124000</td>\n","      <td>1.057124</td>\n","      <td>0.592438</td>\n","      <td>0.592564</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>1.072500</td>\n","      <td>1.070488</td>\n","      <td>0.598559</td>\n","      <td>0.598718</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>1.146900</td>\n","      <td>1.032918</td>\n","      <td>0.601713</td>\n","      <td>0.601795</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>1.013800</td>\n","      <td>1.067168</td>\n","      <td>0.592741</td>\n","      <td>0.592821</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>1.036700</td>\n","      <td>1.108096</td>\n","      <td>0.596436</td>\n","      <td>0.596410</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>1.103400</td>\n","      <td>1.059869</td>\n","      <td>0.591796</td>\n","      <td>0.591795</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[INFO|trainer.py:3081] 2023-07-28 20:11:27,366 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 20:11:27,367 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 20:11:27,369 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 20:12:10,465 >> Saving model checkpoint to /content/checkpoint-500\n","[INFO|configuration_utils.py:458] 2023-07-28 20:12:10,468 >> Configuration saved in /content/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 20:12:16,422 >> Model weights saved in /content/checkpoint-500/pytorch_model.bin\n","[INFO|trainer.py:3081] 2023-07-28 20:13:07,121 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 20:13:07,123 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 20:13:07,125 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 20:13:50,157 >> Saving model checkpoint to /content/checkpoint-1000\n","[INFO|configuration_utils.py:458] 2023-07-28 20:13:50,161 >> Configuration saved in /content/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 20:13:55,222 >> Model weights saved in /content/checkpoint-1000/pytorch_model.bin\n","[INFO|trainer.py:3081] 2023-07-28 20:14:45,858 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 20:14:45,859 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 20:14:45,862 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 20:15:28,953 >> Saving model checkpoint to /content/checkpoint-1500\n","[INFO|configuration_utils.py:458] 2023-07-28 20:15:28,957 >> Configuration saved in /content/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 20:15:30,433 >> Model weights saved in /content/checkpoint-1500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 20:15:33,932 >> Deleting older checkpoint [/content/checkpoint-500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 20:16:21,408 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 20:16:21,410 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 20:16:21,412 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 20:17:04,434 >> Saving model checkpoint to /content/checkpoint-2000\n","[INFO|configuration_utils.py:458] 2023-07-28 20:17:04,437 >> Configuration saved in /content/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 20:17:05,888 >> Model weights saved in /content/checkpoint-2000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 20:17:09,348 >> Deleting older checkpoint [/content/checkpoint-1000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 20:17:56,747 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 20:17:56,748 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 20:17:56,751 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 20:18:39,792 >> Saving model checkpoint to /content/checkpoint-2500\n","[INFO|configuration_utils.py:458] 2023-07-28 20:18:39,796 >> Configuration saved in /content/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 20:18:46,439 >> Model weights saved in /content/checkpoint-2500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 20:19:04,947 >> Deleting older checkpoint [/content/checkpoint-1500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 20:19:52,280 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 20:19:52,282 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 20:19:52,283 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 20:20:35,381 >> Saving model checkpoint to /content/checkpoint-3000\n","[INFO|configuration_utils.py:458] 2023-07-28 20:20:35,385 >> Configuration saved in /content/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 20:20:42,644 >> Model weights saved in /content/checkpoint-3000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 20:20:51,601 >> Deleting older checkpoint [/content/checkpoint-2000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 20:21:39,032 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 20:21:39,034 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 20:21:39,035 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 20:22:22,130 >> Saving model checkpoint to /content/checkpoint-3500\n","[INFO|configuration_utils.py:458] 2023-07-28 20:22:22,134 >> Configuration saved in /content/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 20:22:27,574 >> Model weights saved in /content/checkpoint-3500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 20:22:42,926 >> Deleting older checkpoint [/content/checkpoint-2500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 20:23:30,296 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 20:23:30,298 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 20:23:30,299 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 20:24:13,362 >> Saving model checkpoint to /content/checkpoint-4000\n","[INFO|configuration_utils.py:458] 2023-07-28 20:24:13,368 >> Configuration saved in /content/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 20:24:15,090 >> Model weights saved in /content/checkpoint-4000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 20:24:29,479 >> Deleting older checkpoint [/content/checkpoint-3000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 20:25:16,854 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 20:25:16,856 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 20:25:16,857 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 20:25:59,972 >> Saving model checkpoint to /content/checkpoint-4500\n","[INFO|configuration_utils.py:458] 2023-07-28 20:25:59,976 >> Configuration saved in /content/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 20:26:05,245 >> Model weights saved in /content/checkpoint-4500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 20:26:15,980 >> Deleting older checkpoint [/content/checkpoint-3500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 20:27:03,523 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 20:27:03,529 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 20:27:03,531 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 20:27:46,673 >> Saving model checkpoint to /content/checkpoint-5000\n","[INFO|configuration_utils.py:458] 2023-07-28 20:27:46,677 >> Configuration saved in /content/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 20:27:51,708 >> Model weights saved in /content/checkpoint-5000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 20:28:11,766 >> Deleting older checkpoint [/content/checkpoint-4000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 20:28:59,154 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 20:28:59,156 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 20:28:59,158 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 20:29:42,248 >> Saving model checkpoint to /content/checkpoint-5500\n","[INFO|configuration_utils.py:458] 2023-07-28 20:29:42,251 >> Configuration saved in /content/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 20:29:52,417 >> Model weights saved in /content/checkpoint-5500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 20:30:03,197 >> Deleting older checkpoint [/content/checkpoint-4500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 20:30:50,747 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 20:30:50,749 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 20:30:50,751 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 20:31:33,869 >> Saving model checkpoint to /content/checkpoint-6000\n","[INFO|configuration_utils.py:458] 2023-07-28 20:31:33,872 >> Configuration saved in /content/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 20:31:38,945 >> Model weights saved in /content/checkpoint-6000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 20:31:51,897 >> Deleting older checkpoint [/content/checkpoint-5500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 20:32:39,687 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 20:32:39,689 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 20:32:39,692 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 20:33:22,808 >> Saving model checkpoint to /content/checkpoint-6500\n","[INFO|configuration_utils.py:458] 2023-07-28 20:33:22,811 >> Configuration saved in /content/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 20:33:33,464 >> Model weights saved in /content/checkpoint-6500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 20:33:43,386 >> Deleting older checkpoint [/content/checkpoint-6000] due to args.save_total_limit\n","[INFO|trainer.py:1934] 2023-07-28 20:33:43,467 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2093] 2023-07-28 20:33:43,469 >> Loading best model from /content/checkpoint-5000 (score: 0.6017131697028711).\n","[INFO|trainer.py:2807] 2023-07-28 20:33:45,518 >> Saving model checkpoint to /content\n","[INFO|configuration_utils.py:458] 2023-07-28 20:33:45,526 >> Configuration saved in /content/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 20:33:53,405 >> Model weights saved in /content/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2210] 2023-07-28 20:33:53,410 >> tokenizer config file saved in /content/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2217] 2023-07-28 20:33:53,413 >> Special tokens file saved in /content/special_tokens_map.json\n","[INFO|trainer.py:3081] 2023-07-28 20:33:53,520 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 20:33:53,522 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 20:33:53,525 >>   Batch size = 4\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[INFO|trainer.py:3081] 2023-07-28 20:34:36,481 >> ***** Running Prediction *****\n","[INFO|trainer.py:3083] 2023-07-28 20:34:36,488 >>   Num examples = 3600\n","[INFO|trainer.py:3086] 2023-07-28 20:34:36,490 >>   Batch size = 4\n"]},{"name":"stdout","output_type":"stream","text":["***** eval metrics *****\n","  epoch                   =       0.29\n","  eval_loss               =     1.0329\n","  eval_macro-f1           =     0.6017\n","  eval_micro-f1           =     0.6018\n","  eval_runtime            = 0:00:42.94\n","  eval_samples            =       3900\n","  eval_samples_per_second =     90.807\n","  eval_steps_per_second   =     22.702\n","*** Evaluate ***\n","Best Hyperparameters: {'learning_rate': 1e-05, 'num_train_epochs': 2, 'per_device_eval_batch_size': 4, 'per_device_train_batch_size': 2}\n","Best Validation Macro-F1: 0.6017131697028711\n","***** predict metrics *****\n","  predict_loss               =     1.0749\n","  predict_macro-f1           =     0.5753\n","  predict_micro-f1           =     0.5753\n","  predict_runtime            = 0:00:42.21\n","  predict_samples            =       3600\n","  predict_samples_per_second =     85.282\n","  predict_steps_per_second   =     21.321\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False\n","[INFO|configuration_utils.py:712] 2023-07-28 20:35:19,295 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-28 20:35:19,298 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"finetuning_task\": \"case_hold\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|configuration_utils.py:712] 2023-07-28 20:35:19,348 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-28 20:35:19,350 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 20:35:19,354 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/vocab.txt\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 20:35:19,356 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer.json\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 20:35:19,357 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 20:35:19,358 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 20:35:19,363 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer_config.json\n","[INFO|configuration_utils.py:712] 2023-07-28 20:35:19,366 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-28 20:35:19,369 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|modeling_utils.py:2603] 2023-07-28 20:35:19,470 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/model.safetensors\n","[INFO|modeling_utils.py:3319] 2023-07-28 20:35:22,173 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMultipleChoice: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:3331] 2023-07-28 20:35:22,175 >> Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1500: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","[INFO|trainer.py:1686] 2023-07-28 20:35:50,191 >> ***** Running training *****\n","[INFO|trainer.py:1687] 2023-07-28 20:35:50,192 >>   Num examples = 45,000\n","[INFO|trainer.py:1688] 2023-07-28 20:35:50,193 >>   Num Epochs = 2\n","[INFO|trainer.py:1689] 2023-07-28 20:35:50,196 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:1692] 2023-07-28 20:35:50,199 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n","[INFO|trainer.py:1693] 2023-07-28 20:35:50,200 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1694] 2023-07-28 20:35:50,201 >>   Total optimization steps = 22,500\n","[INFO|trainer.py:1695] 2023-07-28 20:35:50,204 >>   Number of trainable parameters = 108,311,041\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3500' max='22500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 3500/22500 19:00 < 1:43:13, 3.07 it/s, Epoch 0/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro-f1</th>\n","      <th>Micro-f1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>1.230800</td>\n","      <td>1.121775</td>\n","      <td>0.556928</td>\n","      <td>0.556923</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.112900</td>\n","      <td>1.074649</td>\n","      <td>0.576387</td>\n","      <td>0.576410</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>1.100300</td>\n","      <td>1.029728</td>\n","      <td>0.590315</td>\n","      <td>0.590256</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>1.050600</td>\n","      <td>1.012806</td>\n","      <td>0.601811</td>\n","      <td>0.601795</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>1.057300</td>\n","      <td>0.998161</td>\n","      <td>0.599428</td>\n","      <td>0.599487</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>1.015500</td>\n","      <td>1.017272</td>\n","      <td>0.601285</td>\n","      <td>0.601282</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>1.046600</td>\n","      <td>0.998847</td>\n","      <td>0.601803</td>\n","      <td>0.601795</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[INFO|trainer.py:3081] 2023-07-28 20:37:06,156 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 20:37:06,157 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 20:37:06,158 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 20:37:49,355 >> Saving model checkpoint to /content/checkpoint-500\n","[INFO|configuration_utils.py:458] 2023-07-28 20:37:49,358 >> Configuration saved in /content/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 20:38:04,675 >> Model weights saved in /content/checkpoint-500/pytorch_model.bin\n","[INFO|trainer.py:3081] 2023-07-28 20:39:45,712 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 20:39:45,714 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 20:39:45,717 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 20:40:28,854 >> Saving model checkpoint to /content/checkpoint-1000\n","[INFO|configuration_utils.py:458] 2023-07-28 20:40:28,858 >> Configuration saved in /content/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 20:40:39,878 >> Model weights saved in /content/checkpoint-1000/pytorch_model.bin\n","[INFO|trainer.py:3081] 2023-07-28 20:42:32,337 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 20:42:32,342 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 20:42:32,344 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 20:43:15,484 >> Saving model checkpoint to /content/checkpoint-1500\n","[INFO|configuration_utils.py:458] 2023-07-28 20:43:15,487 >> Configuration saved in /content/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 20:43:31,150 >> Model weights saved in /content/checkpoint-1500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 20:44:01,777 >> Deleting older checkpoint [/content/checkpoint-500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 20:45:17,770 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 20:45:17,771 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 20:45:17,774 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 20:46:00,922 >> Saving model checkpoint to /content/checkpoint-2000\n","[INFO|configuration_utils.py:458] 2023-07-28 20:46:00,926 >> Configuration saved in /content/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 20:46:16,760 >> Model weights saved in /content/checkpoint-2000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 20:46:45,043 >> Deleting older checkpoint [/content/checkpoint-1000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 20:48:00,985 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 20:48:00,987 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 20:48:00,988 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 20:48:44,043 >> Saving model checkpoint to /content/checkpoint-2500\n","[INFO|configuration_utils.py:458] 2023-07-28 20:48:44,047 >> Configuration saved in /content/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 20:48:57,126 >> Model weights saved in /content/checkpoint-2500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 20:49:20,166 >> Deleting older checkpoint [/content/checkpoint-1500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 20:50:35,917 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 20:50:35,918 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 20:50:35,920 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 20:51:18,957 >> Saving model checkpoint to /content/checkpoint-3000\n","[INFO|configuration_utils.py:458] 2023-07-28 20:51:18,960 >> Configuration saved in /content/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 20:51:34,314 >> Model weights saved in /content/checkpoint-3000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 20:52:04,372 >> Deleting older checkpoint [/content/checkpoint-2500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 20:53:19,999 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 20:53:20,004 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 20:53:20,005 >>   Batch size = 4\n","[INFO|trainer.py:2807] 2023-07-28 20:54:03,091 >> Saving model checkpoint to /content/checkpoint-3500\n","[INFO|configuration_utils.py:458] 2023-07-28 20:54:03,094 >> Configuration saved in /content/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 20:54:18,541 >> Model weights saved in /content/checkpoint-3500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 20:54:48,528 >> Deleting older checkpoint [/content/checkpoint-3000] due to args.save_total_limit\n","[INFO|trainer.py:1934] 2023-07-28 20:54:48,557 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2093] 2023-07-28 20:54:48,560 >> Loading best model from /content/checkpoint-2000 (score: 0.6018114496654995).\n","[INFO|trainer.py:2807] 2023-07-28 20:54:50,623 >> Saving model checkpoint to /content\n","[INFO|configuration_utils.py:458] 2023-07-28 20:54:50,626 >> Configuration saved in /content/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 20:54:58,882 >> Model weights saved in /content/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2210] 2023-07-28 20:54:58,886 >> tokenizer config file saved in /content/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2217] 2023-07-28 20:54:58,889 >> Special tokens file saved in /content/special_tokens_map.json\n","[INFO|trainer.py:3081] 2023-07-28 20:54:58,995 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 20:54:58,996 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 20:54:58,998 >>   Batch size = 4\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[INFO|trainer.py:3081] 2023-07-28 20:55:41,751 >> ***** Running Prediction *****\n","[INFO|trainer.py:3083] 2023-07-28 20:55:41,753 >>   Num examples = 3600\n","[INFO|trainer.py:3086] 2023-07-28 20:55:41,754 >>   Batch size = 4\n"]},{"name":"stdout","output_type":"stream","text":["***** eval metrics *****\n","  epoch                   =       0.31\n","  eval_loss               =     1.0128\n","  eval_macro-f1           =     0.6018\n","  eval_micro-f1           =     0.6018\n","  eval_runtime            = 0:00:42.74\n","  eval_samples            =       3900\n","  eval_samples_per_second =     91.245\n","  eval_steps_per_second   =     22.811\n","*** Evaluate ***\n","Best Hyperparameters: {'learning_rate': 1e-05, 'num_train_epochs': 2, 'per_device_eval_batch_size': 4, 'per_device_train_batch_size': 4}\n","Best Validation Macro-F1: 0.6018114496654995\n","***** predict metrics *****\n","  predict_loss               =     1.0325\n","  predict_macro-f1           =     0.5769\n","  predict_micro-f1           =     0.5769\n","  predict_runtime            = 0:00:41.87\n","  predict_samples            =       3600\n","  predict_samples_per_second =      85.96\n","  predict_steps_per_second   =      21.49\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False\n","[INFO|configuration_utils.py:712] 2023-07-28 20:56:24,191 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-28 20:56:24,193 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"finetuning_task\": \"case_hold\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|configuration_utils.py:712] 2023-07-28 20:56:24,243 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-28 20:56:24,245 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 20:56:24,249 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/vocab.txt\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 20:56:24,250 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer.json\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 20:56:24,252 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 20:56:24,254 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 20:56:24,256 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer_config.json\n","[INFO|configuration_utils.py:712] 2023-07-28 20:56:24,259 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-28 20:56:24,261 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|modeling_utils.py:2603] 2023-07-28 20:56:24,377 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/model.safetensors\n","[INFO|modeling_utils.py:3319] 2023-07-28 20:56:27,678 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMultipleChoice: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:3331] 2023-07-28 20:56:27,680 >> Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1500: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","[INFO|trainer.py:1686] 2023-07-28 20:56:55,950 >> ***** Running training *****\n","[INFO|trainer.py:1687] 2023-07-28 20:56:55,951 >>   Num examples = 45,000\n","[INFO|trainer.py:1688] 2023-07-28 20:56:55,953 >>   Num Epochs = 1\n","[INFO|trainer.py:1689] 2023-07-28 20:56:55,957 >>   Instantaneous batch size per device = 2\n","[INFO|trainer.py:1692] 2023-07-28 20:56:55,958 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n","[INFO|trainer.py:1693] 2023-07-28 20:56:55,959 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1694] 2023-07-28 20:56:55,960 >>   Total optimization steps = 22,500\n","[INFO|trainer.py:1695] 2023-07-28 20:56:55,963 >>   Number of trainable parameters = 108,311,041\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6500' max='22500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 6500/22500 48:48 < 2:00:11, 2.22 it/s, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro-f1</th>\n","      <th>Micro-f1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>1.312300</td>\n","      <td>1.269219</td>\n","      <td>0.487191</td>\n","      <td>0.487179</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.262600</td>\n","      <td>1.145242</td>\n","      <td>0.538143</td>\n","      <td>0.538205</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>1.131200</td>\n","      <td>1.163530</td>\n","      <td>0.547455</td>\n","      <td>0.547436</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>1.155700</td>\n","      <td>1.135434</td>\n","      <td>0.545203</td>\n","      <td>0.545128</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>1.137200</td>\n","      <td>1.188412</td>\n","      <td>0.559467</td>\n","      <td>0.559487</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>1.223900</td>\n","      <td>1.165895</td>\n","      <td>0.570251</td>\n","      <td>0.570256</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>1.201200</td>\n","      <td>1.175446</td>\n","      <td>0.569712</td>\n","      <td>0.569744</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>1.152900</td>\n","      <td>1.088874</td>\n","      <td>0.576779</td>\n","      <td>0.576923</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>1.158400</td>\n","      <td>1.150989</td>\n","      <td>0.576416</td>\n","      <td>0.576410</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>1.152900</td>\n","      <td>1.082193</td>\n","      <td>0.584826</td>\n","      <td>0.584872</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>1.118800</td>\n","      <td>1.187607</td>\n","      <td>0.567172</td>\n","      <td>0.567179</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>1.112700</td>\n","      <td>1.209438</td>\n","      <td>0.582342</td>\n","      <td>0.582308</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>1.159400</td>\n","      <td>1.085511</td>\n","      <td>0.581076</td>\n","      <td>0.581026</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[INFO|trainer.py:3081] 2023-07-28 20:57:41,129 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 20:57:41,130 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 20:57:41,131 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 20:58:28,071 >> Saving model checkpoint to /content/checkpoint-500\n","[INFO|configuration_utils.py:458] 2023-07-28 20:58:28,074 >> Configuration saved in /content/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 20:59:07,505 >> Model weights saved in /content/checkpoint-500/pytorch_model.bin\n","[INFO|trainer.py:3081] 2023-07-28 21:01:08,722 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 21:01:08,726 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 21:01:08,727 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 21:01:55,796 >> Saving model checkpoint to /content/checkpoint-1000\n","[INFO|configuration_utils.py:458] 2023-07-28 21:01:55,799 >> Configuration saved in /content/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 21:02:31,505 >> Model weights saved in /content/checkpoint-1000/pytorch_model.bin\n","[INFO|trainer.py:3081] 2023-07-28 21:04:42,770 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 21:04:42,772 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 21:04:42,774 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 21:05:29,676 >> Saving model checkpoint to /content/checkpoint-1500\n","[INFO|configuration_utils.py:458] 2023-07-28 21:05:29,679 >> Configuration saved in /content/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 21:06:10,711 >> Model weights saved in /content/checkpoint-1500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 21:07:41,032 >> Deleting older checkpoint [/content/checkpoint-500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 21:08:26,953 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 21:08:26,955 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 21:08:26,957 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 21:09:13,859 >> Saving model checkpoint to /content/checkpoint-2000\n","[INFO|configuration_utils.py:458] 2023-07-28 21:09:13,862 >> Configuration saved in /content/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 21:09:54,423 >> Model weights saved in /content/checkpoint-2000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 21:11:16,532 >> Deleting older checkpoint [/content/checkpoint-1000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 21:12:02,448 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 21:12:02,449 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 21:12:02,453 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 21:12:49,241 >> Saving model checkpoint to /content/checkpoint-2500\n","[INFO|configuration_utils.py:458] 2023-07-28 21:12:49,244 >> Configuration saved in /content/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 21:13:37,253 >> Model weights saved in /content/checkpoint-2500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 21:15:14,931 >> Deleting older checkpoint [/content/checkpoint-1500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 21:16:00,953 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 21:16:00,955 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 21:16:00,957 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 21:16:47,679 >> Saving model checkpoint to /content/checkpoint-3000\n","[INFO|configuration_utils.py:458] 2023-07-28 21:16:47,682 >> Configuration saved in /content/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 21:17:32,758 >> Model weights saved in /content/checkpoint-3000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 21:19:07,834 >> Deleting older checkpoint [/content/checkpoint-2000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 21:19:53,902 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 21:19:53,903 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 21:19:53,906 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 21:20:40,665 >> Saving model checkpoint to /content/checkpoint-3500\n","[INFO|configuration_utils.py:458] 2023-07-28 21:20:40,668 >> Configuration saved in /content/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 21:21:20,750 >> Model weights saved in /content/checkpoint-3500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 21:22:45,833 >> Deleting older checkpoint [/content/checkpoint-2500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 21:23:31,785 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 21:23:31,786 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 21:23:31,789 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 21:24:18,482 >> Saving model checkpoint to /content/checkpoint-4000\n","[INFO|configuration_utils.py:458] 2023-07-28 21:24:18,485 >> Configuration saved in /content/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 21:25:08,385 >> Model weights saved in /content/checkpoint-4000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 21:26:44,933 >> Deleting older checkpoint [/content/checkpoint-3000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 21:27:30,819 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 21:27:30,820 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 21:27:30,823 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 21:28:17,575 >> Saving model checkpoint to /content/checkpoint-4500\n","[INFO|configuration_utils.py:458] 2023-07-28 21:28:17,579 >> Configuration saved in /content/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 21:28:59,459 >> Model weights saved in /content/checkpoint-4500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 21:30:32,709 >> Deleting older checkpoint [/content/checkpoint-3500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 21:31:18,578 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 21:31:18,580 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 21:31:18,581 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 21:32:05,230 >> Saving model checkpoint to /content/checkpoint-5000\n","[INFO|configuration_utils.py:458] 2023-07-28 21:32:05,233 >> Configuration saved in /content/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 21:32:47,449 >> Model weights saved in /content/checkpoint-5000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 21:34:20,424 >> Deleting older checkpoint [/content/checkpoint-4000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 21:35:06,445 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 21:35:06,446 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 21:35:06,450 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 21:35:53,066 >> Saving model checkpoint to /content/checkpoint-5500\n","[INFO|configuration_utils.py:458] 2023-07-28 21:35:53,069 >> Configuration saved in /content/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 21:36:38,134 >> Model weights saved in /content/checkpoint-5500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 21:38:14,947 >> Deleting older checkpoint [/content/checkpoint-4500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 21:39:00,809 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 21:39:00,811 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 21:39:00,813 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 21:39:47,409 >> Saving model checkpoint to /content/checkpoint-6000\n","[INFO|configuration_utils.py:458] 2023-07-28 21:39:47,412 >> Configuration saved in /content/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 21:40:29,750 >> Model weights saved in /content/checkpoint-6000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 21:41:59,682 >> Deleting older checkpoint [/content/checkpoint-5500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 21:42:45,473 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 21:42:45,474 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 21:42:45,477 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 21:43:32,079 >> Saving model checkpoint to /content/checkpoint-6500\n","[INFO|configuration_utils.py:458] 2023-07-28 21:43:32,082 >> Configuration saved in /content/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 21:44:14,915 >> Model weights saved in /content/checkpoint-6500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 21:45:42,284 >> Deleting older checkpoint [/content/checkpoint-6000] due to args.save_total_limit\n","[INFO|trainer.py:1934] 2023-07-28 21:45:42,310 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2093] 2023-07-28 21:45:42,312 >> Loading best model from /content/checkpoint-5000 (score: 0.5848256237608144).\n","[INFO|trainer.py:2807] 2023-07-28 21:45:44,796 >> Saving model checkpoint to /content\n","[INFO|configuration_utils.py:458] 2023-07-28 21:45:44,798 >> Configuration saved in /content/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 21:46:33,612 >> Model weights saved in /content/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2210] 2023-07-28 21:46:33,625 >> tokenizer config file saved in /content/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2217] 2023-07-28 21:46:33,627 >> Special tokens file saved in /content/special_tokens_map.json\n","[INFO|trainer.py:3081] 2023-07-28 21:46:34,182 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 21:46:34,183 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 21:46:34,184 >>   Batch size = 2\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[INFO|trainer.py:3081] 2023-07-28 21:47:20,638 >> ***** Running Prediction *****\n","[INFO|trainer.py:3083] 2023-07-28 21:47:20,639 >>   Num examples = 3600\n","[INFO|trainer.py:3086] 2023-07-28 21:47:20,642 >>   Batch size = 2\n"]},{"name":"stdout","output_type":"stream","text":["***** eval metrics *****\n","  epoch                   =       0.29\n","  eval_loss               =     1.0822\n","  eval_macro-f1           =     0.5848\n","  eval_micro-f1           =     0.5849\n","  eval_runtime            = 0:00:46.44\n","  eval_samples            =       3900\n","  eval_samples_per_second =     83.968\n","  eval_steps_per_second   =     41.984\n","*** Evaluate ***\n","Best Hyperparameters: {'learning_rate': 2e-05, 'num_train_epochs': 1, 'per_device_eval_batch_size': 2, 'per_device_train_batch_size': 2}\n","Best Validation Macro-F1: 0.5848256237608144\n","***** predict metrics *****\n","  predict_loss               =     1.1055\n","  predict_macro-f1           =     0.5674\n","  predict_micro-f1           =     0.5675\n","  predict_runtime            = 0:00:47.19\n","  predict_samples            =       3600\n","  predict_samples_per_second =     76.276\n","  predict_steps_per_second   =     38.138\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False\n","[INFO|configuration_utils.py:712] 2023-07-28 21:48:08,266 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-28 21:48:08,269 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"finetuning_task\": \"case_hold\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|configuration_utils.py:712] 2023-07-28 21:48:08,319 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-28 21:48:08,321 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 21:48:08,325 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/vocab.txt\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 21:48:08,327 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer.json\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 21:48:08,328 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 21:48:08,329 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1839] 2023-07-28 21:48:08,330 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer_config.json\n","[INFO|configuration_utils.py:712] 2023-07-28 21:48:08,334 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-28 21:48:08,337 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|modeling_utils.py:2603] 2023-07-28 21:48:08,401 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/model.safetensors\n","[INFO|modeling_utils.py:3319] 2023-07-28 21:48:11,785 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMultipleChoice: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:3331] 2023-07-28 21:48:11,787 >> Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1500: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","[INFO|trainer.py:1686] 2023-07-28 21:48:41,952 >> ***** Running training *****\n","[INFO|trainer.py:1687] 2023-07-28 21:48:41,954 >>   Num examples = 45,000\n","[INFO|trainer.py:1688] 2023-07-28 21:48:41,955 >>   Num Epochs = 1\n","[INFO|trainer.py:1689] 2023-07-28 21:48:41,958 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:1692] 2023-07-28 21:48:41,959 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n","[INFO|trainer.py:1693] 2023-07-28 21:48:41,961 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1694] 2023-07-28 21:48:41,962 >>   Total optimization steps = 11,250\n","[INFO|trainer.py:1695] 2023-07-28 21:48:41,968 >>   Number of trainable parameters = 108,311,041\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='9500' max='11250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 9500/11250 2:29:46 < 27:35, 1.06 it/s, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro-f1</th>\n","      <th>Micro-f1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>1.221900</td>\n","      <td>1.120786</td>\n","      <td>0.554679</td>\n","      <td>0.554615</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.115900</td>\n","      <td>1.061108</td>\n","      <td>0.571839</td>\n","      <td>0.571795</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>1.109200</td>\n","      <td>1.074857</td>\n","      <td>0.579729</td>\n","      <td>0.579744</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>1.068900</td>\n","      <td>1.020033</td>\n","      <td>0.590998</td>\n","      <td>0.591026</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>1.069800</td>\n","      <td>1.004960</td>\n","      <td>0.596109</td>\n","      <td>0.596154</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>1.025100</td>\n","      <td>0.998305</td>\n","      <td>0.600506</td>\n","      <td>0.600513</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>1.049400</td>\n","      <td>1.005086</td>\n","      <td>0.601465</td>\n","      <td>0.601538</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>1.051500</td>\n","      <td>1.066649</td>\n","      <td>0.600906</td>\n","      <td>0.601026</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>1.055300</td>\n","      <td>1.011513</td>\n","      <td>0.604806</td>\n","      <td>0.604872</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>1.010700</td>\n","      <td>0.989541</td>\n","      <td>0.605101</td>\n","      <td>0.605128</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>1.065500</td>\n","      <td>0.967371</td>\n","      <td>0.605558</td>\n","      <td>0.605641</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>1.089000</td>\n","      <td>0.971661</td>\n","      <td>0.608764</td>\n","      <td>0.608974</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>0.974200</td>\n","      <td>0.970839</td>\n","      <td>0.603697</td>\n","      <td>0.603846</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>1.037100</td>\n","      <td>0.950170</td>\n","      <td>0.618846</td>\n","      <td>0.618974</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>0.968700</td>\n","      <td>0.945766</td>\n","      <td>0.617534</td>\n","      <td>0.617692</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>1.007900</td>\n","      <td>0.959694</td>\n","      <td>0.622947</td>\n","      <td>0.623077</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>0.987500</td>\n","      <td>0.939056</td>\n","      <td>0.618870</td>\n","      <td>0.618974</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>1.036900</td>\n","      <td>0.936348</td>\n","      <td>0.619675</td>\n","      <td>0.619744</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>0.989300</td>\n","      <td>0.935442</td>\n","      <td>0.619727</td>\n","      <td>0.619744</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[INFO|trainer.py:3081] 2023-07-28 21:49:57,371 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 21:49:57,372 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 21:49:57,373 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 21:50:44,205 >> Saving model checkpoint to /content/checkpoint-500\n","[INFO|configuration_utils.py:458] 2023-07-28 21:50:44,208 >> Configuration saved in /content/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 21:51:49,456 >> Model weights saved in /content/checkpoint-500/pytorch_model.bin\n","[INFO|trainer.py:3081] 2023-07-28 21:55:49,951 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 21:55:49,953 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 21:55:49,954 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 21:56:36,697 >> Saving model checkpoint to /content/checkpoint-1000\n","[INFO|configuration_utils.py:458] 2023-07-28 21:56:36,700 >> Configuration saved in /content/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 21:58:16,856 >> Model weights saved in /content/checkpoint-1000/pytorch_model.bin\n","[INFO|trainer.py:3081] 2023-07-28 22:03:21,005 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 22:03:21,006 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 22:03:21,008 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 22:04:07,677 >> Saving model checkpoint to /content/checkpoint-1500\n","[INFO|configuration_utils.py:458] 2023-07-28 22:04:07,680 >> Configuration saved in /content/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 22:06:02,027 >> Model weights saved in /content/checkpoint-1500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 22:09:58,891 >> Deleting older checkpoint [/content/checkpoint-500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 22:11:13,595 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 22:11:13,597 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 22:11:13,598 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 22:12:00,195 >> Saving model checkpoint to /content/checkpoint-2000\n","[INFO|configuration_utils.py:458] 2023-07-28 22:12:00,199 >> Configuration saved in /content/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 22:13:58,209 >> Model weights saved in /content/checkpoint-2000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 22:17:58,866 >> Deleting older checkpoint [/content/checkpoint-1000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 22:19:13,293 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 22:19:13,295 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 22:19:13,297 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 22:19:59,876 >> Saving model checkpoint to /content/checkpoint-2500\n","[INFO|configuration_utils.py:458] 2023-07-28 22:19:59,879 >> Configuration saved in /content/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 22:22:04,246 >> Model weights saved in /content/checkpoint-2500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 22:26:05,551 >> Deleting older checkpoint [/content/checkpoint-1500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 22:27:19,876 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 22:27:19,878 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 22:27:19,879 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 22:28:06,461 >> Saving model checkpoint to /content/checkpoint-3000\n","[INFO|configuration_utils.py:458] 2023-07-28 22:28:06,464 >> Configuration saved in /content/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 22:30:01,633 >> Model weights saved in /content/checkpoint-3000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 22:33:54,314 >> Deleting older checkpoint [/content/checkpoint-2000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 22:35:08,649 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 22:35:08,650 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 22:35:08,651 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 22:35:55,083 >> Saving model checkpoint to /content/checkpoint-3500\n","[INFO|configuration_utils.py:458] 2023-07-28 22:35:55,087 >> Configuration saved in /content/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 22:37:55,225 >> Model weights saved in /content/checkpoint-3500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 22:41:50,606 >> Deleting older checkpoint [/content/checkpoint-2500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 22:43:05,122 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 22:43:05,123 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 22:43:05,127 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 22:43:51,731 >> Saving model checkpoint to /content/checkpoint-4000\n","[INFO|configuration_utils.py:458] 2023-07-28 22:43:51,735 >> Configuration saved in /content/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 22:45:54,089 >> Model weights saved in /content/checkpoint-4000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 22:49:54,895 >> Deleting older checkpoint [/content/checkpoint-3000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 22:51:09,451 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 22:51:09,452 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 22:51:09,455 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 22:51:56,109 >> Saving model checkpoint to /content/checkpoint-4500\n","[INFO|configuration_utils.py:458] 2023-07-28 22:51:56,112 >> Configuration saved in /content/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 22:53:56,414 >> Model weights saved in /content/checkpoint-4500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 22:57:55,541 >> Deleting older checkpoint [/content/checkpoint-3500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 22:59:10,095 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 22:59:10,097 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 22:59:10,098 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 22:59:56,723 >> Saving model checkpoint to /content/checkpoint-5000\n","[INFO|configuration_utils.py:458] 2023-07-28 22:59:56,726 >> Configuration saved in /content/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 23:01:55,569 >> Model weights saved in /content/checkpoint-5000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 23:05:57,142 >> Deleting older checkpoint [/content/checkpoint-4000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 23:07:11,515 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 23:07:11,516 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 23:07:11,517 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 23:07:58,014 >> Saving model checkpoint to /content/checkpoint-5500\n","[INFO|configuration_utils.py:458] 2023-07-28 23:07:58,016 >> Configuration saved in /content/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 23:09:58,550 >> Model weights saved in /content/checkpoint-5500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 23:13:59,006 >> Deleting older checkpoint [/content/checkpoint-4500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 23:15:13,414 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 23:15:13,416 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 23:15:13,419 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 23:16:00,179 >> Saving model checkpoint to /content/checkpoint-6000\n","[INFO|configuration_utils.py:458] 2023-07-28 23:16:00,182 >> Configuration saved in /content/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 23:18:01,521 >> Model weights saved in /content/checkpoint-6000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 23:22:10,732 >> Deleting older checkpoint [/content/checkpoint-5000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 23:23:25,550 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 23:23:25,551 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 23:23:25,554 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 23:24:12,179 >> Saving model checkpoint to /content/checkpoint-6500\n","[INFO|configuration_utils.py:458] 2023-07-28 23:24:12,182 >> Configuration saved in /content/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 23:26:08,682 >> Model weights saved in /content/checkpoint-6500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 23:30:11,657 >> Deleting older checkpoint [/content/checkpoint-5500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 23:31:26,187 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 23:31:26,189 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 23:31:26,190 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 23:32:12,583 >> Saving model checkpoint to /content/checkpoint-7000\n","[INFO|configuration_utils.py:458] 2023-07-28 23:32:12,586 >> Configuration saved in /content/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 23:34:07,788 >> Model weights saved in /content/checkpoint-7000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 23:38:08,110 >> Deleting older checkpoint [/content/checkpoint-6000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 23:39:22,977 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 23:39:22,978 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 23:39:22,979 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 23:40:09,478 >> Saving model checkpoint to /content/checkpoint-7500\n","[INFO|configuration_utils.py:458] 2023-07-28 23:40:09,481 >> Configuration saved in /content/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 23:42:14,752 >> Model weights saved in /content/checkpoint-7500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 23:46:25,068 >> Deleting older checkpoint [/content/checkpoint-6500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 23:47:39,774 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 23:47:39,776 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 23:47:39,777 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 23:48:26,369 >> Saving model checkpoint to /content/checkpoint-8000\n","[INFO|configuration_utils.py:458] 2023-07-28 23:48:26,372 >> Configuration saved in /content/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 23:50:25,533 >> Model weights saved in /content/checkpoint-8000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-28 23:54:25,576 >> Deleting older checkpoint [/content/checkpoint-7000] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-28 23:55:40,380 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-28 23:55:40,381 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-28 23:55:40,383 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-28 23:56:27,090 >> Saving model checkpoint to /content/checkpoint-8500\n","[INFO|configuration_utils.py:458] 2023-07-28 23:56:27,093 >> Configuration saved in /content/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-28 23:58:27,228 >> Model weights saved in /content/checkpoint-8500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-29 00:02:22,565 >> Deleting older checkpoint [/content/checkpoint-7500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-29 00:03:37,374 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-29 00:03:37,376 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-29 00:03:37,377 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-29 00:04:24,105 >> Saving model checkpoint to /content/checkpoint-9000\n","[INFO|configuration_utils.py:458] 2023-07-29 00:04:24,108 >> Configuration saved in /content/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-29 00:06:21,477 >> Model weights saved in /content/checkpoint-9000/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-29 00:10:22,947 >> Deleting older checkpoint [/content/checkpoint-8500] due to args.save_total_limit\n","[INFO|trainer.py:3081] 2023-07-29 00:11:37,881 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-29 00:11:37,883 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-29 00:11:37,883 >>   Batch size = 2\n","[INFO|trainer.py:2807] 2023-07-29 00:12:24,631 >> Saving model checkpoint to /content/checkpoint-9500\n","[INFO|configuration_utils.py:458] 2023-07-29 00:12:24,635 >> Configuration saved in /content/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-29 00:14:24,844 >> Model weights saved in /content/checkpoint-9500/pytorch_model.bin\n","[INFO|trainer.py:2894] 2023-07-29 00:18:26,768 >> Deleting older checkpoint [/content/checkpoint-9000] due to args.save_total_limit\n","[INFO|trainer.py:1934] 2023-07-29 00:18:26,798 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2093] 2023-07-29 00:18:26,801 >> Loading best model from /content/checkpoint-8000 (score: 0.6229474549327568).\n","[INFO|trainer.py:2807] 2023-07-29 00:18:29,476 >> Saving model checkpoint to /content\n","[INFO|configuration_utils.py:458] 2023-07-29 00:18:29,491 >> Configuration saved in /content/config.json\n","[INFO|modeling_utils.py:1851] 2023-07-29 00:20:28,715 >> Model weights saved in /content/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2210] 2023-07-29 00:20:28,757 >> tokenizer config file saved in /content/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2217] 2023-07-29 00:20:28,769 >> Special tokens file saved in /content/special_tokens_map.json\n","[INFO|trainer.py:3081] 2023-07-29 00:20:30,334 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-07-29 00:20:30,335 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-07-29 00:20:30,338 >>   Batch size = 2\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[INFO|trainer.py:3081] 2023-07-29 00:21:16,817 >> ***** Running Prediction *****\n","[INFO|trainer.py:3083] 2023-07-29 00:21:16,819 >>   Num examples = 3600\n","[INFO|trainer.py:3086] 2023-07-29 00:21:16,820 >>   Batch size = 2\n"]},{"name":"stdout","output_type":"stream","text":["***** eval metrics *****\n","  epoch                   =       0.84\n","  eval_loss               =     0.9597\n","  eval_macro-f1           =     0.6229\n","  eval_micro-f1           =     0.6231\n","  eval_runtime            = 0:00:46.47\n","  eval_samples            =       3900\n","  eval_samples_per_second =     83.919\n","  eval_steps_per_second   =     41.959\n","*** Evaluate ***\n","Best Hyperparameters: {'learning_rate': 2e-05, 'num_train_epochs': 1, 'per_device_eval_batch_size': 2, 'per_device_train_batch_size': 4}\n","Best Validation Macro-F1: 0.6229474549327568\n","***** predict metrics *****\n","  predict_loss               =     0.9997\n","  predict_macro-f1           =      0.595\n","  predict_micro-f1           =      0.595\n","  predict_runtime            = 0:00:47.12\n","  predict_samples            =       3600\n","  predict_samples_per_second =     76.391\n","  predict_steps_per_second   =     38.196\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False\n","[INFO|configuration_utils.py:712] 2023-07-29 00:22:04,406 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-29 00:22:04,408 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"finetuning_task\": \"case_hold\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|configuration_utils.py:712] 2023-07-29 00:22:04,456 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-29 00:22:04,458 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|tokenization_utils_base.py:1839] 2023-07-29 00:22:04,462 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/vocab.txt\n","[INFO|tokenization_utils_base.py:1839] 2023-07-29 00:22:04,463 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer.json\n","[INFO|tokenization_utils_base.py:1839] 2023-07-29 00:22:04,464 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1839] 2023-07-29 00:22:04,465 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1839] 2023-07-29 00:22:04,468 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer_config.json\n","[INFO|configuration_utils.py:712] 2023-07-29 00:22:04,472 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n","[INFO|configuration_utils.py:768] 2023-07-29 00:22:04,474 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|modeling_utils.py:2603] 2023-07-29 00:22:04,564 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/model.safetensors\n","[INFO|modeling_utils.py:3319] 2023-07-29 00:22:08,262 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMultipleChoice: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:3331] 2023-07-29 00:22:08,263 >> Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["#!/usr/bin/env python\n","# coding=utf-8\n","\"\"\" Finetuning models on CaseHOLD (e.g. Bert, RoBERTa, LEGAL-BERT).\"\"\"\n","\n","import logging\n","import os\n","from dataclasses import dataclass, field\n","from typing import Optional\n","from sklearn.model_selection import ParameterGrid\n","import numpy as np\n","import random\n","import shutil\n","import glob\n","import os\n","\n","import transformers\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","\t\tAutoModelForMultipleChoice,\n","    DataCollatorWithPadding,\n","    EvalPrediction,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    default_data_collator,\n","    set_seed,\n","    EarlyStoppingCallback,\n","    Trainer\n",")\n","from transformers.trainer_utils import is_main_process\n","from transformers import EarlyStoppingCallback\n","# from casehold_helpers import MultipleChoiceDataset, Split\n","from sklearn.metrics import f1_score\n","# from models.deberta import DebertaForMultipleChoice\n","\n","logger = logging.getLogger(__name__)\n","\n","param_grid = {\n","    'learning_rate': [1e-5, 2e-5],  # Learning rates to try\n","    'num_train_epochs': [1, 2],        # Number of training epochs to try\n","    'per_device_train_batch_size': [2, 4],  # Batch sizes for training\n","    'per_device_eval_batch_size': [2, 4],   # Batch sizes for evaluation\n","}\n","\n","\n","@dataclass\n","class ModelArguments:\n","    \"\"\"\n","    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n","    \"\"\"\n","\n","    model_name_or_path: str = field(\n","        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n","    )\n","    config_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n","    )\n","    tokenizer_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n","    )\n","    cache_dir: Optional[str] = field(\n","        default=None,\n","        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n","    )\n","\n","\n","@dataclass\n","class DataTrainingArguments:\n","    \"\"\"\n","    Arguments pertaining to what data we are going to input our model for training and eval.\n","    \"\"\"\n","\n","    task_name: str = field(default=\"case_hold\", metadata={\"help\": \"The name of the task to train on\"})\n","    max_seq_length: int = field(\n","        default=256,\n","        metadata={\n","            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n","            \"than this will be truncated, sequences shorter will be padded.\"\n","        },\n","    )\n","    pad_to_max_length: bool = field(\n","        default=True,\n","        metadata={\n","            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n","            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n","        },\n","    )\n","    max_train_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_eval_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_predict_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    overwrite_cache: bool = field(\n","        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n","    )\n","\n","\n","def main(training_args,model_args):\n","    # See all possible arguments in src/transformers/training_args.py\n","    # or by passing the --help flag to this script.\n","    # We now keep distinct sets of args, for a cleaner separation of concerns.\n","    data_args = DataTrainingArguments(\n","        max_seq_length=128,\n","        # max_segments=64,\n","        # max_seg_length=128,\n","        overwrite_cache=False,\n","        pad_to_max_length=True,\n","    )\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n","    )\n","    logger.warning(\n","        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n","        training_args.local_rank,\n","        training_args.device,\n","        training_args.n_gpu,\n","        bool(training_args.local_rank != -1),\n","        training_args.fp16,\n","    )\n","    # Set the verbosity to info of the Transformers logger (on main process only):\n","    if is_main_process(training_args.local_rank):\n","        transformers.utils.logging.set_verbosity_info()\n","        transformers.utils.logging.enable_default_handler()\n","        transformers.utils.logging.enable_explicit_format()\n","    logger.info(\"Training/evaluation parameters %s\", training_args)\n","\n","    # Set seed\n","    set_seed(training_args.seed)\n","\n","    # Load pretrained model and tokenizer\n","    config = AutoConfig.from_pretrained(\n","        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n","        num_labels=5,\n","        finetuning_task=data_args.task_name,\n","        cache_dir=model_args.cache_dir,\n","    )\n","\n","    if config.model_type == 'big_bird':\n","        config.attention_type = 'original_full'\n","    elif config.model_type == 'longformer':\n","        config.attention_window = [data_args.max_seq_length] * config.num_hidden_layers\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n","        cache_dir=model_args.cache_dir,\n","        # Default fast tokenizer is buggy on CaseHOLD task, switch to legacy tokenizer\n","        use_fast=True,\n","    )\n","\n","    if config.model_type != 'deberta':\n","        model = AutoModelForMultipleChoice.from_pretrained(\n","            model_args.model_name_or_path,\n","            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","            config=config,\n","            cache_dir=model_args.cache_dir,\n","        )\n","    else:\n","        model = DebertaForMultipleChoice.from_pretrained(\n","            model_args.model_name_or_path,\n","            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","            config=config,\n","            cache_dir=model_args.cache_dir,\n","        )\n","\n","    train_dataset = None\n","    eval_dataset = None\n","\n","    # If do_train passed, train_dataset by default loads train split from file named train.csv in data directory\n","    if training_args.do_train:\n","        train_dataset = \\\n","            MultipleChoiceDataset(\n","                tokenizer=tokenizer,\n","                task=data_args.task_name,\n","                max_seq_length=data_args.max_seq_length,\n","                overwrite_cache=data_args.overwrite_cache,\n","                mode=Split.train,\n","            )\n","\n","    # If do_eval or do_predict passed, eval_dataset by default loads dev split from file named dev.csv in data directory\n","    if training_args.do_eval:\n","        eval_dataset = \\\n","            MultipleChoiceDataset(\n","                tokenizer=tokenizer,\n","                task=data_args.task_name,\n","                max_seq_length=data_args.max_seq_length,\n","                overwrite_cache=data_args.overwrite_cache,\n","                mode=Split.dev,\n","            )\n","\n","    if training_args.do_predict:\n","        predict_dataset = \\\n","            MultipleChoiceDataset(\n","                tokenizer=tokenizer,\n","                task=data_args.task_name,\n","                max_seq_length=data_args.max_seq_length,\n","                overwrite_cache=data_args.overwrite_cache,\n","                mode=Split.test,\n","            )\n","\n","    if training_args.do_train:\n","        if data_args.max_train_samples is not None:\n","            train_dataset = train_dataset[:data_args.max_train_samples]\n","        # Log a few random samples from the training set:\n","        for index in random.sample(range(len(train_dataset)), 3):\n","            logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n","\n","    if training_args.do_eval:\n","        if data_args.max_eval_samples is not None:\n","            eval_dataset = eval_dataset[:data_args.max_eval_samples]\n","\n","    if training_args.do_predict:\n","        if data_args.max_predict_samples is not None:\n","            predict_dataset = predict_dataset[:data_args.max_predict_samples]\n","\n","    # Define custom compute_metrics function, returns macro F1 metric for CaseHOLD task\n","    def compute_metrics(p: EvalPrediction):\n","        preds = np.argmax(p.predictions, axis=1)\n","        # Compute macro and micro F1 for 5-class CaseHOLD task\n","        macro_f1 = f1_score(y_true=p.label_ids, y_pred=preds, average='macro', zero_division=0)\n","        micro_f1 = f1_score(y_true=p.label_ids, y_pred=preds, average='micro', zero_division=0)\n","        return {'macro-f1': macro_f1, 'micro-f1': micro_f1}\n","\n","    # Initialize our Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=eval_dataset,\n","        compute_metrics=compute_metrics,\n","        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n","    )\n","\n","    # Training\n","    if training_args.do_train:\n","        trainer.train(\n","            model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n","        )\n","        trainer.save_model()\n","        # Re-save the tokenizer for model sharing\n","        if trainer.is_world_process_zero():\n","            tokenizer.save_pretrained(training_args.output_dir)\n","\n","    # Evaluation on eval_dataset\n","    if training_args.do_eval:\n","        logger.info(\"*** Evaluate ***\")\n","        metrics = trainer.evaluate(eval_dataset=eval_dataset)\n","\n","        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n","        metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n","\n","        trainer.log_metrics(\"eval\", metrics)\n","        trainer.save_metrics(\"eval\", metrics)\n","        print(\"*** Evaluate ***\")\n","        #metrics = trainer.evaluate(eval_dataset=eval_dataset)\n","        macro_f1 = metrics['eval_macro-f1']\n","        best_accuracy = 0.0\n","        best_params = {}\n","        if macro_f1 > best_accuracy:\n","           best_accuracy = macro_f1\n","           best_params = params\n","        print(\"Best Hyperparameters:\", best_params)\n","        print(\"Best Validation Macro-F1:\", best_accuracy)\n","\n","    # Predict on eval_dataset\n","    if training_args.do_predict:\n","        logger.info(\"*** Predict ***\")\n","\n","        predictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix=\"predict\")\n","\n","        max_predict_samples = (\n","            data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)\n","        )\n","        metrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n","\n","        trainer.log_metrics(\"predict\", metrics)\n","        trainer.save_metrics(\"predict\", metrics)\n","\n","        output_predict_file = os.path.join(training_args.output_dir, \"test_predictions.csv\")\n","        if trainer.is_world_process_zero():\n","            with open(output_predict_file, \"w\") as writer:\n","                for index, pred_list in enumerate(predictions):\n","                    pred_line = '\\t'.join([f'{pred:.5f}' for pred in pred_list])\n","                    writer.write(f\"{index}\\t{pred_line}\\n\")\n","\n","\n","# Print the best hyperparameters and its corresponding validation macro-f1 score\n","\n","    # Clean up checkpoints\n","    checkpoints = [filepath for filepath in glob.glob(f'{training_args.output_dir}/*/') if '/checkpoint' in filepath]\n","    for checkpoint in checkpoints:\n","        shutil.rmtree(checkpoint)\n","\n","\n","# def _mp_fn(index):\n","# For xla_spawn (TPUs)\n","# main()\n","\n","\n","if __name__ == \"__main__\":\n","    # main()\n","\n","    training_args = TrainingArguments(\n","        do_train=True,\n","        do_eval=True,\n","        do_predict=True,\n","        output_dir=os.getcwd(),\n","        overwrite_output_dir=True,\n","        num_train_epochs=2,\n","        per_device_train_batch_size=4,\n","        per_device_eval_batch_size=4,\n","        save_steps=500,\n","        save_total_limit=2,\n","        fp16=False,\n","        logging_dir=\"./logs\",\n","        logging_steps=200,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=500,\n","        logging_first_step=False,\n","        load_best_model_at_end=True,\n","        metric_for_best_model=\"macro-f1\",\n","    )\n","    model_names_or_paths = [\n","        \"casehold/custom-legalbert\",\n","        \"bert-base-uncased\",\n","        \"bert-base-cased\",\n","        # Add more model names or paths as needed\n","    ]\n","    for model_name_or_path in model_names_or_paths:\n","        # Set the model_name_or_path in the ModelArguments\n","        model_args = ModelArguments(\n","            model_name_or_path=model_name_or_path,\n","            # ... Rest of the model arguments ...\n","        )\n","    for params in ParameterGrid(param_grid):\n","    # Set the current hyperparameters in the TrainingArguments object\n","        training_args.learning_rate = params['learning_rate']\n","        training_args.num_train_epochs = params['num_train_epochs']\n","        training_args.per_device_train_batch_size = params['per_device_train_batch_size']\n","        training_args.per_device_eval_batch_size = params['per_device_eval_batch_size']\n","        main(training_args,model_args)\n","\t\t # Train the model\n","    #train_result = trainer.train()\n","    #metrics = train_result.metrics\n","\n","    # Evaluate the model\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":239,"status":"ok","timestamp":1691120072505,"user":{"displayName":"sweta simran","userId":"15795161217929105177"},"user_tz":-60},"id":"CGqP3_yAWzTs","outputId":"7759f65a-5dae-445c-ba32-5b9dc6f3afe2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Arbitration Notices: ['arbitration', 'arbitration', 'arbitration', 'arbitration', 'arbitration', 'arbitration', 'arbitration', 'arbitration', 'arbitration', 'arbitration', 'arbitration', 'arbitration', 'arbitration', 'arbitration', 'arbitration', 'arbitration', 'arbitration']\n","Acknowledgments: ['agree', 'agree', 'acknowledge', 'agree', 'understand', 'agree', 'bound by', 'agree', 'bound by', 'acknowledge', 'agree', 'agree', 'agree', 'agree', 'agree', 'agree', 'agree', 'agree', 'bound by', 'understand', 'agree', 'bound by', 'agree', 'agree', 'agree', 'agree', 'agree', 'agree', 'agree', 'agree', 'agree', 'agree', 'agree', 'agree', 'agree', 'bound by', 'understand', 'agree', 'agree', 'agree', 'bound by', 'bound by', 'agree', 'acknowledge', 'agree', 'agree', 'bound by', 'agree', 'agree', 'agree', 'agree', 'agree', 'agree', 'agree', 'agree', 'agree', 'agree', 'agree', 'agree', 'agree', 'agree', 'agree', 'agree']\n"]}],"source":["import re\n","\n","def extract_legal_notices(text):\n","    # Define regular expressions for common legal notice patterns\n","    arbitration_pattern = r'\\barbitration\\b'\n","    acknowledgment_pattern = r'\\b(?:acknowledge|agree|understand|bound by)\\b'\n","\n","    # Find all occurrences of legal notices and acknowledgments in the text\n","    arbitration_matches = re.findall(arbitration_pattern, text, re.IGNORECASE)\n","    acknowledgment_matches = re.findall(acknowledgment_pattern, text, re.IGNORECASE)\n","\n","    # Return a dictionary containing the extracted patterns\n","    extracted_patterns = {\n","        'arbitration_notices': arbitration_matches,\n","        'acknowledgments': acknowledgment_matches\n","    }\n","    return extracted_patterns\n","\n","def main():\n","    # Read the text file containing incorrect predictions\n","    with open('/content/sample_data/incorrect_predictions.txt', 'r') as file:\n","        text = file.read()\n","\n","    # Extract patterns from the text\n","    patterns = extract_legal_notices(text)\n","\n","    # Print the extracted patterns\n","    print(\"Arbitration Notices:\", patterns['arbitration_notices'])\n","    print(\"Acknowledgments:\", patterns['acknowledgments'])\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16396,"status":"ok","timestamp":1691121176574,"user":{"displayName":"sweta simran","userId":"15795161217929105177"},"user_tz":-60},"id":"JthVxXGBp4fz","outputId":"30f69bf9-5353-4da4-eaa9-8dacf9c26852"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n","Collecting transformers\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n","\u001b[31mERROR: Could not find a version that satisfies the requirement collections (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for collections\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install nltk\n","!pip install transformers\n","!pip install collections"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":131,"referenced_widgets":["f03d1b35e95744a7a2f0a3def9a7adea","45e493381f1047bcb023f4bc87681ac3","d784552dce224dc1a2f0f2a3c75408ed","0cd5694aac534d55822cd56d694920e6","b80a82523ffb4fbb875063313ebc3b8d","3122b297870a4c8ba32b6101d49f14c6","e4720ef6343a412bb30e29cf9d3d9a3c","e4a26bb02a99473d977b4a52f9f7a312","e6bcdebbc7e44e0a9a8659c87ec2eef2","a8a1028fb3504388a4798a496502c0a3","d7c4922a13024eb19b0a39124cd571fa","7e726ca1e4914baeb000f136a71d3594","1d9e9b4b173140799d96f93996c060d1","842be355a6584082b190c2b60b87c6a6","6198f29b9c0342819b615019b19fa6c9","09bdf91e87074962a927560cbe8c1388","24d284a2f10448fe865c0b818ec2e05d","aac8a6ac1d0b4d3094ae27934e02ee5b","8b889fbf67b7427497c3639cf0543667","976d029833e64408bc08994c5ce0e9c2","1968a5ad1e0945bfb0d107f98fa7eacb","3040e9f0c6824be1af52e17f3f0f3a07","986f56df7aab458ca4f8635847e69c05","4274bde6c7014a25bb2d7a3b8bf8b076","57b47bf82a1e45aeb14e33b2c9b555d3","3798faa3532449fb82132571b17215ba","9a3affa4b0a342e895996f5f1f01501f","e0e00fad3a0e466faa21e492c9ab483d","1df1c835668942a49926bc4277b690d0","c914802ef7214ae1bffabdbb0a5ab917","29cb054b56984e44bbf75c3a314d0848","6b5993acb961495c90a18d7a312efdf7","ca348fca7a1444c681c8e9249cffad65"]},"executionInfo":{"elapsed":2079,"status":"ok","timestamp":1691123633814,"user":{"displayName":"sweta simran","userId":"15795161217929105177"},"user_tz":-60},"id":"9K9cobXq0Kfv","outputId":"d4ff99eb-fc57-447b-845d-f678c059f60a"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f03d1b35e95744a7a2f0a3def9a7adea","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7e726ca1e4914baeb000f136a71d3594","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"986f56df7aab458ca4f8635847e69c05","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Corresponding word for Input ID 2156:  house\n"]}],"source":["from transformers import GPT2Tokenizer\n","\n","def main():\n","    input_text = \"except where our dispute is being resolved pursuant to an arbitration ( as provided below ) , if you are a resident of the united states or canada , you agree that any claim or dispute you may have against evernote must be resolved exclusively by a state or federal court located in san mateo county , california .\"\n","    input_ids = [0, 26837, 147, 84, 4464, 16, 145, 8179, 22918, 7, 41, 16211, 36, 25, 1286, 874, 4839, 2156, 114, 47, 32, 10, 3313, 9, 5, 10409, 982, 50, 64, 2095, 2156, 47, 2854, 14, 143, 2026, 50, 4464, 47, 189, 33, 136, 364, 12170, 6457, 531, 28, 8179, 8992, 30, 10, 194, 50, 752, 461, 2034, 11, 15610, 12563, 139, 2109, 2156, 13011, 1594, 43052, 479, 1437, 50118, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","\n","    # Initialize the GPT-2 tokenizer\n","    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","\n","    # Decode the input ID 2156 to get the corresponding word\n","    corresponding_word = tokenizer.decode(2156)\n","\n","    print(f\"Corresponding word for Input ID 2156: {corresponding_word}\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2933,"status":"ok","timestamp":1691123195788,"user":{"displayName":"sweta simran","userId":"15795161217929105177"},"user_tz":-60},"id":"zDwDsK_4n-Q9","outputId":"13f56492-778d-4bec-e1c4-5b7067de6b91"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["Most occurring input IDs and their counts:\n","Input ID: 1, Count: 13164\n","Input ID: 2156, Count: 352\n","Input ID: 5, Count: 275\n","Input ID: 50, Count: 255\n","Input ID: 7, Count: 222\n","Input ID: 9, Count: 196\n","Input ID: 8, Count: 167\n","Input ID: 0, Count: 157\n","Input ID: 2, Count: 157\n","Input ID: 479, Count: 156\n","Input ID: 1437, Count: 156\n","Input ID: 50118, Count: 156\n","Input ID: 143, Count: 123\n","Input ID: 47, Count: 122\n","\n","Corresponding words for the most occurring input IDs:\n","Input ID: 1, Words: [ u n u s e d 0 ]\n","Input ID: 2156, Words: s e e\n","Input ID: 5, Words: [ u n u s e d 4 ]\n","Input ID: 50, Words: [ u n u s e d 4 9 ]\n","Input ID: 7, Words: [ u n u s e d 6 ]\n","Input ID: 9, Words: [ u n u s e d 8 ]\n","Input ID: 8, Words: [ u n u s e d 7 ]\n","Input ID: 0, Words: [ P A D ]\n","Input ID: 2, Words: [ u n u s e d 1 ]\n","Input ID: 479, Words: [ u n u s e d 4 7 4 ]\n","Input ID: 1437, Words: မ\n","Input ID: 50118, Words: [ U N K ]\n","Input ID: 143, Words: [ u n u s e d 1 3 8 ]\n","Input ID: 47, Words: [ u n u s e d 4 6 ]\n"]}],"source":["from collections import Counter\n","from transformers import BertTokenizer\n","\n","\n","\n","\n","\n","import re\n","import nltk\n","from collections import Counter\n","\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","\n","def remove_stopwords(input_text):\n","    # Convert the input text to lowercase\n","    input_text = input_text.lower()\n","\n","    # Remove non-alphanumeric characters and split into words\n","    words = re.findall(r'\\b\\w+\\b', input_text)\n","\n","    # Remove stop words\n","    stop_words = set(stopwords.words('english'))\n","    words = [word for word in words if word not in stop_words]\n","\n","    # Join the words back into a sentence\n","    processed_text = ' '.join(words)\n","    return processed_text\n","\n","\n","def find_most_occurring_ids_and_words(text):\n","    # Initialize the BERT tokenizer\n","    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","    # Split the text into lines\n","    lines = text.strip().split('\\n')\n","\n","    # Initialize a Counter to store the occurrences of each input ID\n","    id_counter = Counter()\n","\n","    # Process each line in the text\n","    for line in lines:\n","        # Find the index of \"Input IDs: \" to extract the input IDs\n","        id_index = line.find(\"Input IDs: \")\n","        if id_index != -1:\n","            input_ids_str = line[id_index + len(\"Input IDs: \"):].strip()\n","\n","            # Convert the input IDs from a list of integers\n","            # Handle cases where input_ids_str is enclosed in brackets\n","            input_ids_str = input_ids_str.strip(\"[]\")\n","            input_ids = [int(id_str) for id_str in input_ids_str.split(', ')]\n","\n","            # Count occurrences of each input ID\n","            id_counter.update(input_ids)\n","\n","    # Find the most occurring input IDs and their counts\n","    most_occurring_ids = id_counter.most_common()\n","\n","    # Get the vocabulary of the BERT tokenizer\n","    vocab = tokenizer.get_vocab()\n","\n","    # Get the corresponding words for the most occurring input IDs\n","    id_to_words = {id: tokenizer.convert_ids_to_tokens(id) for id, _ in most_occurring_ids}\n","\n","    # Return the most occurring input IDs and their corresponding words\n","    return most_occurring_ids, id_to_words\n","\n","from collections import Counter\n","import nltk\n","from nltk.corpus import stopwords\n","from transformers import BertTokenizer\n","\n","nltk.download(\"stopwords\")\n","\n","def main():\n","    input_file = \"/content/sample_data/incorrect_predictions.txt\"  # Replace this with the path to your input file\n","\n","    # Initialize the BERT tokenizer\n","    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","\n","    # Load the input file\n","    with open(input_file, \"r\", encoding=\"utf-8\") as file:\n","        lines = file.readlines()\n","\n","    # Process the lines to remove stop words and count occurrences of input IDs\n","    input_ids_count = Counter()\n","\n","    for line in lines:\n","        if \"Input IDs: \" in line:\n","            input_ids_str = line.split(\"Input IDs: \")[1].strip()\n","            input_ids = [int(id_str) for id_str in input_ids_str[1:-1].split(\", \")]\n","\n","            # Remove stop words from the input IDs\n","            input_ids = [id for id in input_ids if id not in stopwords.words(\"english\")]\n","\n","            # Count occurrences of each input ID\n","            input_ids_count.update(input_ids)\n","\n","    # Get the most occurring input IDs\n","    most_occurring_input_ids = [(input_id, count) for input_id, count in input_ids_count.items() if count > 100]\n","\n","    # Sort the input IDs by count in descending order\n","    most_occurring_input_ids.sort(key=lambda x: x[1], reverse=True)\n","\n","    # Display the most occurring input IDs and their counts\n","    print(\"Most occurring input IDs and their counts:\")\n","    for input_id, count in most_occurring_input_ids:\n","        print(f\"Input ID: {input_id}, Count: {count}\")\n","\n","    # Process the input texts to get the corresponding words for the most occurring input IDs\n","    print(\"\\nCorresponding words for the most occurring input IDs:\")\n","    for input_id, count in most_occurring_input_ids:\n","        input_text = tokenizer.decode(input_id)\n","        print(f\"Input ID: {input_id}, Words: {input_text}\")\n","\n","if __name__ == \"__main__\":\n","    main()\n","#With these modifications, the code should now decode the input IDs using the BERT tokenizer to get the corresponding words. The output should display the most occurring input IDs and their counts, along with the corresponding words for those input IDs. Please replace \"your_input_file.txt\" with the actual path to your input file and run the code again.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1WlQ9v5WLsbvtnZrh4MwMUI9LeW7kbTEd","timestamp":1693254364784}],"gpuType":"V100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"018e017133cc48fea26244709c3ce738":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f2f88b76ef6549c0bf5bb673960add6a","IPY_MODEL_65465f712c7d44d48ef13ca54c7a0bd6","IPY_MODEL_ee2773b8e9cd4c39af6a7f10e3ae4f48"],"layout":"IPY_MODEL_1cf9071d1cd8474a88d0cfbfadcf3cf6"}},"027c2bbdca934000980c12afe16d24e4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03394d00c4514c398c1db60b2e528e04":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c1ed27cb61f4c528439177902142c16","max":21041,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b8ae4688334947abbdc800cfb12ce2de","value":21041}},"041c1c2c385f439b9975cc5a73758f7b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"044deea78dc04dfebd969c86d5929508":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9d28b43412d40008f1f07691eec9663","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0bb9347b6f1049aba75419ce6122fbab","value":440449768}},"05eb5a9b0b354ff4b05492396f31c59b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07f57c4aa65a4ee0adac45bf2b94ebc2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08db827145d747f197009e1fa3eb2bf7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0908082874724b6681d8af1a10f4aa57":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"093daac294574bf5b036b2d9b30e3b18":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09bdf91e87074962a927560cbe8c1388":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a250f49e430498881c9f2cd0e695854":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c83fc2e6ae784f738cb1ba8979054ea4","max":1607,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c06450233ebf4de88fd9fccda3365086","value":1607}},"0b16d58053a045849897820e3a7ffe83":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_334076ec8d644e4e87e01cb287b64831","placeholder":"​","style":"IPY_MODEL_d9afaf3b0bed424a8c2336bc3a85db80","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"0b5e53eb379a46fab9a80de7e89db759":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_760dbdf67db245e69d427e622d902930","placeholder":"​","style":"IPY_MODEL_f6231b35767542d3a02b2d201a4a76e1","value":" 232k/232k [00:00&lt;00:00, 672kB/s]"}},"0bb9347b6f1049aba75419ce6122fbab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0c6c496bad0046b08977e7d4a1f979d0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0cd5694aac534d55822cd56d694920e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8a1028fb3504388a4798a496502c0a3","placeholder":"​","style":"IPY_MODEL_d7c4922a13024eb19b0a39124cd571fa","value":" 1.04M/1.04M [00:00&lt;00:00, 1.46MB/s]"}},"0dfc0eceea84402b84929f446cf845c4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e0637a70b5c4b91adee1abf57367543":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bae25b13f9174004ba576e137b21e180","placeholder":"​","style":"IPY_MODEL_842b34b9117644a9bf16d0a68e2f13d1","value":" 16.3M/16.3M [00:02&lt;00:00, 11.7MB/s]"}},"0f78340249be49328ea94f7c22cd95ab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0faca49748a646b29484e57ee882fe7a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2ca65cea389c48b4a7ed61a1ad0dcac7","IPY_MODEL_5822aa2747124f949a865101362b4799","IPY_MODEL_0fc259f08b1d4db9b3dbbfbfadd56a5b"],"layout":"IPY_MODEL_ed93be7ed0e94c4abef4b788c4f16c72"}},"0fc259f08b1d4db9b3dbbfbfadd56a5b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_05eb5a9b0b354ff4b05492396f31c59b","placeholder":"​","style":"IPY_MODEL_bb37f09a3d62447781b0b85dc630ab66","value":" 466k/466k [00:00&lt;00:00, 893kB/s]"}},"100222d038aa4f608f826703b3d8e5b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"10046d513ebf4c9782fa1f4926406fda":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"10c1bb9fe3854f8892e9173216681029":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1380dd34fabe46b696cae4ac9130bb8e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"13edfeef3f8e4f09ae25e07efe3e7f29":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"175f9c4606a44269826bea6f85d0bcb9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1968a5ad1e0945bfb0d107f98fa7eacb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19b03633a6fc44b0b55e053bcd0e3c65":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_63e07ebaf6af41f7b8631514740421e1","max":511342,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5ced662924fe49d9af2af3729eb5ed1e","value":511342}},"1b06caffeff94a909dc1c2a29bb99d13":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1b102b8c2d484f529dcdcb5b7a5a058c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1bdab01b1ba64194a83d2f8ca10e24bf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_027c2bbdca934000980c12afe16d24e4","placeholder":"​","style":"IPY_MODEL_72629b334b1649ebbd44d997044698aa","value":"Downloading builder script: 100%"}},"1cf9071d1cd8474a88d0cfbfadcf3cf6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d9e9b4b173140799d96f93996c060d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_24d284a2f10448fe865c0b818ec2e05d","placeholder":"​","style":"IPY_MODEL_aac8a6ac1d0b4d3094ae27934e02ee5b","value":"Downloading (…)olve/main/merges.txt: 100%"}},"1df1c835668942a49926bc4277b690d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1e07598ff0c847f0938a44a9b0c4ef0e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1e29619a90cd48b18ada7d5cce2bbd39":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ea84f047b0f4a0083fb61a4869edd09":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1efe37c4d4af44c08f57dc3f0e556a64":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1f0e89b7fce7466991cb7cdddca227ad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_88ed28f105f24dccb6a79a82c2307511","IPY_MODEL_c0abb82712b24bba86d4157e1f808ae9","IPY_MODEL_225067431398426bb24352b8222b76d9"],"layout":"IPY_MODEL_0dfc0eceea84402b84929f446cf845c4"}},"1f43ca31bae84fc99cb4e13722b5fe1f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fa3b8dd78d9437799087ab4bb1bccd4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aecfb08bc7b7419ea420ca0846370479","placeholder":"​","style":"IPY_MODEL_10c1bb9fe3854f8892e9173216681029","value":"Downloading metadata: 100%"}},"225067431398426bb24352b8222b76d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a35e28d9075c4ec0a66488d426e87208","placeholder":"​","style":"IPY_MODEL_67c1f3d339514e48a19ad8e8ec186ba0","value":" 32.9k/32.9k [00:00&lt;00:00, 644kB/s]"}},"225244aa22314ea283d13b2458c9bb12":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8da073d7f08442d6a5f49a6e11f4abb3","IPY_MODEL_9c962a45969a44df91db0fdc0ef4b518","IPY_MODEL_415a42aea42140608562076acc4ac4e5"],"layout":"IPY_MODEL_1f43ca31bae84fc99cb4e13722b5fe1f"}},"22854f47b3ad4b2ea6af0a0c44af6af5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23a7694490ff4e48aa9b9e84c5b3eed7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24d284a2f10448fe865c0b818ec2e05d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2608744b99164200ba18ba625dc9c303":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bd1aa50b342e4b338d5053ee0ee4985b","IPY_MODEL_be83be31ea7f47f4a3e2e830b7d64dc7","IPY_MODEL_5b59b5f1a1b148df93d6d91e6e52da68"],"layout":"IPY_MODEL_d959e20713a04777addce249dfd18a06"}},"2610a6e4ab1b48fa85ad3fa76fcf7f99":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9935f7c01e1449b698e39ae775250ace","placeholder":"​","style":"IPY_MODEL_0f78340249be49328ea94f7c22cd95ab","value":"Running tokenizer on prediction dataset: 100%"}},"263a01d58cd842d0a13e986ab8f37f85":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"279d2edac05b4dad830b87c6d0977599":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef4504de431c4dad8dcec37a960ccd06","placeholder":"​","style":"IPY_MODEL_8f0eaf06d9d74f1083d14d8df3f50068","value":" 21.0k/21.0k [00:00&lt;00:00, 430kB/s]"}},"296111eaaab2460e8aa0515eb103fe82":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"29cb054b56984e44bbf75c3a314d0848":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"29f1e97852634d70878e86e92e8096a4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_56cc2e17506d4a93af48a3f47595dfde","IPY_MODEL_e0169761bfb54da990a144ef63ada248","IPY_MODEL_e6eb8ad47044414f936c44b75fdb5c2e"],"layout":"IPY_MODEL_5505a2a9351d40579ba1577352efab60"}},"2ae893aba8124b5c8fb2662682b91c10":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_175f9c4606a44269826bea6f85d0bcb9","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fd38229f818f4aeeba121c404c615039","value":28}},"2be51803c81e42299430bba9cb98bdb5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_409ec96032344760be05980b1eb972f9","IPY_MODEL_c9f90de294d941f2a951aaad8a6f7d10","IPY_MODEL_cea52e6d84384c23ba5e454d48c0b1c8"],"layout":"IPY_MODEL_c35d2103ebb64539884edcb111e83556"}},"2c38d4790dd649ec8827203406ab4f2b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2c4e500778274b419b7bfc63f56e428e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ca65cea389c48b4a7ed61a1ad0dcac7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8502518447274e3f94837fe169609cdd","placeholder":"​","style":"IPY_MODEL_ccdf056b54d34126ab4e07762e63ac3a","value":"Downloading (…)/main/tokenizer.json: 100%"}},"2e07411366c94542b1bc877705d0b2c0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3040e9f0c6824be1af52e17f3f0f3a07":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3122b297870a4c8ba32b6101d49f14c6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32230d53ae9a41d78084f537e97fbc87":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3242dc9cae174272b7832b13c5cbd7f3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32a2dea83fc14f38b9a7429de18b6f24":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3242dc9cae174272b7832b13c5cbd7f3","placeholder":"​","style":"IPY_MODEL_c2c316cbb1d84e5daf6eaae6198ce327","value":"Downloading metadata: 100%"}},"32de876342ae4f71873de6e6e2f965e8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"334076ec8d644e4e87e01cb287b64831":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3456d453a5a84bf4bdfd6ae8a75c7f4e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36568abb55b949f2b3d1b2ee3537ed43":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e29619a90cd48b18ada7d5cce2bbd39","placeholder":"​","style":"IPY_MODEL_b12b0976b05145e2b7fa4c937ad7a024","value":"Generating train split: 100%"}},"3700b4a06c8643749c3fab13457c9126":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_32a2dea83fc14f38b9a7429de18b6f24","IPY_MODEL_03394d00c4514c398c1db60b2e528e04","IPY_MODEL_c12c74e23fba4d4c998b897b7d264e62"],"layout":"IPY_MODEL_9d11e58451e347a4bb1716e48a27a01b"}},"3798faa3532449fb82132571b17215ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b5993acb961495c90a18d7a312efdf7","placeholder":"​","style":"IPY_MODEL_ca348fca7a1444c681c8e9249cffad65","value":" 665/665 [00:00&lt;00:00, 36.4kB/s]"}},"3803480e26994889b5295db64bfb54f8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"38b40dafc599424a91b5b465d76f7f21":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"38f385ed5ddf4bf0ab4fa79268dfa96a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d001688a98c4c3cba13b0091452184a","placeholder":"​","style":"IPY_MODEL_86e20fd8fcac4136ab3e3f1815ff9ec9","value":" 511k/511k [00:01&lt;00:00, 302kB/s]"}},"39db27f176ef4c51941d63acf4cd5829":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3b48afacc0834191b31963b71c333583":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3e317e239dae4bbe86be9c42c8619089":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7124e48443ae4f67a082fb5d971c08a1","IPY_MODEL_0a250f49e430498881c9f2cd0e695854","IPY_MODEL_4a7742845306497daca38a3c38ef2e9e"],"layout":"IPY_MODEL_5e4d09e8821d4754adf3e9cabd0746d6"}},"3e966e1d707741b1863c662906485518":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3fa4a88f3f9541e591949a4d0be4d6bb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fae098be5b348869e17596c5d27271a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_08db827145d747f197009e1fa3eb2bf7","placeholder":"​","style":"IPY_MODEL_d3c436fd27754afaa7af0853548a29b0","value":" 440M/440M [00:07&lt;00:00, 55.5MB/s]"}},"3fffc34d1bdc44db8c06a023d26b2f02":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"409ec96032344760be05980b1eb972f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c6c496bad0046b08977e7d4a1f979d0","placeholder":"​","style":"IPY_MODEL_f5cbf7a5d5d049c9a30ea5391d19e6d7","value":"Generating train split: 100%"}},"415a42aea42140608562076acc4ac4e5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9bf0b071fca649c8ae734244f2bddbb7","placeholder":"​","style":"IPY_MODEL_8e6c9c8d64c44a4c9cc02d8d8a33ea6a","value":" 32.9k/32.9k [00:00&lt;00:00, 373kB/s]"}},"4241120bd25a48569e9c388d5f971034":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42686c3a3d9742fcb4709f5418c2c118":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4274bde6c7014a25bb2d7a3b8bf8b076":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0e00fad3a0e466faa21e492c9ab483d","placeholder":"​","style":"IPY_MODEL_1df1c835668942a49926bc4277b690d0","value":"Downloading (…)lve/main/config.json: 100%"}},"42adbd27a7bd49c385a0d58b833f7154":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4304e4222989448c95cd142d29826394":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"430ea0c945ef445c8d508bcfc0fd8482":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"442aa70f9886400d91db22c76f52c384":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5654469094e24821897d079767136228","max":23330,"min":0,"orientation":"horizontal","style":"IPY_MODEL_be0edf59be5948f0a2613d2093b79c33","value":23330}},"45916c76ecef4a4188dfb041da3942ec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"45e493381f1047bcb023f4bc87681ac3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3122b297870a4c8ba32b6101d49f14c6","placeholder":"​","style":"IPY_MODEL_e4720ef6343a412bb30e29cf9d3d9a3c","value":"Downloading (…)olve/main/vocab.json: 100%"}},"4628f77f505e43d8b85d03e1c3553466":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7890672eec294c578a85d0aef2a97bc3","placeholder":"​","style":"IPY_MODEL_fcfff04f32524fa28f02686651e0211f","value":"Downloading (…)lve/main/config.json: 100%"}},"479fd428bfb64c91b8c09dab4cb5aab3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c55afeb8d8c9476d8e1f0750b5b9c634","IPY_MODEL_bf518457f4494d6bb59f4ed33ee9f19e","IPY_MODEL_a3d71a12da20417d96c075c29a85a3ba"],"layout":"IPY_MODEL_ad2e01a5a3fb4bafb0dbb585c3f4c2aa"}},"4932192d7f984fe4b927b3b6eb51b16a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f1cb9844b7b9492fa3feb5616cdb5d6d","IPY_MODEL_b8e4ff5b1bd647cdb5f3fc79d3fd211e","IPY_MODEL_c3a65bed4ffa4bfc854f5054dcab5e71"],"layout":"IPY_MODEL_f92e669990784453bf5854c8b38b13fd"}},"49b2d89cb90c445a87d98d4410a1ce5c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a7742845306497daca38a3c38ef2e9e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_94ef3fc063f44befaca2a741c6e8bd0e","placeholder":"​","style":"IPY_MODEL_6d8db85995934c468f0e9a56d52d5873","value":" 1607/1607 [00:00&lt;00:00,  6.92 examples/s]"}},"4ae19ac474044a859e515303c8252387":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4c64063c79174c7d9c4c6add7d641ec3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b6b7be8c86e04cad97e037990f5372d8","IPY_MODEL_da93f3aaa34b427a8fd9e73dd85ab047","IPY_MODEL_c772bdb98003478cae852ed85ca61887"],"layout":"IPY_MODEL_b4c49dd9aa4f45da9badcf1f807f4ca4"}},"4d46c83672584647ab676514936de69f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4e109b2bd06d445ab4794b48dedb4f6d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f516cdc6c19457a884561016a018dc9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"504a6042fbfb4f11bd5df9d928056460":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50dfdba2a16a4468a776f38a4a959b22":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6bd7b209bb3a4674af2fcf6acf059fbe","IPY_MODEL_ce68de4bc52a4d009526da735fa04a8b","IPY_MODEL_0e0637a70b5c4b91adee1abf57367543"],"layout":"IPY_MODEL_c951de5e84044ec08ad22b9f867e8cd9"}},"5505a2a9351d40579ba1577352efab60":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5654469094e24821897d079767136228":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56c0699771894c5c83b7d5f4a1d4e2b2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56cc2e17506d4a93af48a3f47595dfde":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49b2d89cb90c445a87d98d4410a1ce5c","placeholder":"​","style":"IPY_MODEL_c5148a331c324ae48e40d1aa37ce6dd5","value":"Generating test split: 100%"}},"57654d55a3a34ff486ee1af57f9e78e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c3fecfa10bf4280aed8c9dcb93f94b3","max":10000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1380dd34fabe46b696cae4ac9130bb8e","value":10000}},"57b47bf82a1e45aeb14e33b2c9b555d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c914802ef7214ae1bffabdbb0a5ab917","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_29cb054b56984e44bbf75c3a314d0848","value":665}},"5822aa2747124f949a865101362b4799":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b2bf3936028e46f284f0e133ab0442f0","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bfbf019599b9459596a8680831405baa","value":466062}},"5832b1b1c5d5415c83038598caf8cf6a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"589cf887c9f247e5a953f617a03a9b0c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0908082874724b6681d8af1a10f4aa57","placeholder":"​","style":"IPY_MODEL_32230d53ae9a41d78084f537e97fbc87","value":" 1400/1400 [00:16&lt;00:00, 83.98 examples/s]"}},"5928a32ca68f4b1c8b43972d06b15be9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f6499fd109240f6b13130c9e17189d3","placeholder":"​","style":"IPY_MODEL_62e79989e9bf4b24ba1f907057401aa9","value":" 23.3k/23.3k [00:00&lt;00:00, 464kB/s]"}},"5979352ca84d49e8a2550af96ea06dbe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"59b3d82644804198878c767ed418e8df":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59bc23c52b64456a8e6f7f56ec377c32":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cec4f7284e424a17a0a97d4d59754f86","placeholder":"​","style":"IPY_MODEL_936d34e02e9245e0bc479a43550e8958","value":" 10000/10000 [00:01&lt;00:00, 15942.60 examples/s]"}},"5aca35719125455a89005f383d86a3ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5b59b5f1a1b148df93d6d91e6e52da68":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_504a6042fbfb4f11bd5df9d928056460","placeholder":"​","style":"IPY_MODEL_263a01d58cd842d0a13e986ab8f37f85","value":" 60000/60000 [00:08&lt;00:00, 7145.10 examples/s]"}},"5b9ca847e912494cbdba608f5523143a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3fa4a88f3f9541e591949a4d0be4d6bb","placeholder":"​","style":"IPY_MODEL_4304e4222989448c95cd142d29826394","value":" 5532/5532 [00:00&lt;00:00, 10087.29 examples/s]"}},"5c1ed27cb61f4c528439177902142c16":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ced662924fe49d9af2af3729eb5ed1e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5d93ec15abcc4df6b67ce8f72e884ddf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e4d09e8821d4754adf3e9cabd0746d6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e5e6d22764d4cd8a18e7367038c37a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_430ea0c945ef445c8d508bcfc0fd8482","placeholder":"​","style":"IPY_MODEL_e7d0d3942fbc41a79bc9ab75db47d1f7","value":"Running tokenizer on validation dataset: 100%"}},"5e71d56bed6849ba8fc65c11b063f9d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42686c3a3d9742fcb4709f5418c2c118","placeholder":"​","style":"IPY_MODEL_5ff8b9d7669a4458a6548c8f16564937","value":"Downloading model.safetensors: 100%"}},"5ff8b9d7669a4458a6548c8f16564937":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"60cd536e9a25432e85eeea404a11202c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6198f29b9c0342819b615019b19fa6c9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1968a5ad1e0945bfb0d107f98fa7eacb","placeholder":"​","style":"IPY_MODEL_3040e9f0c6824be1af52e17f3f0f3a07","value":" 456k/456k [00:00&lt;00:00, 3.72MB/s]"}},"62e79989e9bf4b24ba1f907057401aa9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"63410da6bca94c4794fe26e8a7bd99cd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63e07ebaf6af41f7b8631514740421e1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64a7d7b5be79439698d9e9530c573e0f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65465f712c7d44d48ef13ca54c7a0bd6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_22854f47b3ad4b2ea6af0a0c44af6af5","max":10000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_39db27f176ef4c51941d63acf4cd5829","value":10000}},"66f2f15ea05246c28c1a204c9836a1fe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"671926b782c5411b8740f0ec5c8fdacc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3c68f94de8c4237805f183a1f356f92","placeholder":"​","style":"IPY_MODEL_9c29bbb44ab84ef8907c1b80512ca591","value":"Downloading data: 100%"}},"67c1f3d339514e48a19ad8e8ec186ba0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"686a423d59c34d7789815d5be405bae9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ac734f9711c4906b1144f63987a7582":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b5993acb961495c90a18d7a312efdf7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bd7b209bb3a4674af2fcf6acf059fbe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4809f95c8f541a48bdeddeb7f2c681c","placeholder":"​","style":"IPY_MODEL_78ad33dc3d0e4b76a744c408338119d9","value":"Downloading data: 100%"}},"6d7ba530350c44babd55091e1172bee3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d8db85995934c468f0e9a56d52d5873":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6eefe08a488a4bd3993a9fa0da8c1b90":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f6499fd109240f6b13130c9e17189d3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70749376b1b4496d9b54a5e011c0b50c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6eefe08a488a4bd3993a9fa0da8c1b90","max":21041,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3e966e1d707741b1863c662906485518","value":21041}},"70cfb9cdca9c4d1e94808f6beb438530":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7124e48443ae4f67a082fb5d971c08a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ac734f9711c4906b1144f63987a7582","placeholder":"​","style":"IPY_MODEL_1b06caffeff94a909dc1c2a29bb99d13","value":"Generating test split: 100%"}},"72629b334b1649ebbd44d997044698aa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"760dbdf67db245e69d427e622d902930":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"761448a6a3584487bd4a0eef06122ce7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77904bf74a2340f09bc9c59ca8d82bdf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"78163e08a3ca4e839a9d5e02c6a76b8f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d82b200fd9ee4b0dbd0efa3ddb704384","placeholder":"​","style":"IPY_MODEL_66f2f15ea05246c28c1a204c9836a1fe","value":"Downloading (…)okenizer_config.json: 100%"}},"7890672eec294c578a85d0aef2a97bc3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78ad33dc3d0e4b76a744c408338119d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"795eb28905d145038df3c7b896e9d1a8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"79eff3c76cf54d6ab44ff5a6b8afd0dc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a7abf4eaa884c34ab85b0e91fe37f07":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4737d5039084a46bca644028fad843e","max":5532,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b9dfdd8934904f2bb3fdb11522641705","value":5532}},"7b502efce2a749f391703c9fda995817":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7d001688a98c4c3cba13b0091452184a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d2d97335fae456483df4b0912ade350":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e726ca1e4914baeb000f136a71d3594":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1d9e9b4b173140799d96f93996c060d1","IPY_MODEL_842be355a6584082b190c2b60b87c6a6","IPY_MODEL_6198f29b9c0342819b615019b19fa6c9"],"layout":"IPY_MODEL_09bdf91e87074962a927560cbe8c1388"}},"7f6938181e764e739162991a7c6a10ea":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7fcd159bd65842c59ba86477d663bc68":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_36568abb55b949f2b3d1b2ee3537ed43","IPY_MODEL_7a7abf4eaa884c34ab85b0e91fe37f07","IPY_MODEL_5b9ca847e912494cbdba608f5523143a"],"layout":"IPY_MODEL_32de876342ae4f71873de6e6e2f965e8"}},"83a4d85fb138464a985006ddf96efead":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_56c0699771894c5c83b7d5f4a1d4e2b2","placeholder":"​","style":"IPY_MODEL_7b502efce2a749f391703c9fda995817","value":" 2275/2275 [00:00&lt;00:00, 10026.27 examples/s]"}},"842b34b9117644a9bf16d0a68e2f13d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"842be355a6584082b190c2b60b87c6a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b889fbf67b7427497c3639cf0543667","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_976d029833e64408bc08994c5ce0e9c2","value":456318}},"844ce341d3f5416db18e8023b232de94":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8fbbeb6ab86d45f3af0c23f3367607ef","placeholder":"​","style":"IPY_MODEL_9a8ffcbdeb7d44a58c098e567708ebaf","value":" 28.0/28.0 [00:00&lt;00:00, 629B/s]"}},"8502518447274e3f94837fe169609cdd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85785c406ebe4760afc670f7948842e2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5e71d56bed6849ba8fc65c11b063f9d2","IPY_MODEL_044deea78dc04dfebd969c86d5929508","IPY_MODEL_3fae098be5b348869e17596c5d27271a"],"layout":"IPY_MODEL_4241120bd25a48569e9c388d5f971034"}},"86e20fd8fcac4136ab3e3f1815ff9ec9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"88ed28f105f24dccb6a79a82c2307511":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_59b3d82644804198878c767ed418e8df","placeholder":"​","style":"IPY_MODEL_acaea09850b6496da8d7dba5e19b85f1","value":"Downloading readme: 100%"}},"89748838c40e4169a0460777d18cb9af":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_671926b782c5411b8740f0ec5c8fdacc","IPY_MODEL_fc7bbe67316149b1a0c5491448875588","IPY_MODEL_38f385ed5ddf4bf0ab4fa79268dfa96a"],"layout":"IPY_MODEL_c74c1ca77e8145269c4a057504acca50"}},"89fb254c313844fd8b9408326c6778ab":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8aeb0d302fa14feabd080529ab5c57e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8b889fbf67b7427497c3639cf0543667":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c3fecfa10bf4280aed8c9dcb93f94b3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d7822942e784eee999f80f963426de8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a88643a6e945447da49edac244c01613","IPY_MODEL_442aa70f9886400d91db22c76f52c384","IPY_MODEL_5928a32ca68f4b1c8b43972d06b15be9"],"layout":"IPY_MODEL_63410da6bca94c4794fe26e8a7bd99cd"}},"8d7e655358124f109fea406a52dbb838":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c4e500778274b419b7bfc63f56e428e","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8d977d7ac721400e94f8182f0313e1d9","value":570}},"8d977d7ac721400e94f8182f0313e1d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8da073d7f08442d6a5f49a6e11f4abb3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_70cfb9cdca9c4d1e94808f6beb438530","placeholder":"​","style":"IPY_MODEL_45916c76ecef4a4188dfb041da3942ec","value":"Downloading readme: 100%"}},"8db9c65e49134f2896f859576eddadfe":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e5d55c80c204b9aa1e5e551f41b1c9c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8e6c9c8d64c44a4c9cc02d8d8a33ea6a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f0eaf06d9d74f1083d14d8df3f50068":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8fbbeb6ab86d45f3af0c23f3367607ef":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90667c8574a04077b5d51e0b70e8908a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"936d34e02e9245e0bc479a43550e8958":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"94c9e97efd6c4f628dbf73b443e1e2c9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94ef3fc063f44befaca2a741c6e8bd0e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"958537996c2340168f02de3a73c0e034":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e13ac8b55c2b46c6b9b089269d550ebc","IPY_MODEL_57654d55a3a34ff486ee1af57f9e78e6","IPY_MODEL_59bc23c52b64456a8e6f7f56ec377c32"],"layout":"IPY_MODEL_c94f0926e23043f9b8a71309b92019f2"}},"976d029833e64408bc08994c5ce0e9c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"986f56df7aab458ca4f8635847e69c05":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4274bde6c7014a25bb2d7a3b8bf8b076","IPY_MODEL_57b47bf82a1e45aeb14e33b2c9b555d3","IPY_MODEL_3798faa3532449fb82132571b17215ba"],"layout":"IPY_MODEL_9a3affa4b0a342e895996f5f1f01501f"}},"9935f7c01e1449b698e39ae775250ace":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a3affa4b0a342e895996f5f1f01501f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a8ffcbdeb7d44a58c098e567708ebaf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9bf0b071fca649c8ae734244f2bddbb7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c29bbb44ab84ef8907c1b80512ca591":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c5b8a7d920d4adcb775c27540f80a91":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e42adafaed124a4a8ba1551451fb20b7","IPY_MODEL_de998f33582046e4b6c114024fab5682","IPY_MODEL_83a4d85fb138464a985006ddf96efead"],"layout":"IPY_MODEL_8db9c65e49134f2896f859576eddadfe"}},"9c962a45969a44df91db0fdc0ef4b518":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_64a7d7b5be79439698d9e9530c573e0f","max":32885,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5aca35719125455a89005f383d86a3ba","value":32885}},"9d11e58451e347a4bb1716e48a27a01b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a05ffac668684b9c8086cc6097700f5d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1283fab055c415b98e9b3368c83c2ae":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a35e28d9075c4ec0a66488d426e87208":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3d71a12da20417d96c075c29a85a3ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8d0a24a93bb41b48cbe9c03dc2134a8","placeholder":"​","style":"IPY_MODEL_ba3238bf9336424dabffeca80282ecf9","value":" 60000/60000 [00:02&lt;00:00, 24232.21 examples/s]"}},"a417062dc88a4d97a41b408c626417db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1bdab01b1ba64194a83d2f8ca10e24bf","IPY_MODEL_a528e54b36e240e19e843dbaeb53c94a","IPY_MODEL_fa2b279f40164f1f9de2d3f7cfec4472"],"layout":"IPY_MODEL_4e109b2bd06d445ab4794b48dedb4f6d"}},"a424cd7b6a5e45408a13aa6c8cd22a22":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a528e54b36e240e19e843dbaeb53c94a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e82cb59e81f84f1ebeb500a1cd4192fd","max":23330,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bc7a132b6e7e490388a999a505782b10","value":23330}},"a6e5c2cad02348d88005533c82c6ca92":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a88643a6e945447da49edac244c01613":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f516cdc6c19457a884561016a018dc9","placeholder":"​","style":"IPY_MODEL_100222d038aa4f608f826703b3d8e5b2","value":"Downloading builder script: 100%"}},"a8a1028fb3504388a4798a496502c0a3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9d28b43412d40008f1f07691eec9663":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aac8a6ac1d0b4d3094ae27934e02ee5b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aae1dc15b15f4a19903d5221e178a8dd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac370f028c814f38a26f2a9dfb939104":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac479cbb932645bfa3b4527b38f0e0a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aae1dc15b15f4a19903d5221e178a8dd","placeholder":"​","style":"IPY_MODEL_296111eaaab2460e8aa0515eb103fe82","value":" 10000/10000 [00:01&lt;00:00, 7232.97 examples/s]"}},"acaea09850b6496da8d7dba5e19b85f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ad21dd22325a4d6393fb1d4a2472235f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ad2e01a5a3fb4bafb0dbb585c3f4c2aa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ada8af032996408d978fb5a87790c421":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ade01a070eb742e68e5f673c1cd93f02":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_89fb254c313844fd8b9408326c6778ab","max":10000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4ae19ac474044a859e515303c8252387","value":10000}},"ae12c82bb38142a5b000cffc5c1f87ea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aecfb08bc7b7419ea420ca0846370479":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0c634c3affe448998ba5d19ea0bf49a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b12b0976b05145e2b7fa4c937ad7a024":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b2bf3936028e46f284f0e133ab0442f0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b32fea3637774c6ea5a28ca4833606dd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4628f77f505e43d8b85d03e1c3553466","IPY_MODEL_8d7e655358124f109fea406a52dbb838","IPY_MODEL_e8ff6445711c48b8a69d81bf08ac4592"],"layout":"IPY_MODEL_ccc324b1a6a742d893f3b37707e043f1"}},"b3311519f56e41d492a4e9ef039f2560":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b3514618fe784696b1f92d3b0a2eba45":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_78163e08a3ca4e839a9d5e02c6a76b8f","IPY_MODEL_2ae893aba8124b5c8fb2662682b91c10","IPY_MODEL_844ce341d3f5416db18e8023b232de94"],"layout":"IPY_MODEL_a424cd7b6a5e45408a13aa6c8cd22a22"}},"b3c68f94de8c4237805f183a1f356f92":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b43474552fbd45c98ffa444b052e7a84":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2610a6e4ab1b48fa85ad3fa76fcf7f99","IPY_MODEL_d0d7b32a4d184a7abb1107b2e1f6afda","IPY_MODEL_589cf887c9f247e5a953f617a03a9b0c"],"layout":"IPY_MODEL_bf72b0307f9941cc8a74b99f2408e740"}},"b4802db5e3f540608795aa6393159201":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4c49dd9aa4f45da9badcf1f807f4ca4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b57cac2a661c4ca59cf3363c70c2915d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_60cd536e9a25432e85eeea404a11202c","placeholder":"​","style":"IPY_MODEL_b63646922d23402f97f8e1c315f164c5","value":"Downloading data: 100%"}},"b63646922d23402f97f8e1c315f164c5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b6b7be8c86e04cad97e037990f5372d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5f64a6498714d4987944334ef8e80b1","placeholder":"​","style":"IPY_MODEL_c021b3af3e384c109ace73099cf06332","value":"Running tokenizer on validation dataset: 100%"}},"b80a82523ffb4fbb875063313ebc3b8d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8ae4688334947abbdc800cfb12ce2de":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b8d0a24a93bb41b48cbe9c03dc2134a8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8e4ff5b1bd647cdb5f3fc79d3fd211e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d7ba530350c44babd55091e1172bee3","max":2275,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3803480e26994889b5295db64bfb54f8","value":2275}},"b9dfdd8934904f2bb3fdb11522641705":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ba3238bf9336424dabffeca80282ecf9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bae25b13f9174004ba576e137b21e180":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bae6efb7df4e488bbf7471c2f4838dc0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bb37f09a3d62447781b0b85dc630ab66":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bb74d1a827014de89a3adde9dea3b60e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc7a132b6e7e490388a999a505782b10":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bd1aa50b342e4b338d5053ee0ee4985b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_daac365702144c1bb550e4d98f0aa500","placeholder":"​","style":"IPY_MODEL_3b48afacc0834191b31963b71c333583","value":"Running tokenizer on train dataset: 100%"}},"be0edf59be5948f0a2613d2093b79c33":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"be83be31ea7f47f4a3e2e830b7d64dc7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_093daac294574bf5b036b2d9b30e3b18","max":60000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d5b3fab0ca424d58a2bf27127be12403","value":60000}},"bf518457f4494d6bb59f4ed33ee9f19e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e07411366c94542b1bc877705d0b2c0","max":60000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c55eacacaf7649a891adcd0c0242453e","value":60000}},"bf72b0307f9941cc8a74b99f2408e740":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"bfbf019599b9459596a8680831405baa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c021b3af3e384c109ace73099cf06332":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c06450233ebf4de88fd9fccda3365086":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c0abb82712b24bba86d4157e1f808ae9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cea610e123784c3f8700bd8f2507b32e","max":32885,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1efe37c4d4af44c08f57dc3f0e556a64","value":32885}},"c12c74e23fba4d4c998b897b7d264e62":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_23a7694490ff4e48aa9b9e84c5b3eed7","placeholder":"​","style":"IPY_MODEL_d440c2c66a05472aa91533dec8d55291","value":" 21.0k/21.0k [00:00&lt;00:00, 557kB/s]"}},"c2c316cbb1d84e5daf6eaae6198ce327":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c35d2103ebb64539884edcb111e83556":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3a65bed4ffa4bfc854f5054dcab5e71":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6e5c2cad02348d88005533c82c6ca92","placeholder":"​","style":"IPY_MODEL_da52333cda2e46b0906ef02729f85968","value":" 2275/2275 [00:00&lt;00:00, 8866.90 examples/s]"}},"c5148a331c324ae48e40d1aa37ce6dd5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c55afeb8d8c9476d8e1f0750b5b9c634":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb44a253e49f48d9a4c3cc25af56679c","placeholder":"​","style":"IPY_MODEL_10046d513ebf4c9782fa1f4926406fda","value":"Generating train split: 100%"}},"c55eacacaf7649a891adcd0c0242453e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c5f64a6498714d4987944334ef8e80b1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7427839bf174edeba2b97302e9e0744":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b57cac2a661c4ca59cf3363c70c2915d","IPY_MODEL_19b03633a6fc44b0b55e053bcd0e3c65","IPY_MODEL_f5a4de5788024c2d8ea62c2d0eadbfeb"],"layout":"IPY_MODEL_79eff3c76cf54d6ab44ff5a6b8afd0dc"}},"c74c1ca77e8145269c4a057504acca50":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c772bdb98003478cae852ed85ca61887":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_94c9e97efd6c4f628dbf73b443e1e2c9","placeholder":"​","style":"IPY_MODEL_2c38d4790dd649ec8827203406ab4f2b","value":" 2275/2275 [00:00&lt;00:00, 11408.26 examples/s]"}},"c82a86d204e74813bd3f1fd37e8ab70e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e97b9131dbe9438994a89fa2b3c5c976","max":10000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_db389b4b36514e8882272dd3e852c08e","value":10000}},"c83fc2e6ae784f738cb1ba8979054ea4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c914802ef7214ae1bffabdbb0a5ab917":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c94f0926e23043f9b8a71309b92019f2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c951de5e84044ec08ad22b9f867e8cd9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9f90de294d941f2a951aaad8a6f7d10":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_041c1c2c385f439b9975cc5a73758f7b","max":5532,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d3d4fb94563c46df8e13ee78dc6957a9","value":5532}},"ca348fca7a1444c681c8e9249cffad65":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cbdab9b8ebc64392b6f20859d78060a4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0b16d58053a045849897820e3a7ffe83","IPY_MODEL_f7c34df14bf5459288935eaa5f2bb27a","IPY_MODEL_0b5e53eb379a46fab9a80de7e89db759"],"layout":"IPY_MODEL_a05ffac668684b9c8086cc6097700f5d"}},"ccc324b1a6a742d893f3b37707e043f1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ccdf056b54d34126ab4e07762e63ac3a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ce68de4bc52a4d009526da735fa04a8b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7734d35f5d948d1b96e4b3420af92f2","max":16255623,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e36087148ed14dde8cad0ba372d6a958","value":16255623}},"cea52e6d84384c23ba5e454d48c0b1c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4802db5e3f540608795aa6393159201","placeholder":"​","style":"IPY_MODEL_1b102b8c2d484f529dcdcb5b7a5a058c","value":" 5532/5532 [00:00&lt;00:00, 17014.62 examples/s]"}},"cea610e123784c3f8700bd8f2507b32e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cec4f7284e424a17a0a97d4d59754f86":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0d7b32a4d184a7abb1107b2e1f6afda":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac370f028c814f38a26f2a9dfb939104","max":1400,"min":0,"orientation":"horizontal","style":"IPY_MODEL_13edfeef3f8e4f09ae25e07efe3e7f29","value":1400}},"d21080b841624469a3c1f902a3761cb9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef7d31e3ad28466fb2801fb577e77008","placeholder":"​","style":"IPY_MODEL_5979352ca84d49e8a2550af96ea06dbe","value":"Running tokenizer on prediction dataset: 100%"}},"d3c436fd27754afaa7af0853548a29b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d3d4fb94563c46df8e13ee78dc6957a9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d3f8781634744768bc5be832a0678282":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d440c2c66a05472aa91533dec8d55291":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d5b3fab0ca424d58a2bf27127be12403":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d7734d35f5d948d1b96e4b3420af92f2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d784552dce224dc1a2f0f2a3c75408ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4a26bb02a99473d977b4a52f9f7a312","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e6bcdebbc7e44e0a9a8659c87ec2eef2","value":1042301}},"d7c4922a13024eb19b0a39124cd571fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d82b200fd9ee4b0dbd0efa3ddb704384":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d959e20713a04777addce249dfd18a06":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9afaf3b0bed424a8c2336bc3a85db80":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"da52333cda2e46b0906ef02729f85968":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"da93f3aaa34b427a8fd9e73dd85ab047":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd8952e4fd87410f85a9470f0279f568","max":2275,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bae6efb7df4e488bbf7471c2f4838dc0","value":2275}},"daac365702144c1bb550e4d98f0aa500":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db389b4b36514e8882272dd3e852c08e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dd8952e4fd87410f85a9470f0279f568":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de998f33582046e4b6c114024fab5682":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3456d453a5a84bf4bdfd6ae8a75c7f4e","max":2275,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ada8af032996408d978fb5a87790c421","value":2275}},"df7f8b8230dc4313b24f88f8bfee5222":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfe8ea0932f442e5af001317921794a8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e003feb1cbdd4ea8b92f1d397fe5b052":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0169761bfb54da990a144ef63ada248":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb74d1a827014de89a3adde9dea3b60e","max":1607,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8e5d55c80c204b9aa1e5e551f41b1c9c","value":1607}},"e0de6a433af24f2fa8995f0035feeb29":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1fa3b8dd78d9437799087ab4bb1bccd4","IPY_MODEL_70749376b1b4496d9b54a5e011c0b50c","IPY_MODEL_279d2edac05b4dad830b87c6d0977599"],"layout":"IPY_MODEL_7d2d97335fae456483df4b0912ade350"}},"e0e00fad3a0e466faa21e492c9ab483d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e13ac8b55c2b46c6b9b089269d550ebc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_07f57c4aa65a4ee0adac45bf2b94ebc2","placeholder":"​","style":"IPY_MODEL_ae12c82bb38142a5b000cffc5c1f87ea","value":"Generating validation split: 100%"}},"e36087148ed14dde8cad0ba372d6a958":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e42adafaed124a4a8ba1551451fb20b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_761448a6a3584487bd4a0eef06122ce7","placeholder":"​","style":"IPY_MODEL_4d46c83672584647ab676514936de69f","value":"Generating validation split: 100%"}},"e4720ef6343a412bb30e29cf9d3d9a3c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e4737d5039084a46bca644028fad843e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4a26bb02a99473d977b4a52f9f7a312":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6bcdebbc7e44e0a9a8659c87ec2eef2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e6eb8ad47044414f936c44b75fdb5c2e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0c634c3affe448998ba5d19ea0bf49a","placeholder":"​","style":"IPY_MODEL_77904bf74a2340f09bc9c59ca8d82bdf","value":" 1607/1607 [00:00&lt;00:00, 9552.07 examples/s]"}},"e712427370224a6498457db89aec1d23":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5e5e6d22764d4cd8a18e7367038c37a7","IPY_MODEL_c82a86d204e74813bd3f1fd37e8ab70e","IPY_MODEL_ac479cbb932645bfa3b4527b38f0e0a2"],"layout":"IPY_MODEL_fd6fd0d418b14bde98f2590a3806843b"}},"e7d0d3942fbc41a79bc9ab75db47d1f7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e82cb59e81f84f1ebeb500a1cd4192fd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8ff6445711c48b8a69d81bf08ac4592":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5832b1b1c5d5415c83038598caf8cf6a","placeholder":"​","style":"IPY_MODEL_8aeb0d302fa14feabd080529ab5c57e4","value":" 570/570 [00:00&lt;00:00, 8.66kB/s]"}},"e97b9131dbe9438994a89fa2b3c5c976":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed93be7ed0e94c4abef4b788c4f16c72":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee2773b8e9cd4c39af6a7f10e3ae4f48":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e003feb1cbdd4ea8b92f1d397fe5b052","placeholder":"​","style":"IPY_MODEL_795eb28905d145038df3c7b896e9d1a8","value":" 10000/10000 [00:01&lt;00:00, 15291.85 examples/s]"}},"ef4504de431c4dad8dcec37a960ccd06":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef7d31e3ad28466fb2801fb577e77008":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f03d1b35e95744a7a2f0a3def9a7adea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_45e493381f1047bcb023f4bc87681ac3","IPY_MODEL_d784552dce224dc1a2f0f2a3c75408ed","IPY_MODEL_0cd5694aac534d55822cd56d694920e6"],"layout":"IPY_MODEL_b80a82523ffb4fbb875063313ebc3b8d"}},"f1cb9844b7b9492fa3feb5616cdb5d6d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f6938181e764e739162991a7c6a10ea","placeholder":"​","style":"IPY_MODEL_90667c8574a04077b5d51e0b70e8908a","value":"Generating validation split: 100%"}},"f2f88b76ef6549c0bf5bb673960add6a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ea84f047b0f4a0083fb61a4869edd09","placeholder":"​","style":"IPY_MODEL_1e07598ff0c847f0938a44a9b0c4ef0e","value":"Generating test split: 100%"}},"f4809f95c8f541a48bdeddeb7f2c681c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5a4de5788024c2d8ea62c2d0eadbfeb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_df7f8b8230dc4313b24f88f8bfee5222","placeholder":"​","style":"IPY_MODEL_b3311519f56e41d492a4e9ef039f2560","value":" 511k/511k [00:00&lt;00:00, 1.76MB/s]"}},"f5cbf7a5d5d049c9a30ea5391d19e6d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f6231b35767542d3a02b2d201a4a76e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f66b64b9d9d9455397e14144435215f3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d21080b841624469a3c1f902a3761cb9","IPY_MODEL_ade01a070eb742e68e5f673c1cd93f02","IPY_MODEL_ff9a18eaa3d0460ba54072515f5a43ca"],"layout":"IPY_MODEL_d3f8781634744768bc5be832a0678282"}},"f7c34df14bf5459288935eaa5f2bb27a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1283fab055c415b98e9b3368c83c2ae","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dfe8ea0932f442e5af001317921794a8","value":231508}},"f92e669990784453bf5854c8b38b13fd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa2b279f40164f1f9de2d3f7cfec4472":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_686a423d59c34d7789815d5be405bae9","placeholder":"​","style":"IPY_MODEL_38b40dafc599424a91b5b465d76f7f21","value":" 23.3k/23.3k [00:00&lt;00:00, 673kB/s]"}},"fb44a253e49f48d9a4c3cc25af56679c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc7bbe67316149b1a0c5491448875588":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3fffc34d1bdc44db8c06a023d26b2f02","max":511342,"min":0,"orientation":"horizontal","style":"IPY_MODEL_42adbd27a7bd49c385a0d58b833f7154","value":511342}},"fcfff04f32524fa28f02686651e0211f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd38229f818f4aeeba121c404c615039":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fd6fd0d418b14bde98f2590a3806843b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff9a18eaa3d0460ba54072515f5a43ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d93ec15abcc4df6b67ce8f72e884ddf","placeholder":"​","style":"IPY_MODEL_ad21dd22325a4d6393fb1d4a2472235f","value":" 10000/10000 [00:01&lt;00:00, 7498.06 examples/s]"}}}}},"nbformat":4,"nbformat_minor":0}