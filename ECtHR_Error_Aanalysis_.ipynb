{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49908,"status":"ok","timestamp":1693746409048,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"},"user_tz":-60},"id":"mPbkVfpo90HG","outputId":"b1c18821-63b1-4966-a343-bb5280156aab"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch) (16.0.6)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch) (2.1.3)\n","Requirement already satisfied: mpmath\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-\u003etorch) (1.3.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.32.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,\u003c0.14,\u003e=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors\u003e=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.15.1-\u003etransformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.15.1-\u003etransformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (3.2.0)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (3.4)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (2.0.4)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (2023.7.22)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy\u003e=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n","Requirement already satisfied: scipy\u003e=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n","Requirement already satisfied: joblib\u003e=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n","Requirement already satisfied: threadpoolctl\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.4)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow\u003e=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: dill\u003c0.3.8,\u003e=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests\u003e=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm\u003e=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec[http]\u003e=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n","Requirement already satisfied: huggingface-hub\u003c1.0.0,\u003e=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: attrs\u003e=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer\u003c4.0,\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (3.2.0)\n","Requirement already satisfied: multidict\u003c7.0,\u003e=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (6.0.4)\n","Requirement already satisfied: async-timeout\u003c5.0,\u003e=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (4.0.3)\n","Requirement already satisfied: yarl\u003c2.0,\u003e=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (1.9.2)\n","Requirement already satisfied: frozenlist\u003e=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (1.4.0)\n","Requirement already satisfied: aiosignal\u003e=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (1.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0.0,\u003e=0.14.0-\u003edatasets) (3.12.2)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0.0,\u003e=0.14.0-\u003edatasets) (4.7.1)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (3.4)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (2.0.4)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (2023.7.22)\n","Requirement already satisfied: python-dateutil\u003e=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas-\u003edatasets) (2.8.2)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-\u003edatasets) (2023.3)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil\u003e=2.8.1-\u003epandas-\u003edatasets) (1.16.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex\u003e=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.10.1)\n","Requirement already satisfied: numpy\u003c1.27.0,\u003e=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.23.5)\n","Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.32.1)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.22.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n","Requirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.16.4)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.1)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,\u003c0.14,\u003e=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.13.3)\n","Requirement already satisfied: safetensors\u003e=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.3.3)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n","Requirement already satisfied: torch!=1.12.0,\u003e=1.9 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.15.1-\u003etransformers[torch]) (2023.6.0)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.15.1-\u003etransformers[torch]) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,\u003e=1.9-\u003etransformers[torch]) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,\u003e=1.9-\u003etransformers[torch]) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,\u003e=1.9-\u003etransformers[torch]) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,\u003e=1.9-\u003etransformers[torch]) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch!=1.12.0,\u003e=1.9-\u003etransformers[torch]) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch!=1.12.0,\u003e=1.9-\u003etransformers[torch]) (16.0.6)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers[torch]) (3.2.0)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers[torch]) (3.4)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers[torch]) (2.0.4)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers[torch]) (2023.7.22)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch!=1.12.0,\u003e=1.9-\u003etransformers[torch]) (2.1.3)\n","Requirement already satisfied: mpmath\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-\u003etorch!=1.12.0,\u003e=1.9-\u003etransformers[torch]) (1.3.0)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.4)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow\u003e=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: dill\u003c0.3.8,\u003e=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests\u003e=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm\u003e=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec[http]\u003e=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n","Requirement already satisfied: huggingface-hub\u003c1.0.0,\u003e=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: attrs\u003e=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer\u003c4.0,\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (3.2.0)\n","Requirement already satisfied: multidict\u003c7.0,\u003e=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (6.0.4)\n","Requirement already satisfied: async-timeout\u003c5.0,\u003e=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (4.0.3)\n","Requirement already satisfied: yarl\u003c2.0,\u003e=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (1.9.2)\n","Requirement already satisfied: frozenlist\u003e=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (1.4.0)\n","Requirement already satisfied: aiosignal\u003e=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (1.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0.0,\u003e=0.14.0-\u003edatasets) (3.12.2)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0.0,\u003e=0.14.0-\u003edatasets) (4.7.1)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (3.4)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (2.0.4)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (2023.7.22)\n","Requirement already satisfied: python-dateutil\u003e=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas-\u003edatasets) (2.8.2)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-\u003edatasets) (2023.3)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil\u003e=2.8.1-\u003epandas-\u003edatasets) (1.16.0)\n","Requirement already satisfied: nlpaug in /usr/local/lib/python3.10/dist-packages (1.1.11)\n","Requirement already satisfied: numpy\u003e=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.23.5)\n","Requirement already satisfied: pandas\u003e=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.5.3)\n","Requirement already satisfied: requests\u003e=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.31.0)\n","Requirement already satisfied: gdown\u003e=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (4.6.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown\u003e=4.0.0-\u003enlpaug) (3.12.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown\u003e=4.0.0-\u003enlpaug) (1.16.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown\u003e=4.0.0-\u003enlpaug) (4.66.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown\u003e=4.0.0-\u003enlpaug) (4.11.2)\n","Requirement already satisfied: python-dateutil\u003e=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas\u003e=1.2.0-\u003enlpaug) (2.8.2)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas\u003e=1.2.0-\u003enlpaug) (2023.3)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.22.0-\u003enlpaug) (3.2.0)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.22.0-\u003enlpaug) (3.4)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.22.0-\u003enlpaug) (2.0.4)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.22.0-\u003enlpaug) (2023.7.22)\n","Requirement already satisfied: soupsieve\u003e1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4-\u003egdown\u003e=4.0.0-\u003enlpaug) (2.4.1)\n","Requirement already satisfied: PySocks!=1.5.7,\u003e=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.22.0-\u003enlpaug) (1.7.1)\n"]}],"source":["! pip install torch\n","! pip install transformers\n","! pip install scikit-learn\n","! pip install tqdm\n","! pip install numpy\n","! pip install datasets\n","! pip install nltk\n","import nltk\n","nltk.download('stopwords')\n","! pip install scipy\n","! pip install transformers[torch] accelerate\n","\n","! pip install datasets\n","import json\n","from datasets import Dataset\n","\n","import pandas as pd\n","! pip install nlpaug"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36922,"status":"ok","timestamp":1693746445965,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"},"user_tz":-60},"id":"cVNLetLL-Eaw","outputId":"be15856e-f3a9-4087-d0a7-a570c444aa59"},"outputs":[{"name":"stderr","output_type":"stream","text":["[WARNING|modeling_utils.py:3553] 2023-09-03 13:07:09,823 \u003e\u003e Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from dataclasses import dataclass\n","from typing import Optional, Tuple\n","\n","import torch\n","import numpy as np\n","from torch import nn\n","from transformers.file_utils import ModelOutput\n","\n","\n","@dataclass\n","class SimpleOutput(ModelOutput):\n","    last_hidden_state: torch.FloatTensor = None\n","    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n","    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n","    attentions: Optional[Tuple[torch.FloatTensor]] = None\n","    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n","\n","\n","def sinusoidal_init(num_embeddings: int, embedding_dim: int):\n","    # keep dim 0 for padding token position encoding zero vector\n","    position_enc = np.array([\n","        [pos / np.power(10000, 2 * i / embedding_dim) for i in range(embedding_dim)]\n","        if pos != 0 else np.zeros(embedding_dim) for pos in range(num_embeddings)])\n","\n","    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2])  # dim 2i\n","    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2])  # dim 2i+1\n","    return torch.from_numpy(position_enc).type(torch.FloatTensor)\n","\n","\n","class HierarchicalBert(nn.Module):\n","\n","    def __init__(self, encoder, max_segments=64, max_segment_length=128):\n","        super(HierarchicalBert, self).__init__()\n","        supported_models = ['bert', 'roberta', 'deberta']\n","        assert encoder.config.model_type in supported_models  # other model types are not supported so far\n","        # Pre-trained segment (token-wise) encoder, e.g., BERT\n","        self.encoder = encoder\n","        # Specs for the segment-wise encoder\n","        self.hidden_size = encoder.config.hidden_size\n","        self.max_segments = max_segments\n","        self.max_segment_length = max_segment_length\n","        # Init sinusoidal positional embeddings\n","        self.seg_pos_embeddings = nn.Embedding(max_segments + 1, encoder.config.hidden_size,\n","                                               padding_idx=0,\n","                                               _weight=sinusoidal_init(max_segments + 1, encoder.config.hidden_size))\n","        # Init segment-wise transformer-based encoder\n","        self.seg_encoder = nn.Transformer(d_model=encoder.config.hidden_size,\n","                                          nhead=encoder.config.num_attention_heads,\n","                                          batch_first=True, dim_feedforward=encoder.config.intermediate_size,\n","                                          activation=encoder.config.hidden_act,\n","                                          dropout=encoder.config.hidden_dropout_prob,\n","                                          layer_norm_eps=encoder.config.layer_norm_eps,\n","                                          num_encoder_layers=2, num_decoder_layers=0).encoder\n","\n","    def forward(self,\n","                input_ids=None,\n","                attention_mask=None,\n","                token_type_ids=None,\n","                position_ids=None,\n","                head_mask=None,\n","                inputs_embeds=None,\n","                labels=None,\n","                output_attentions=None,\n","                output_hidden_states=None,\n","                return_dict=None,\n","                ):\n","        # Hypothetical Example\n","        # Batch of 4 documents: (batch_size, n_segments, max_segment_length) --\u003e (4, 64, 128)\n","        # BERT-BASE encoder: 768 hidden units\n","\n","        # Squash samples and segments into a single axis (batch_size * n_segments, max_segment_length) --\u003e (256, 128)\n","        input_ids_reshape = input_ids.contiguous().view(-1, input_ids.size(-1))\n","        attention_mask_reshape = attention_mask.contiguous().view(-1, attention_mask.size(-1))\n","        if token_type_ids is not None:\n","            token_type_ids_reshape = token_type_ids.contiguous().view(-1, token_type_ids.size(-1))\n","        else:\n","            token_type_ids_reshape = None\n","\n","        # Encode segments with BERT --\u003e (256, 128, 768)\n","        encoder_outputs = self.encoder(input_ids=input_ids_reshape,\n","                                       attention_mask=attention_mask_reshape,\n","                                       token_type_ids=token_type_ids_reshape)[0]\n","\n","        # Reshape back to (batch_size, n_segments, max_segment_length, output_size) --\u003e (4, 64, 128, 768)\n","        encoder_outputs = encoder_outputs.contiguous().view(input_ids.size(0), self.max_segments,\n","                                                            self.max_segment_length,\n","                                                            self.hidden_size)\n","\n","        # Gather CLS outputs per segment --\u003e (4, 64, 768)\n","        encoder_outputs = encoder_outputs[:, :, 0]\n","\n","        # Infer real segments, i.e., mask paddings\n","        seg_mask = (torch.sum(input_ids, 2) != 0).to(input_ids.dtype)\n","        # Infer and collect segment positional embeddings\n","        seg_positions = torch.arange(1, self.max_segments + 1).to(input_ids.device) * seg_mask\n","        # Add segment positional embeddings to segment inputs\n","        encoder_outputs += self.seg_pos_embeddings(seg_positions)\n","\n","        # Encode segments with segment-wise transformer\n","        seg_encoder_outputs = self.seg_encoder(encoder_outputs)\n","\n","        # Collect document representation\n","        outputs, _ = torch.max(seg_encoder_outputs, 1)\n","\n","        return SimpleOutput(last_hidden_state=outputs, hidden_states=outputs)\n","\n","\n","if __name__ == \"__main__\":\n","    from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n","    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","\n","    # Use as a stand-alone encoder\n","    bert = AutoModel.from_pretrained('bert-base-uncased')\n","    model = HierarchicalBert(encoder=bert, max_segments=64, max_segment_length=128)\n","\n","    fake_inputs = {'input_ids': [], 'attention_mask': [], 'token_type_ids': []}\n","    for i in range(4):\n","        # Tokenize segment\n","        temp_inputs = tokenizer(['dog ' * 126] * 64)\n","        fake_inputs['input_ids'].append(temp_inputs['input_ids'])\n","        fake_inputs['attention_mask'].append(temp_inputs['attention_mask'])\n","        fake_inputs['token_type_ids'].append(temp_inputs['token_type_ids'])\n","\n","    fake_inputs['input_ids'] = torch.as_tensor(fake_inputs['input_ids'])\n","    fake_inputs['attention_mask'] = torch.as_tensor(fake_inputs['attention_mask'])\n","    fake_inputs['token_type_ids'] = torch.as_tensor(fake_inputs['token_type_ids'])\n","\n","    output = model(fake_inputs['input_ids'], fake_inputs['attention_mask'], fake_inputs['token_type_ids'])\n","\n","    # 4 document representations of 768 features are expected\n","    assert output[0].shape == torch.Size([4, 768])\n","\n","    # Use with HuggingFace AutoModelForSequenceClassification and Trainer API\n","\n","    # Init Classifier\n","    model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=10)\n","    # Replace flat BERT encoder with hierarchical BERT encoder\n","    model.bert = HierarchicalBert(encoder=model.bert, max_segments=64, max_segment_length=128)\n","    output = model(fake_inputs['input_ids'], fake_inputs['attention_mask'], fake_inputs['token_type_ids'])\n","\n","    # 4 document outputs with 10 (num_labels) logits are expected\n","    assert output.logits.shape == torch.Size([4, 10])\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":36,"status":"ok","timestamp":1693746445966,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"},"user_tz":-60},"id":"xL0qmiOg-K_E"},"outputs":[],"source":["from torch import nn\n","from transformers import Trainer\n","\n","\n","class MultilabelTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.pop(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","        loss_fct = nn.BCEWithLogitsLoss()\n","        loss = loss_fct(logits.view(-1, self.model.config.num_labels),\n","                        labels.float().view(-1, self.model.config.num_labels))\n","        return (loss, outputs) if return_outputs else loss\n","\n","\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1769,"status":"ok","timestamp":1693725341280,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"},"user_tz":-60},"id":"FjgzToay-TPc","outputId":"1f7f1b38-d390-4f9e-a861-b0b7c1662089"},"outputs":[{"name":"stdout","output_type":"stream","text":["*********\n","0\n","#####\n","1000\n","Number of Texts: 1000\n","Number of Labels: 1000\n","Most Common Original Label: 3\n","Occurrences: 394\n","First Occurrence Index of Label : 2\n","Text: ['5.  The applicant was born in 1965 and lives in Smědčice.', '6.  On 9 November 2006 the applicant requested a building permit for temporary stables for horses. On 6 January 2011 the Rokycany Planning Office (stavební úřad) dismissed his request and on 26 May 2011 the Plzeň Regional Office (krajský úřad) upheld that decision.', '7.  On 29 March 2013 the Plzeň Regional Court (krajský soud) dismissed a complaint lodged by the applicant against the decision of the Plzeň Regional Office.', '8.  On 31 July 2013 the Supreme Administrative Court (Nejvyšší správní soud) dismissed an appeal on points of law lodged by the applicant. The decision was served on the applicant on 28 August 2013.', '9.  On 29 October 2013 the applicant lodged a constitutional complaint (ústavní stížnost).', '10.  On 31 March 2014 the Constitutional Court (Ústavní soud) rejected the applicant’s appeal as being lodged out of time. It held that as the Supreme Administrative Court’s decision had been served on him on 28 August 2013, the last day of the two-month time-limit for lodging a constitutional appeal was 28 October 2013.', '11.  On 8 April 2014 the applicant wrote to the Constitutional Court urging it to set aside its decision. He argued that as 28 October 2013 had been a national holiday, domestic procedural rules provided that the last day for lodging his appeal had been the following day, namely 29 October 2013.', '12.  By a letter of 11 April 2014 the Registrar (generální sekretář) of the Constitutional Court acknowledged that the judge-rapporteur had undoubtedly overlooked the fact that the time-limit had been complied with. However, as the Constitutional Court did not have the power to set aside its own decision, he advised the applicant to lodge an application with the European Court of Human Rights.']\n"]}],"source":["##EcTHR(B)\n","# Load JSON data from file\n","file_path = '/content/sample_data/incorrect_predictions_dataset (3).json'\n","\n","with open(file_path, 'r') as json_file:\n","    data = json.load(json_file)\n","\n","# Create a dictionary to store original_labels\n","labels_dict = {}\n","text_dict ={}\n","labels =[]\n","\n","# Iterate through the data and extract original_labels\n","for entry in data:\n","    if \"original_labels\" in entry:\n","        original_labels = entry[\"original_labels\"]\n","        if \"Index\" in entry:\n","            index = entry[\"Index\"]\n","            labels_dict[index] = original_labels\n","        if \"text\" in entry:\n","            text = entry[\"text\"]\n","            text_dict[index]={\n","                \"original_labels\": original_labels,\n","                \"text\": text\n","            }\n","        elif len(labels_dict) \u003e 0:\n","            last_index = max(labels_dict.keys())\n","            labels_dict[last_index].extend(original_labels)\n","        labels.append(original_labels)\n","\n","print(\"*********\")\n","print(len(text_dict))\n","# Initialize lists to store texts and labels\n","texts = []\n","text= []\n","#labels = label_lists[0:173]\n","print(\"#####\")\n","print(len(labels))\n","# Iterate through the data and extract relevant information\n","for entry in data:\n","    if \"text\" in entry:\n","        text = entry[\"text\"]\n","        if \"Index\" in entry:\n","            index = entry[\"Index\"]\n","            #labels.append(labels_dict.get(index, []))\n","        else:\n","            # Use the last index available in labels_dict\n","            last_index = max(labels_dict.keys())\n","            #labels.append(labels_dict.get(last_index, []))\n","        texts.append(text)\n","\n","print(\"Number of Texts:\", len(texts))\n","print(\"Number of Labels:\", len(labels))\n","\n","original_label_counts = {}\n","for label_list in labels:\n","    for label in label_list:\n","        if label in original_label_counts:\n","            original_label_counts[label] += 1\n","        else:\n","            original_label_counts[label] = 1\n","\n","# Find the most occurring original label\n","most_common_original_label = max(original_label_counts, key=original_label_counts.get)\n","print(\"Most Common Original Label:\", most_common_original_label)\n","print(\"Occurrences:\", original_label_counts[most_common_original_label])\n","\n","\n","first_occurrence_index = None\n","for index, label_list in enumerate(labels):\n","    if most_common_original_label in label_list:\n","        first_occurrence_index = index\n","        break\n","\n","print(\"First Occurrence Index of Label :\", first_occurrence_index)\n","\n","print(\"Text:\", texts[first_occurrence_index])\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1032,"status":"ok","timestamp":1693746446966,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"},"user_tz":-60},"id":"PkPzVmmX-WLN","outputId":"dcd3dad8-3b13-45e0-8d19-8565e12585be"},"outputs":[{"name":"stdout","output_type":"stream","text":["*********\n","0\n","#####\n","1000\n","Number of Texts: 1000\n","Number of Labels: 1000\n","Most Common Original Label: 3\n","Occurrences: 299\n","First Occurrence Index of Label : 2\n","Text: ['5.  The applicant was born in 1965 and lives in Smědčice.', '6.  On 9 November 2006 the applicant requested a building permit for temporary stables for horses. On 6 January 2011 the Rokycany Planning Office (stavební úřad) dismissed his request and on 26 May 2011 the Plzeň Regional Office (krajský úřad) upheld that decision.', '7.  On 29 March 2013 the Plzeň Regional Court (krajský soud) dismissed a complaint lodged by the applicant against the decision of the Plzeň Regional Office.', '8.  On 31 July 2013 the Supreme Administrative Court (Nejvyšší správní soud) dismissed an appeal on points of law lodged by the applicant. The decision was served on the applicant on 28 August 2013.', '9.  On 29 October 2013 the applicant lodged a constitutional complaint (ústavní stížnost).', '10.  On 31 March 2014 the Constitutional Court (Ústavní soud) rejected the applicant’s appeal as being lodged out of time. It held that as the Supreme Administrative Court’s decision had been served on him on 28 August 2013, the last day of the two-month time-limit for lodging a constitutional appeal was 28 October 2013.', '11.  On 8 April 2014 the applicant wrote to the Constitutional Court urging it to set aside its decision. He argued that as 28 October 2013 had been a national holiday, domestic procedural rules provided that the last day for lodging his appeal had been the following day, namely 29 October 2013.', '12.  By a letter of 11 April 2014 the Registrar (generální sekretář) of the Constitutional Court acknowledged that the judge-rapporteur had undoubtedly overlooked the fact that the time-limit had been complied with. However, as the Constitutional Court did not have the power to set aside its own decision, he advised the applicant to lodge an application with the European Court of Human Rights.']\n"]}],"source":["##EctHR(A)\n","# Load JSON data from file\n","file_path = '/content/sample_data/incorrect_predictions_dataset (6).json'\n","\n","with open(file_path, 'r') as json_file:\n","    data = json.load(json_file)\n","\n","# Create a dictionary to store original_labels\n","labels_dict = {}\n","text_dict ={}\n","labels =[]\n","\n","# Iterate through the data and extract original_labels\n","for entry in data:\n","    if \"original_labels\" in entry:\n","        original_labels = entry[\"original_labels\"]\n","        if \"Index\" in entry:\n","            index = entry[\"Index\"]\n","            labels_dict[index] = original_labels\n","        if \"text\" in entry:\n","            text = entry[\"text\"]\n","            text_dict[index]={\n","                \"original_labels\": original_labels,\n","                \"text\": text\n","            }\n","        elif len(labels_dict) \u003e 0:\n","            last_index = max(labels_dict.keys())\n","            labels_dict[last_index].extend(original_labels)\n","        labels.append(original_labels)\n","\n","print(\"*********\")\n","print(len(text_dict))\n","# Initialize lists to store texts and labels\n","texts = []\n","text= []\n","#labels = label_lists[0:173]\n","print(\"#####\")\n","print(len(labels))\n","# Iterate through the data and extract relevant information\n","for entry in data:\n","    if \"text\" in entry:\n","        text = entry[\"text\"]\n","        if \"Index\" in entry:\n","            index = entry[\"Index\"]\n","            #labels.append(labels_dict.get(index, []))\n","        else:\n","            # Use the last index available in labels_dict\n","            last_index = max(labels_dict.keys())\n","            #labels.append(labels_dict.get(last_index, []))\n","        texts.append(text)\n","\n","print(\"Number of Texts:\", len(texts))\n","print(\"Number of Labels:\", len(labels))\n","\n","original_label_counts = {}\n","for label_list in labels:\n","    for label in label_list:\n","        if label in original_label_counts:\n","            original_label_counts[label] += 1\n","        else:\n","            original_label_counts[label] = 1\n","\n","# Find the most occurring original label\n","most_common_original_label = max(original_label_counts, key=original_label_counts.get)\n","print(\"Most Common Original Label:\", most_common_original_label)\n","print(\"Occurrences:\", original_label_counts[most_common_original_label])\n","\n","\n","first_occurrence_index = None\n","for index, label_list in enumerate(labels):\n","    if most_common_original_label in label_list:\n","        first_occurrence_index = index\n","        break\n","\n","print(\"First Occurrence Index of Label :\", first_occurrence_index)\n","\n","print(\"Text:\", texts[first_occurrence_index])\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4251,"status":"ok","timestamp":1693746451215,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"},"user_tz":-60},"id":"LyL2XS3K-ZUQ","outputId":"1664c659-4d52-4774-8779-d72fc14da876"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original Text: ['5.  The applicant was born in 1965 and lives in Smědčice.', '6.  On 9 November 2006 the applicant requested a building permit for temporary stables for horses. On 6 January 2011 the Rokycany Planning Office (stavební úřad) dismissed his request and on 26 May 2011 the Plzeň Regional Office (krajský úřad) upheld that decision.', '7.  On 29 March 2013 the Plzeň Regional Court (krajský soud) dismissed a complaint lodged by the applicant against the decision of the Plzeň Regional Office.', '8.  On 31 July 2013 the Supreme Administrative Court (Nejvyšší správní soud) dismissed an appeal on points of law lodged by the applicant. The decision was served on the applicant on 28 August 2013.', '9.  On 29 October 2013 the applicant lodged a constitutional complaint (ústavní stížnost).', '10.  On 31 March 2014 the Constitutional Court (Ústavní soud) rejected the applicant’s appeal as being lodged out of time. It held that as the Supreme Administrative Court’s decision had been served on him on 28 August 2013, the last day of the two-month time-limit for lodging a constitutional appeal was 28 October 2013.', '11.  On 8 April 2014 the applicant wrote to the Constitutional Court urging it to set aside its decision. He argued that as 28 October 2013 had been a national holiday, domestic procedural rules provided that the last day for lodging his appeal had been the following day, namely 29 October 2013.', '12.  By a letter of 11 April 2014 the Registrar (generální sekretář) of the Constitutional Court acknowledged that the judge-rapporteur had undoubtedly overlooked the fact that the time-limit had been complied with. However, as the Constitutional Court did not have the power to set aside its own decision, he advised the applicant to lodge an application with the European Court of Human Rights.']\n","100\n","101\n"]}],"source":["import nlpaug.augmenter.word as naw\n","import random\n","\n","# Most common original label and corresponding text\n","most_common_original_label = most_common_original_label\n","corresponding_text = texts[first_occurrence_index]\n","\n","# Initialize augmenter\n","aug = naw.SynonymAug(aug_src='wordnet', aug_p=0.3)\n","\n","# Apply data augmentation to the corresponding text\n","augmented_texts = [aug.augment(corresponding_text) for _ in range(100)]  # Generate 5 augmented examples\n","\n","# Print augmented texts\n","print(\"Original Text:\", corresponding_text)\n","print(len(augmented_texts))\n","\n","augmented_labels=[]\n","for i in range(len(augmented_texts)+1):\n","    augmented_labels.append([most_common_original_label])\n","\n","print(len(augmented_labels))"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"elapsed":3374,"status":"ok","timestamp":1693746454571,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"},"user_tz":-60},"id":"GwqN-i6A-izh","outputId":"bc8fe561-c63e-41ee-b1a1-8bfb1fc1a995"},"outputs":[{"name":"stdout","output_type":"stream","text":["9000\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2564f82134b64c60bf6072afdcda4e43","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/101 [00:00\u003c?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["9101\n"]}],"source":["from datasets import load_dataset\n","existing_dataset = load_dataset(\"lex_glue\", 'ecthr_a')\n","augmented_texts_flat = [text[0] for text in augmented_texts]\n","data = {\n","    \"text\": [corresponding_text] + augmented_texts_flat,\n","    \"label\": [most_common_original_label] * (len(augmented_texts) + 1)  # Use the most common label\n","}\n","augmented_dataset = Dataset.from_dict(data)\n","\n","# Create a Dataset instance\n","print(len(existing_dataset[\"train\"][\"text\"]))\n","augmented_dataset = Dataset.from_dict(data)\n","\n","data_dict = {\"text\": augmented_dataset[\"text\"], \"labels\": augmented_labels}\n","\n","# Create a dataset using the data dictionary\n","dataset = Dataset.from_dict(data_dict)\n","\n","# Convert the \"text\" feature to a Sequence of strings\n","dataset = dataset.map(lambda example: {\"text\": [example[\"text\"]]})\n","\n","#flat_labels = [label[0] for label in augmented_labels]\n","merged_dataset = Dataset.from_dict({\n","    \"text\": existing_dataset[\"train\"][\"text\"] + dataset[\"text\"],\n","    \"labels\": existing_dataset[\"train\"][\"labels\"] + augmented_labels,\n","})\n","print(len(merged_dataset[\"text\"]))"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":913},"executionInfo":{"elapsed":6550260,"status":"ok","timestamp":1693745502253,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"},"user_tz":-60},"id":"kfqm4MwV-pAK","outputId":"cbd85d3c-1e15-43dc-fbff-18c6440e70b9"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n","[WARNING|modeling_utils.py:3553] 2023-09-03 11:02:38,513 \u003e\u003e Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"06948cbf67de48a9924ff4a07a4d24e2","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on train dataset:   0%|          | 0/9101 [00:00\u003c?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    \u003cdiv\u003e\n","      \n","      \u003cprogress value='4552' max='4552' style='width:300px; height:20px; vertical-align: middle;'\u003e\u003c/progress\u003e\n","      [4552/4552 1:44:42, Epoch 2/2]\n","    \u003c/div\u003e\n","    \u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n"," \u003ctr style=\"text-align: left;\"\u003e\n","      \u003cth\u003eStep\u003c/th\u003e\n","      \u003cth\u003eTraining Loss\u003c/th\u003e\n","      \u003cth\u003eValidation Loss\u003c/th\u003e\n","      \u003cth\u003eMacro-f1\u003c/th\u003e\n","      \u003cth\u003eMicro-f1\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e500\u003c/td\u003e\n","      \u003ctd\u003e0.181200\u003c/td\u003e\n","      \u003ctd\u003e0.217330\u003c/td\u003e\n","      \u003ctd\u003e0.569648\u003c/td\u003e\n","      \u003ctd\u003e0.701415\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e1000\u003c/td\u003e\n","      \u003ctd\u003e0.148000\u003c/td\u003e\n","      \u003ctd\u003e0.186409\u003c/td\u003e\n","      \u003ctd\u003e0.596324\u003c/td\u003e\n","      \u003ctd\u003e0.737864\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e1500\u003c/td\u003e\n","      \u003ctd\u003e0.160600\u003c/td\u003e\n","      \u003ctd\u003e0.189201\u003c/td\u003e\n","      \u003ctd\u003e0.674166\u003c/td\u003e\n","      \u003ctd\u003e0.764607\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e2000\u003c/td\u003e\n","      \u003ctd\u003e0.132500\u003c/td\u003e\n","      \u003ctd\u003e0.171507\u003c/td\u003e\n","      \u003ctd\u003e0.689657\u003c/td\u003e\n","      \u003ctd\u003e0.768851\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e2500\u003c/td\u003e\n","      \u003ctd\u003e0.112700\u003c/td\u003e\n","      \u003ctd\u003e0.165421\u003c/td\u003e\n","      \u003ctd\u003e0.727190\u003c/td\u003e\n","      \u003ctd\u003e0.786453\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e3000\u003c/td\u003e\n","      \u003ctd\u003e0.123700\u003c/td\u003e\n","      \u003ctd\u003e0.172998\u003c/td\u003e\n","      \u003ctd\u003e0.713155\u003c/td\u003e\n","      \u003ctd\u003e0.780405\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e3500\u003c/td\u003e\n","      \u003ctd\u003e0.115700\u003c/td\u003e\n","      \u003ctd\u003e0.171059\u003c/td\u003e\n","      \u003ctd\u003e0.735166\u003c/td\u003e\n","      \u003ctd\u003e0.791411\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e4000\u003c/td\u003e\n","      \u003ctd\u003e0.108900\u003c/td\u003e\n","      \u003ctd\u003e0.165579\u003c/td\u003e\n","      \u003ctd\u003e0.735990\u003c/td\u003e\n","      \u003ctd\u003e0.797810\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e4500\u003c/td\u003e\n","      \u003ctd\u003e0.096300\u003c/td\u003e\n","      \u003ctd\u003e0.162297\u003c/td\u003e\n","      \u003ctd\u003e0.751358\u003c/td\u003e\n","      \u003ctd\u003e0.802083\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\u003cp\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["***** train metrics *****\n","  epoch                    =        2.0\n","  total_flos               = 83181797GF\n","  train_loss               =     0.1416\n","  train_runtime            = 1:44:44.18\n","  train_samples            =       9101\n","  train_samples_per_second =      2.896\n","  train_steps_per_second   =      0.724\n"]},{"data":{"text/html":[],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["***** eval metrics *****\n","  epoch                   =        2.0\n","  eval_loss               =     0.1623\n","  eval_macro-f1           =     0.7514\n","  eval_micro-f1           =     0.8021\n","  eval_runtime            = 0:01:42.88\n","  eval_samples            =       1000\n","  eval_samples_per_second =       9.72\n","  eval_steps_per_second   =       2.43\n","***** predict metrics *****\n","  predict_loss               =     0.1616\n","  predict_macro-f1           =     0.7359\n","  predict_micro-f1           =     0.7992\n","  predict_runtime            = 0:01:42.87\n","  predict_samples            =       1000\n","  predict_samples_per_second =      9.721\n","  predict_steps_per_second   =       2.43\n"]}],"source":["#!/usr/bin/env python\n","# coding=utf-8\n","\"\"\" Finetuning models on the ECtHR dataset (e.g. Bert, RoBERTa, LEGAL-BERT).\"\"\"\n","\n","import logging\n","import os\n","import random\n","import sys\n","from dataclasses import dataclass, field\n","from typing import Optional\n","\n","import datasets\n","import numpy as np\n","from datasets import load_dataset\n","from sklearn.metrics import f1_score\n","#from trainer import MultilabelTrainer\n","from scipy.special import expit\n","from torch import nn\n","import glob\n","import shutil\n","\n","import transformers\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    DataCollatorWithPadding,\n","    EvalPrediction,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    default_data_collator,\n","    set_seed,\n","    EarlyStoppingCallback,\n",")\n","from transformers.trainer_utils import get_last_checkpoint\n","from transformers.utils import check_min_version\n","from transformers.utils.versions import require_version\n","#from models.hierbert import HierarchicalBert\n","#from models.deberta import DebertaForSequenceClassification\n","\n","\n","# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n","check_min_version(\"4.9.0\")\n","\n","require_version(\"datasets\u003e=1.8.0\", \"To fix: pip install -r examples/pytorch/text-classification/requirements.txt\")\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","@dataclass\n","class DataTrainingArguments:\n","    \"\"\"\n","    Arguments pertaining to what data we are going to input our model for training and eval.\n","\n","    Using `HfArgumentParser` we can turn this class\n","    into argparse arguments to be able to specify them on\n","    the command line.\n","    \"\"\"\n","\n","    max_seq_length: Optional[int] = field(\n","        default=4096,\n","        metadata={\n","            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n","            \"than this will be truncated, sequences shorter will be padded.\"\n","        },\n","    )\n","    max_segments: Optional[int] = field(\n","        default=64,\n","        metadata={\n","            \"help\": \"The maximum number of segments (paragraphs) to be considered. Sequences longer \"\n","                    \"than this will be truncated, sequences shorter will be padded.\"\n","        },\n","    )\n","    max_seg_length: Optional[int] = field(\n","        default=128,\n","        metadata={\n","            \"help\": \"The maximum segment (paragraph) length to be considered. Segments longer \"\n","                    \"than this will be truncated, sequences shorter will be padded.\"\n","        },\n","    )\n","    overwrite_cache: bool = field(\n","        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n","    )\n","    pad_to_max_length: bool = field(\n","        default=True,\n","        metadata={\n","            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n","            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n","        },\n","    )\n","    max_train_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_eval_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_predict_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    task: Optional[str] = field(\n","        default='ecthr_b',\n","        metadata={\n","            \"help\": \"Define downstream task\"\n","        },\n","    )\n","    server_ip: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n","    server_port: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n","\n","\n","@dataclass\n","class ModelArguments:\n","    \"\"\"\n","    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n","    \"\"\"\n","\n","    model_name_or_path: str = field(\n","        default=None, metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n","    )\n","    hierarchical: bool = field(\n","        default=True, metadata={\"help\": \"Whether to use a hierarchical variant or not\"}\n","    )\n","    config_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n","    )\n","    tokenizer_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n","    )\n","    cache_dir: Optional[str] = field(\n","        default=None,\n","        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n","    )\n","    do_lower_case: Optional[bool] = field(\n","        default=True,\n","        metadata={\"help\": \"arg to indicate if tokenizer should do lower case in AutoTokenizer.from_pretrained()\"},\n","    )\n","    use_fast_tokenizer: bool = field(\n","        default=True,\n","        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n","    )\n","    model_revision: str = field(\n","        default=\"main\",\n","        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n","    )\n","    use_auth_token: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n","            \"with private models).\"\n","        },\n","    )\n","\n","\n","def main(training_args):\n","    # See all possible arguments in src/transformers/training_args.py\n","    # or by passing the --help flag to this script.\n","    # We now keep distinct sets of args, for a cleaner separation of concerns.\n","\n","    model_args = ModelArguments(\n","        model_name_or_path=\"nlpaueb/legal-bert-base-uncased\",\n","        #bert-base-uncased\n","        #microsoft/deberta-v3-base\n","        hierarchical=True,\n","        do_lower_case=True,\n","        use_fast_tokenizer=True,\n","    )\n","    data_args = DataTrainingArguments(\n","        max_seq_length=128,\n","        max_segments=64,\n","        max_seg_length=128,\n","        overwrite_cache=False,\n","        pad_to_max_length=True,\n","    )\n","\n","    # Fix boolean parameter\n","    if model_args.do_lower_case == 'False' or not model_args.do_lower_case:\n","        model_args.do_lower_case = False\n","    else:\n","        model_args.do_lower_case = True\n","\n","    if model_args.hierarchical == 'False' or not model_args.hierarchical:\n","        model_args.hierarchical = False\n","    else:\n","        model_args.hierarchical = True\n","\n","    # Setup distant debugging if needed\n","    if data_args.server_ip and data_args.server_port:\n","        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n","        import ptvsd\n","\n","        print(\"Waiting for debugger attach\")\n","        ptvsd.enable_attach(address=(data_args.server_ip, data_args.server_port), redirect_output=True)\n","        ptvsd.wait_for_attach()\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        handlers=[logging.StreamHandler(sys.stdout)],\n","    )\n","\n","    log_level = training_args.get_process_log_level()\n","    logger.setLevel(log_level)\n","    datasets.utils.logging.set_verbosity(log_level)\n","    transformers.utils.logging.set_verbosity(log_level)\n","    transformers.utils.logging.enable_default_handler()\n","    transformers.utils.logging.enable_explicit_format()\n","\n","    # Log on each process the small summary:\n","    logger.warning(\n","        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n","        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n","    )\n","    logger.info(f\"Training/evaluation parameters {training_args}\")\n","\n","    # Detecting last checkpoint.\n","    last_checkpoint = None\n","    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n","        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n","        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) \u003e 0:\n","            raise ValueError(\n","                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n","                \"Use --overwrite_output_dir to overcome.\"\n","            )\n","        elif last_checkpoint is not None:\n","            logger.info(\n","                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n","                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n","            )\n","\n","    # Set seed before initializing model.\n","    set_seed(training_args.seed)\n","\n","    # In distributed training, the load_dataset function guarantees that only one local process can concurrently\n","    # download the dataset.\n","    # Downloading and loading eurlex dataset from the hub.\n","    if training_args.do_train:\n","        train_dataset = merged_dataset\n","\n","    if training_args.do_eval:\n","        eval_dataset = load_dataset(\"lex_glue\", name=data_args.task, split=\"validation\", data_dir='data', cache_dir=model_args.cache_dir)\n","\n","    if training_args.do_predict:\n","        predict_dataset = load_dataset(\"lex_glue\", name=data_args.task, split=\"test\", data_dir='data', cache_dir=model_args.cache_dir)\n","\n","    # Labels\n","    label_list = list(range(10))\n","    num_labels = len(label_list)\n","\n","    # Load pretrained model and tokenizer\n","    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n","    # download model \u0026 vocab.\n","    config = AutoConfig.from_pretrained(\n","        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n","        num_labels=num_labels,\n","        finetuning_task=f\"{data_args.task}\",\n","        cache_dir=model_args.cache_dir,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n","        do_lower_case=model_args.do_lower_case,\n","        cache_dir=model_args.cache_dir,\n","        use_fast=model_args.use_fast_tokenizer,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","    if config.model_type == 'deberta' and model_args.hierarchical:\n","        model = DebertaForSequenceClassification.from_pretrained(\n","            model_args.model_name_or_path,\n","            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","            config=config,\n","            cache_dir=model_args.cache_dir,\n","            revision=model_args.model_revision,\n","            use_auth_token=True if model_args.use_auth_token else None,\n","        )\n","    else:\n","        model = AutoModelForSequenceClassification.from_pretrained(\n","            model_args.model_name_or_path,\n","            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","            config=config,\n","            cache_dir=model_args.cache_dir,\n","            revision=model_args.model_revision,\n","            use_auth_token=True if model_args.use_auth_token else None,\n","        )\n","\n","    if model_args.hierarchical:\n","        # Hack the classifier encoder to use hierarchical BERT\n","        if config.model_type in ['bert', 'deberta']:\n","            if config.model_type == 'bert':\n","                segment_encoder = model.bert\n","            else:\n","                segment_encoder = model.deberta\n","            model_encoder = HierarchicalBert(encoder=segment_encoder,\n","                                             max_segments=data_args.max_segments,\n","                                             max_segment_length=data_args.max_seg_length)\n","            if config.model_type == 'bert':\n","                model.bert = model_encoder\n","            elif config.model_type == 'deberta':\n","                model.deberta = model_encoder\n","            else:\n","                raise NotImplementedError(f\"{config.model_type} is no supported yet!\")\n","        elif config.model_type == 'roberta':\n","            model_encoder = HierarchicalBert(encoder=model.roberta, max_segments=data_args.max_segments,\n","                                             max_segment_length=data_args.max_seg_length)\n","            model.roberta = model_encoder\n","            # Build a new classification layer, as well\n","            dense = nn.Linear(config.hidden_size, config.hidden_size)\n","            dense.load_state_dict(model.classifier.dense.state_dict())  # load weights\n","            dropout = nn.Dropout(config.hidden_dropout_prob).to(model.device)\n","            out_proj = nn.Linear(config.hidden_size, config.num_labels).to(model.device)\n","            out_proj.load_state_dict(model.classifier.out_proj.state_dict())  # load weights\n","            model.classifier = nn.Sequential(dense, dropout, out_proj).to(model.device)\n","        elif config.model_type in ['longformer', 'big_bird']:\n","            pass\n","        else:\n","            raise NotImplementedError(f\"{config.model_type} is no supported yet!\")\n","\n","    # Preprocessing the datasets\n","    # Padding strategy\n","    if data_args.pad_to_max_length:\n","        padding = \"max_length\"\n","    else:\n","        # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n","        padding = False\n","\n","    def preprocess_function(examples):\n","        # Tokenize the texts\n","        if model_args.hierarchical:\n","            case_template = [[0] * data_args.max_seg_length]\n","            if config.model_type == 'roberta':\n","                batch = {'input_ids': [], 'attention_mask': []}\n","                for case in examples['text']:\n","                    case_encodings = tokenizer(case[:data_args.max_segments], padding=padding,\n","                                               max_length=data_args.max_seg_length, truncation=True)\n","                    batch['input_ids'].append(case_encodings['input_ids'] + case_template * (\n","                                data_args.max_segments - len(case_encodings['input_ids'])))\n","                    batch['attention_mask'].append(case_encodings['attention_mask'] + case_template * (\n","                                data_args.max_segments - len(case_encodings['attention_mask'])))\n","            else:\n","                batch = {'input_ids': [], 'attention_mask': [], 'token_type_ids': []}\n","                for case in examples['text']:\n","                    case_encodings = tokenizer(case[:data_args.max_segments], padding=padding,\n","                                               max_length=data_args.max_seg_length, truncation=True)\n","                    batch['input_ids'].append(case_encodings['input_ids'] + case_template * (\n","                            data_args.max_segments - len(case_encodings['input_ids'])))\n","                    batch['attention_mask'].append(case_encodings['attention_mask'] + case_template * (\n","                            data_args.max_segments - len(case_encodings['attention_mask'])))\n","                    batch['token_type_ids'].append(case_encodings['token_type_ids'] + case_template * (\n","                            data_args.max_segments - len(case_encodings['token_type_ids'])))\n","        elif config.model_type in ['longformer', 'big_bird']:\n","            cases = []\n","            max_position_embeddings = config.max_position_embeddings - 2 if config.model_type == 'longformer' \\\n","                else config.max_position_embeddings\n","            for case in examples['text']:\n","                cases.append(f' {tokenizer.sep_token} '.join(\n","                    [' '.join(fact.split()[:data_args.max_seg_length]) for fact in case[:data_args.max_segments]]))\n","            batch = tokenizer(cases, padding=padding, max_length=max_position_embeddings, truncation=True)\n","            if config.model_type == 'longformer':\n","                global_attention_mask = np.zeros((len(cases), max_position_embeddings), dtype=np.int32)\n","                # global attention on cls token\n","                global_attention_mask[:, 0] = 1\n","                batch['global_attention_mask'] = list(global_attention_mask)\n","        else:\n","            cases = []\n","            for case in examples['text']:\n","                cases.append(f'\\n'.join(case))\n","            batch = tokenizer(cases, padding=padding, max_length=512, truncation=True)\n","\n","        batch[\"labels\"] = [[1 if label in labels else 0 for label in label_list] for labels in examples[\"labels\"]]\n","\n","        return batch\n","\n","    if training_args.do_train:\n","        if data_args.max_train_samples is not None:\n","            train_dataset = train_dataset.select(range(data_args.max_train_samples))\n","        with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n","            train_dataset = train_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on train dataset\",\n","            )\n","        # Log a few random samples from the training set:\n","        for index in random.sample(range(len(train_dataset)), 3):\n","            logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n","\n","    if training_args.do_eval:\n","        if data_args.max_eval_samples is not None:\n","            eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))\n","        with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n","            eval_dataset = eval_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on validation dataset\",\n","            )\n","\n","    if training_args.do_predict:\n","        if data_args.max_predict_samples is not None:\n","            predict_dataset = predict_dataset.select(range(data_args.max_predict_samples))\n","        with training_args.main_process_first(desc=\"prediction dataset map pre-processing\"):\n","            predict_dataset = predict_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on prediction dataset\",\n","            )\n","\n","    # You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n","    # predictions and label_ids field) and has to return a dictionary string to float.\n","    def compute_metrics(p: EvalPrediction):\n","        # Fix gold labels\n","        y_true = np.zeros((p.label_ids.shape[0], p.label_ids.shape[1] + 1), dtype=np.int32)\n","        y_true[:, :-1] = p.label_ids\n","        y_true[:, -1] = (np.sum(p.label_ids, axis=1) == 0).astype('int32')\n","        # Fix predictions\n","        logits = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","        preds = (expit(logits) \u003e 0.5).astype('int32')\n","        y_pred = np.zeros((p.label_ids.shape[0], p.label_ids.shape[1] + 1), dtype=np.int32)\n","        y_pred[:, :-1] = preds\n","        y_pred[:, -1] = (np.sum(preds, axis=1) == 0).astype('int32')\n","        # Compute scores\n","        macro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='macro', zero_division=0)\n","        micro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='micro', zero_division=0)\n","        return {'macro-f1': macro_f1, 'micro-f1': micro_f1}\n","\n","    # Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n","    if data_args.pad_to_max_length:\n","        data_collator = default_data_collator\n","    elif training_args.fp16:\n","        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n","    else:\n","        data_collator = None\n","\n","    # Initialize our Trainer\n","    trainer = MultilabelTrainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset if training_args.do_train else None,\n","        eval_dataset=eval_dataset if training_args.do_eval else None,\n","        compute_metrics=compute_metrics,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        callbacks=[EarlyStoppingCallback(early_stopping_patience=15)]\n","    )\n","\n","    # Training\n","    if training_args.do_train:\n","        checkpoint = None\n","        if training_args.resume_from_checkpoint is not None:\n","            checkpoint = training_args.resume_from_checkpoint\n","        elif last_checkpoint is not None:\n","            checkpoint = last_checkpoint\n","        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n","        metrics = train_result.metrics\n","        max_train_samples = (\n","            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n","        )\n","        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n","\n","        trainer.save_model()  # Saves the tokenizer too for easy upload\n","\n","        trainer.log_metrics(\"train\", metrics)\n","        trainer.save_metrics(\"train\", metrics)\n","        trainer.save_state()\n","\n","    # Evaluation\n","    if training_args.do_eval:\n","        logger.info(\"*** Evaluate ***\")\n","        metrics = trainer.evaluate(eval_dataset=eval_dataset)\n","\n","        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n","        metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n","\n","        trainer.log_metrics(\"eval\", metrics)\n","        trainer.save_metrics(\"eval\", metrics)\n","\n","    # Prediction\n","    if training_args.do_predict:\n","        logger.info(\"*** Predict ***\")\n","        predictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix=\"predict\")\n","\n","        max_predict_samples = (\n","            data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)\n","        )\n","        metrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n","\n","        trainer.log_metrics(\"predict\", metrics)\n","        trainer.save_metrics(\"predict\", metrics)\n","\n","        output_predict_file = os.path.join(training_args.output_dir, \"test_predictions.csv\")\n","        if trainer.is_world_process_zero():\n","            with open(output_predict_file, \"w\") as writer:\n","                for index, pred_list in enumerate(predictions[0]):\n","                    pred_line = '\\t'.join([f'{pred:.5f}' for pred in pred_list])\n","                    writer.write(f\"{index}\\t{pred_line}\\n\")\n","\n","    # Clean up checkpoints\n","    checkpoints = [filepath for filepath in glob.glob(f'{training_args.output_dir}/*/') if '/checkpoint' in filepath]\n","    for checkpoint in checkpoints:\n","        shutil.rmtree(checkpoint)\n","\n","\n","if __name__ == \"__main__\":\n","    training_args = TrainingArguments(\n","        do_train = True,\n","        do_eval = True,\n","        do_predict = True,\n","        output_dir=os.getcwd(),\n","        overwrite_output_dir=True,\n","        num_train_epochs=2,\n","        per_device_train_batch_size=4,\n","        per_device_eval_batch_size=4,\n","        save_steps=500,\n","        save_total_limit=2,\n","        fp16=False,\n","        logging_dir=\"./logs\",\n","        logging_steps=100,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=500,\n","        logging_first_step=False,\n","        load_best_model_at_end = True,\n","        metric_for_best_model=\"micro-f1\",\n","    )\n","    main(training_args)\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":653,"status":"ok","timestamp":1693746455219,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"},"user_tz":-60},"id":"b1DMwCqSG1xw"},"outputs":[],"source":["import torch\n","from torch import nn\n","from transformers import DebertaPreTrainedModel, DebertaModel\n","from transformers.modeling_outputs import SequenceClassifierOutput, MultipleChoiceModelOutput\n","from transformers.activations import ACT2FN\n","\n","\n","class ContextPooler(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.dense = nn.Linear(config.pooler_hidden_size, config.pooler_hidden_size)\n","        self.dropout = StableDropout(config.pooler_dropout)\n","        self.config = config\n","\n","    def forward(self, hidden_states):\n","        # We \"pool\" the model by simply taking the hidden state corresponding\n","        # to the first token.\n","\n","        context_token = hidden_states[:, 0]\n","        context_token = self.dropout(context_token)\n","        pooled_output = self.dense(context_token)\n","        pooled_output = ACT2FN[self.config.pooler_hidden_act](pooled_output)\n","        return pooled_output\n","\n","    @property\n","    def output_dim(self):\n","        return self.config.hidden_size\n","\n","\n","class DropoutContext(object):\n","    def __init__(self):\n","        self.dropout = 0\n","        self.mask = None\n","        self.scale = 1\n","        self.reuse_mask = True\n","\n","\n","def get_mask(input, local_context):\n","    if not isinstance(local_context, DropoutContext):\n","        dropout = local_context\n","        mask = None\n","    else:\n","        dropout = local_context.dropout\n","        dropout *= local_context.scale\n","        mask = local_context.mask if local_context.reuse_mask else None\n","\n","    if dropout \u003e 0 and mask is None:\n","        mask = (1 - torch.empty_like(input).bernoulli_(1 - dropout)).bool()\n","\n","    if isinstance(local_context, DropoutContext):\n","        if local_context.mask is None:\n","            local_context.mask = mask\n","\n","    return mask, dropout\n","\n","\n","class XDropout(torch.autograd.Function):\n","    \"\"\"Optimized dropout function to save computation and memory by using mask operation instead of multiplication.\"\"\"\n","\n","    @staticmethod\n","    def forward(ctx, input, local_ctx):\n","        mask, dropout = get_mask(input, local_ctx)\n","        ctx.scale = 1.0 / (1 - dropout)\n","        if dropout \u003e 0:\n","            ctx.save_for_backward(mask)\n","            return input.masked_fill(mask, 0) * ctx.scale\n","        else:\n","            return input\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        if ctx.scale \u003e 1:\n","            (mask,) = ctx.saved_tensors\n","            return grad_output.masked_fill(mask, 0) * ctx.scale, None\n","        else:\n","            return grad_output, None\n","\n","\n","class StableDropout(nn.Module):\n","    \"\"\"\n","    Optimized dropout module for stabilizing the training\n","\n","    Args:\n","        drop_prob (float): the dropout probabilities\n","    \"\"\"\n","\n","    def __init__(self, drop_prob):\n","        super().__init__()\n","        self.drop_prob = drop_prob\n","        self.count = 0\n","        self.context_stack = None\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Call the module\n","\n","        Args:\n","            x (:obj:`torch.tensor`): The input tensor to apply dropout\n","        \"\"\"\n","        if self.training and self.drop_prob \u003e 0:\n","            return XDropout.apply(x, self.get_context())\n","        return x\n","\n","    def clear_context(self):\n","        self.count = 0\n","        self.context_stack = None\n","\n","    def init_context(self, reuse_mask=True, scale=1):\n","        if self.context_stack is None:\n","            self.context_stack = []\n","        self.count = 0\n","        for c in self.context_stack:\n","            c.reuse_mask = reuse_mask\n","            c.scale = scale\n","\n","    def get_context(self):\n","        if self.context_stack is not None:\n","            if self.count \u003e= len(self.context_stack):\n","                self.context_stack.append(DropoutContext())\n","            ctx = self.context_stack[self.count]\n","            ctx.dropout = self.drop_prob\n","            self.count += 1\n","            return ctx\n","        else:\n","            return self.drop_prob\n","\n","\n","class DebertaForSequenceClassification(DebertaPreTrainedModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","\n","        num_labels = getattr(config, \"num_labels\", 2)\n","        self.num_labels = num_labels\n","\n","        self.deberta = DebertaModel(config)\n","\n","        self.classifier = nn.Linear(config.hidden_size, num_labels)\n","        drop_out = getattr(config, \"cls_dropout\", None)\n","        drop_out = self.config.hidden_dropout_prob if drop_out is None else drop_out\n","        self.dropout = nn.Dropout(drop_out)\n","\n","        self.init_weights()\n","\n","    def get_input_embeddings(self):\n","        return self.deberta.get_input_embeddings()\n","\n","    def set_input_embeddings(self, new_embeddings):\n","        self.deberta.set_input_embeddings(new_embeddings)\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","        r\"\"\"\n","        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n","            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n","            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n","            If :obj:`config.num_labels \u003e 1` a classification loss is computed (Cross-Entropy).\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.deberta(\n","            input_ids,\n","            token_type_ids=token_type_ids,\n","            attention_mask=attention_mask,\n","            position_ids=position_ids,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        pooled_output = self.dropout(outputs[1])\n","        logits = self.classifier(pooled_output)\n","\n","        loss = None\n","        if labels is not None:\n","            if self.num_labels == 1:\n","                # regression task\n","                loss_fn = nn.MSELoss()\n","                logits = logits.view(-1).to(labels.dtype)\n","                loss = loss_fn(logits, labels.view(-1))\n","            elif labels.dim() == 1 or labels.size(-1) == 1:\n","                label_index = (labels \u003e= 0).nonzero()\n","                labels = labels.long()\n","                if label_index.size(0) \u003e 0:\n","                    labeled_logits = torch.gather(logits, 0, label_index.expand(label_index.size(0), logits.size(1)))\n","                    labels = torch.gather(labels, 0, label_index.view(-1))\n","                    loss_fct = nn.CrossEntropyLoss()\n","                    loss = loss_fct(labeled_logits.view(-1, self.num_labels).float(), labels.view(-1))\n","                else:\n","                    loss = torch.tensor(0).to(logits)\n","            else:\n","                log_softmax = nn.LogSoftmax(-1)\n","                loss = -((log_softmax(logits) * labels).sum(-1)).mean()\n","        if not return_dict:\n","            output = (logits,) + outputs[1:]\n","            return ((loss,) + output) if loss is not None else output\n","        else:\n","            return SequenceClassifierOutput(\n","                loss=loss,\n","                logits=logits,\n","                hidden_states=outputs.hidden_states,\n","                attentions=outputs.attentions,\n","            )\n","\n","\n","class DebertaForMultipleChoice(DebertaPreTrainedModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","\n","        self.deberta = DebertaModel(config)\n","        self.pooler = ContextPooler(config)\n","        output_dim = self.pooler.output_dim\n","        drop_out = getattr(config, \"cls_dropout\", None)\n","        drop_out = self.config.hidden_dropout_prob if drop_out is None else drop_out\n","        self.dropout = StableDropout(drop_out)\n","        self.classifier = nn.Linear(output_dim, 1)\n","\n","        self.init_weights()\n","\n","    def forward(\n","            self,\n","            input_ids=None,\n","            attention_mask=None,\n","            token_type_ids=None,\n","            position_ids=None,\n","            inputs_embeds=None,\n","            labels=None,\n","            output_attentions=None,\n","            output_hidden_states=None,\n","            return_dict=None,\n","    ):\n","        r\"\"\"\n","        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n","            Labels for computing the multiple choice classification loss. Indices should be in ``[0, ...,\n","            num_choices-1]`` where :obj:`num_choices` is the size of the second dimension of the input tensors. (See\n","            :obj:`input_ids` above)\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n","\n","        input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n","        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n","        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n","        position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n","        inputs_embeds = (\n","            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n","            if inputs_embeds is not None\n","            else None\n","        )\n","\n","        outputs = self.deberta(\n","            input_ids,\n","            token_type_ids=token_type_ids,\n","            attention_mask=attention_mask,\n","            position_ids=position_ids,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        encoder_layer = outputs[0]\n","        pooled_output = self.pooler(encoder_layer)\n","\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","        reshaped_logits = logits.view(-1, num_choices)\n","\n","        loss = None\n","        if labels is not None:\n","            loss_fct = nn.CrossEntropyLoss()\n","            loss = loss_fct(reshaped_logits, labels)\n","\n","        if not return_dict:\n","            output = (reshaped_logits,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return MultipleChoiceModelOutput(\n","            loss=loss,\n","            logits=reshaped_logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":715},"id":"cEr1Eva7iyXa"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n","[WARNING|modeling_utils.py:3553] 2023-09-03 13:37:43,952 \u003e\u003e Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c7a2483e39454b79ab12db160738c1aa","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on train dataset:   0%|          | 0/9101 [00:00\u003c?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    \u003cdiv\u003e\n","      \n","      \u003cprogress value='3000' max='9102' style='width:300px; height:20px; vertical-align: middle;'\u003e\u003c/progress\u003e\n","      [3000/9102 51:22 \u003c 1:44:33, 0.97 it/s, Epoch 0/2]\n","    \u003c/div\u003e\n","    \u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n"," \u003ctr style=\"text-align: left;\"\u003e\n","      \u003cth\u003eStep\u003c/th\u003e\n","      \u003cth\u003eTraining Loss\u003c/th\u003e\n","      \u003cth\u003eValidation Loss\u003c/th\u003e\n","      \u003cth\u003eMacro-f1\u003c/th\u003e\n","      \u003cth\u003eMicro-f1\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e500\u003c/td\u003e\n","      \u003ctd\u003e0.271000\u003c/td\u003e\n","      \u003ctd\u003e0.317060\u003c/td\u003e\n","      \u003ctd\u003e0.073892\u003c/td\u003e\n","      \u003ctd\u003e0.221029\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e1000\u003c/td\u003e\n","      \u003ctd\u003e0.258900\u003c/td\u003e\n","      \u003ctd\u003e0.321555\u003c/td\u003e\n","      \u003ctd\u003e0.105303\u003c/td\u003e\n","      \u003ctd\u003e0.258658\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e1500\u003c/td\u003e\n","      \u003ctd\u003e0.272600\u003c/td\u003e\n","      \u003ctd\u003e0.329581\u003c/td\u003e\n","      \u003ctd\u003e0.107343\u003c/td\u003e\n","      \u003ctd\u003e0.286716\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e2000\u003c/td\u003e\n","      \u003ctd\u003e0.263400\u003c/td\u003e\n","      \u003ctd\u003e0.314531\u003c/td\u003e\n","      \u003ctd\u003e0.051320\u003c/td\u003e\n","      \u003ctd\u003e0.194183\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e2500\u003c/td\u003e\n","      \u003ctd\u003e0.274700\u003c/td\u003e\n","      \u003ctd\u003e0.318877\u003c/td\u003e\n","      \u003ctd\u003e0.089247\u003c/td\u003e\n","      \u003ctd\u003e0.243753\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e3000\u003c/td\u003e\n","      \u003ctd\u003e0.266500\u003c/td\u003e\n","      \u003ctd\u003e0.318695\u003c/td\u003e\n","      \u003ctd\u003e0.065239\u003c/td\u003e\n","      \u003ctd\u003e0.232662\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\u003cp\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["***** train metrics *****\n","  epoch                    =       0.66\n","  total_flos               = 31145346GF\n","  train_loss               =      0.269\n","  train_runtime            = 0:51:23.39\n","  train_samples            =       9101\n","  train_samples_per_second =      5.903\n","  train_steps_per_second   =      2.952\n"]},{"data":{"text/html":[],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["***** eval metrics *****\n","  epoch                   =       0.66\n","  eval_loss               =     0.3296\n","  eval_macro-f1           =     0.1073\n","  eval_micro-f1           =     0.2867\n","  eval_runtime            = 0:02:09.32\n","  eval_samples            =       1000\n","  eval_samples_per_second =      7.732\n","  eval_steps_per_second   =      3.866\n","***** predict metrics *****\n","  predict_loss               =     0.3529\n","  predict_macro-f1           =     0.1161\n","  predict_micro-f1           =     0.2775\n","  predict_runtime            = 0:02:09.26\n","  predict_samples            =       1000\n","  predict_samples_per_second =      7.736\n","  predict_steps_per_second   =      3.868\n"]}],"source":["#!/usr/bin/env python\n","# coding=utf-8\n","\"\"\" Finetuning models on the ECtHR dataset (e.g. Bert, RoBERTa, LEGAL-BERT).\"\"\"\n","\n","import logging\n","import os\n","import random\n","import sys\n","from dataclasses import dataclass, field\n","from typing import Optional\n","\n","import datasets\n","import numpy as np\n","from datasets import load_dataset\n","from sklearn.metrics import f1_score\n","#from trainer import MultilabelTrainer\n","from scipy.special import expit\n","from torch import nn\n","import glob\n","import shutil\n","\n","import transformers\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    DataCollatorWithPadding,\n","    EvalPrediction,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    default_data_collator,\n","    set_seed,\n","    EarlyStoppingCallback,\n",")\n","from transformers.trainer_utils import get_last_checkpoint\n","from transformers.utils import check_min_version\n","from transformers.utils.versions import require_version\n","#from models.hierbert import HierarchicalBert\n","#from models.deberta import DebertaForSequenceClassification\n","\n","\n","# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n","check_min_version(\"4.9.0\")\n","\n","require_version(\"datasets\u003e=1.8.0\", \"To fix: pip install -r examples/pytorch/text-classification/requirements.txt\")\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","@dataclass\n","class DataTrainingArguments:\n","    \"\"\"\n","    Arguments pertaining to what data we are going to input our model for training and eval.\n","\n","    Using `HfArgumentParser` we can turn this class\n","    into argparse arguments to be able to specify them on\n","    the command line.\n","    \"\"\"\n","\n","    max_seq_length: Optional[int] = field(\n","        default=4096,\n","        metadata={\n","            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n","            \"than this will be truncated, sequences shorter will be padded.\"\n","        },\n","    )\n","    max_segments: Optional[int] = field(\n","        default=64,\n","        metadata={\n","            \"help\": \"The maximum number of segments (paragraphs) to be considered. Sequences longer \"\n","                    \"than this will be truncated, sequences shorter will be padded.\"\n","        },\n","    )\n","    max_seg_length: Optional[int] = field(\n","        default=128,\n","        metadata={\n","            \"help\": \"The maximum segment (paragraph) length to be considered. Segments longer \"\n","                    \"than this will be truncated, sequences shorter will be padded.\"\n","        },\n","    )\n","    overwrite_cache: bool = field(\n","        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n","    )\n","    pad_to_max_length: bool = field(\n","        default=True,\n","        metadata={\n","            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n","            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n","        },\n","    )\n","    max_train_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_eval_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_predict_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    task: Optional[str] = field(\n","        default='ecthr_a',\n","        metadata={\n","            \"help\": \"Define downstream task\"\n","        },\n","    )\n","    server_ip: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n","    server_port: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n","\n","\n","@dataclass\n","class ModelArguments:\n","    \"\"\"\n","    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n","    \"\"\"\n","\n","    model_name_or_path: str = field(\n","        default=None, metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n","    )\n","    hierarchical: bool = field(\n","        default=True, metadata={\"help\": \"Whether to use a hierarchical variant or not\"}\n","    )\n","    config_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n","    )\n","    tokenizer_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n","    )\n","    cache_dir: Optional[str] = field(\n","        default=None,\n","        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n","    )\n","    do_lower_case: Optional[bool] = field(\n","        default=True,\n","        metadata={\"help\": \"arg to indicate if tokenizer should do lower case in AutoTokenizer.from_pretrained()\"},\n","    )\n","    use_fast_tokenizer: bool = field(\n","        default=True,\n","        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n","    )\n","    model_revision: str = field(\n","        default=\"main\",\n","        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n","    )\n","    use_auth_token: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n","            \"with private models).\"\n","        },\n","    )\n","\n","\n","def main(training_args):\n","    # See all possible arguments in src/transformers/training_args.py\n","    # or by passing the --help flag to this script.\n","    # We now keep distinct sets of args, for a cleaner separation of concerns.\n","\n","    model_args = ModelArguments(\n","        model_name_or_path=\"microsoft/deberta-base\",\n","        hierarchical=True,\n","        do_lower_case=True,\n","        use_fast_tokenizer=True,\n","    )\n","    data_args = DataTrainingArguments(\n","        max_seq_length=128,\n","        max_segments=64,\n","        max_seg_length=128,\n","        overwrite_cache=False,\n","        pad_to_max_length=True,\n","    )\n","\n","    # Fix boolean parameter\n","    if model_args.do_lower_case == 'False' or not model_args.do_lower_case:\n","        model_args.do_lower_case = False\n","    else:\n","        model_args.do_lower_case = True\n","\n","    if model_args.hierarchical == 'False' or not model_args.hierarchical:\n","        model_args.hierarchical = False\n","    else:\n","        model_args.hierarchical = True\n","\n","    # Setup distant debugging if needed\n","    if data_args.server_ip and data_args.server_port:\n","        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n","        import ptvsd\n","\n","        print(\"Waiting for debugger attach\")\n","        ptvsd.enable_attach(address=(data_args.server_ip, data_args.server_port), redirect_output=True)\n","        ptvsd.wait_for_attach()\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        handlers=[logging.StreamHandler(sys.stdout)],\n","    )\n","\n","    log_level = training_args.get_process_log_level()\n","    logger.setLevel(log_level)\n","    datasets.utils.logging.set_verbosity(log_level)\n","    transformers.utils.logging.set_verbosity(log_level)\n","    transformers.utils.logging.enable_default_handler()\n","    transformers.utils.logging.enable_explicit_format()\n","\n","    # Log on each process the small summary:\n","    logger.warning(\n","        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n","        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n","    )\n","    logger.info(f\"Training/evaluation parameters {training_args}\")\n","\n","    # Detecting last checkpoint.\n","    last_checkpoint = None\n","    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n","        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n","        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) \u003e 0:\n","            raise ValueError(\n","                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n","                \"Use --overwrite_output_dir to overcome.\"\n","            )\n","        elif last_checkpoint is not None:\n","            logger.info(\n","                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n","                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n","            )\n","\n","    # Set seed before initializing model.\n","    set_seed(training_args.seed)\n","\n","    # In distributed training, the load_dataset function guarantees that only one local process can concurrently\n","    # download the dataset.\n","    # Downloading and loading eurlex dataset from the hub.\n","    if training_args.do_train:\n","        train_dataset = merged_dataset\n","\n","    if training_args.do_eval:\n","        eval_dataset = load_dataset(\"lex_glue\", name=data_args.task, split=\"validation\", data_dir='data', cache_dir=model_args.cache_dir)\n","\n","    if training_args.do_predict:\n","        predict_dataset = load_dataset(\"lex_glue\", name=data_args.task, split=\"test\", data_dir='data', cache_dir=model_args.cache_dir)\n","\n","    # Labels\n","    label_list = list(range(10))\n","    num_labels = len(label_list)\n","\n","    # Load pretrained model and tokenizer\n","    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n","    # download model \u0026 vocab.\n","    config = AutoConfig.from_pretrained(\n","        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n","        num_labels=num_labels,\n","        finetuning_task=f\"{data_args.task}\",\n","        cache_dir=model_args.cache_dir,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n","        do_lower_case=model_args.do_lower_case,\n","        cache_dir=model_args.cache_dir,\n","        use_fast=model_args.use_fast_tokenizer,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","    if config.model_type == 'deberta' and model_args.hierarchical:\n","        model = DebertaForSequenceClassification.from_pretrained(\n","            model_args.model_name_or_path,\n","            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","            config=config,\n","            cache_dir=model_args.cache_dir,\n","            revision=model_args.model_revision,\n","            use_auth_token=True if model_args.use_auth_token else None,\n","        )\n","    else:\n","        model = AutoModelForSequenceClassification.from_pretrained(\n","            model_args.model_name_or_path,\n","            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","            config=config,\n","            cache_dir=model_args.cache_dir,\n","            revision=model_args.model_revision,\n","            use_auth_token=True if model_args.use_auth_token else None,\n","        )\n","\n","    if model_args.hierarchical:\n","        # Hack the classifier encoder to use hierarchical BERT\n","        if config.model_type in ['bert', 'deberta']:\n","            if config.model_type == 'bert':\n","                segment_encoder = model.bert\n","            else:\n","                segment_encoder = model.deberta\n","            model_encoder = HierarchicalBert(encoder=segment_encoder,\n","                                             max_segments=data_args.max_segments,\n","                                             max_segment_length=data_args.max_seg_length)\n","            if config.model_type == 'bert':\n","                model.bert = model_encoder\n","            elif config.model_type == 'deberta':\n","                model.deberta = model_encoder\n","            else:\n","                raise NotImplementedError(f\"{config.model_type} is no supported yet!\")\n","        elif config.model_type == 'roberta':\n","            model_encoder = HierarchicalBert(encoder=model.roberta, max_segments=data_args.max_segments,\n","                                             max_segment_length=data_args.max_seg_length)\n","            model.roberta = model_encoder\n","            # Build a new classification layer, as well\n","            dense = nn.Linear(config.hidden_size, config.hidden_size)\n","            dense.load_state_dict(model.classifier.dense.state_dict())  # load weights\n","            dropout = nn.Dropout(config.hidden_dropout_prob).to(model.device)\n","            out_proj = nn.Linear(config.hidden_size, config.num_labels).to(model.device)\n","            out_proj.load_state_dict(model.classifier.out_proj.state_dict())  # load weights\n","            model.classifier = nn.Sequential(dense, dropout, out_proj).to(model.device)\n","        elif config.model_type in ['longformer', 'big_bird']:\n","            pass\n","        else:\n","            raise NotImplementedError(f\"{config.model_type} is no supported yet!\")\n","\n","    # Preprocessing the datasets\n","    # Padding strategy\n","    if data_args.pad_to_max_length:\n","        padding = \"max_length\"\n","    else:\n","        # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n","        padding = False\n","\n","    def preprocess_function(examples):\n","        # Tokenize the texts\n","        if model_args.hierarchical:\n","            case_template = [[0] * data_args.max_seg_length]\n","            if config.model_type == 'roberta':\n","                batch = {'input_ids': [], 'attention_mask': []}\n","                for case in examples['text']:\n","                    case_encodings = tokenizer(case[:data_args.max_segments], padding=padding,\n","                                               max_length=data_args.max_seg_length, truncation=True)\n","                    batch['input_ids'].append(case_encodings['input_ids'] + case_template * (\n","                                data_args.max_segments - len(case_encodings['input_ids'])))\n","                    batch['attention_mask'].append(case_encodings['attention_mask'] + case_template * (\n","                                data_args.max_segments - len(case_encodings['attention_mask'])))\n","            else:\n","                batch = {'input_ids': [], 'attention_mask': [], 'token_type_ids': []}\n","                for case in examples['text']:\n","                    case_encodings = tokenizer(case[:data_args.max_segments], padding=padding,\n","                                               max_length=data_args.max_seg_length, truncation=True)\n","                    batch['input_ids'].append(case_encodings['input_ids'] + case_template * (\n","                            data_args.max_segments - len(case_encodings['input_ids'])))\n","                    batch['attention_mask'].append(case_encodings['attention_mask'] + case_template * (\n","                            data_args.max_segments - len(case_encodings['attention_mask'])))\n","                    batch['token_type_ids'].append(case_encodings['token_type_ids'] + case_template * (\n","                            data_args.max_segments - len(case_encodings['token_type_ids'])))\n","        elif config.model_type in ['longformer', 'big_bird']:\n","            cases = []\n","            max_position_embeddings = config.max_position_embeddings - 2 if config.model_type == 'longformer' \\\n","                else config.max_position_embeddings\n","            for case in examples['text']:\n","                cases.append(f' {tokenizer.sep_token} '.join(\n","                    [' '.join(fact.split()[:data_args.max_seg_length]) for fact in case[:data_args.max_segments]]))\n","            batch = tokenizer(cases, padding=padding, max_length=max_position_embeddings, truncation=True)\n","            if config.model_type == 'longformer':\n","                global_attention_mask = np.zeros((len(cases), max_position_embeddings), dtype=np.int32)\n","                # global attention on cls token\n","                global_attention_mask[:, 0] = 1\n","                batch['global_attention_mask'] = list(global_attention_mask)\n","        else:\n","            cases = []\n","            for case in examples['text']:\n","                cases.append(f'\\n'.join(case))\n","            batch = tokenizer(cases, padding=padding, max_length=512, truncation=True)\n","\n","        batch[\"labels\"] = [[1 if label in labels else 0 for label in label_list] for labels in examples[\"labels\"]]\n","\n","        return batch\n","\n","    if training_args.do_train:\n","        if data_args.max_train_samples is not None:\n","            train_dataset = train_dataset.select(range(data_args.max_train_samples))\n","        with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n","            train_dataset = train_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on train dataset\",\n","            )\n","        # Log a few random samples from the training set:\n","        for index in random.sample(range(len(train_dataset)), 3):\n","            logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n","\n","    if training_args.do_eval:\n","        if data_args.max_eval_samples is not None:\n","            eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))\n","        with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n","            eval_dataset = eval_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on validation dataset\",\n","            )\n","\n","    if training_args.do_predict:\n","        if data_args.max_predict_samples is not None:\n","            predict_dataset = predict_dataset.select(range(data_args.max_predict_samples))\n","        with training_args.main_process_first(desc=\"prediction dataset map pre-processing\"):\n","            predict_dataset = predict_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on prediction dataset\",\n","            )\n","\n","    # You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n","    # predictions and label_ids field) and has to return a dictionary string to float.\n","    def compute_metrics(p: EvalPrediction):\n","        # Fix gold labels\n","        y_true = np.zeros((p.label_ids.shape[0], p.label_ids.shape[1] + 1), dtype=np.int32)\n","        y_true[:, :-1] = p.label_ids\n","        y_true[:, -1] = (np.sum(p.label_ids, axis=1) == 0).astype('int32')\n","        # Fix predictions\n","        logits = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","        preds = (expit(logits) \u003e 0.5).astype('int32')\n","        y_pred = np.zeros((p.label_ids.shape[0], p.label_ids.shape[1] + 1), dtype=np.int32)\n","        y_pred[:, :-1] = preds\n","        y_pred[:, -1] = (np.sum(preds, axis=1) == 0).astype('int32')\n","        # Compute scores\n","        macro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='macro', zero_division=0)\n","        micro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='micro', zero_division=0)\n","        return {'macro-f1': macro_f1, 'micro-f1': micro_f1}\n","\n","    # Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n","    if data_args.pad_to_max_length:\n","        data_collator = default_data_collator\n","    elif training_args.fp16:\n","        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n","    else:\n","        data_collator = None\n","\n","    # Initialize our Trainer\n","    trainer = MultilabelTrainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset if training_args.do_train else None,\n","        eval_dataset=eval_dataset if training_args.do_eval else None,\n","        compute_metrics=compute_metrics,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n","    )\n","\n","    # Training\n","    if training_args.do_train:\n","        checkpoint = None\n","        if training_args.resume_from_checkpoint is not None:\n","            checkpoint = training_args.resume_from_checkpoint\n","        elif last_checkpoint is not None:\n","            checkpoint = last_checkpoint\n","        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n","        metrics = train_result.metrics\n","        max_train_samples = (\n","            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n","        )\n","        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n","\n","        trainer.save_model()  # Saves the tokenizer too for easy upload\n","\n","        trainer.log_metrics(\"train\", metrics)\n","        trainer.save_metrics(\"train\", metrics)\n","        trainer.save_state()\n","\n","    # Evaluation\n","    if training_args.do_eval:\n","        logger.info(\"*** Evaluate ***\")\n","        metrics = trainer.evaluate(eval_dataset=eval_dataset)\n","\n","        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n","        metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n","\n","        trainer.log_metrics(\"eval\", metrics)\n","        trainer.save_metrics(\"eval\", metrics)\n","\n","    # Prediction\n","    if training_args.do_predict:\n","        logger.info(\"*** Predict ***\")\n","        predictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix=\"predict\")\n","\n","        max_predict_samples = (\n","            data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)\n","        )\n","        metrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n","\n","        trainer.log_metrics(\"predict\", metrics)\n","        trainer.save_metrics(\"predict\", metrics)\n","\n","        output_predict_file = os.path.join(training_args.output_dir, \"test_predictions.csv\")\n","        if trainer.is_world_process_zero():\n","            with open(output_predict_file, \"w\") as writer:\n","                for index, pred_list in enumerate(predictions[0]):\n","                    pred_line = '\\t'.join([f'{pred:.5f}' for pred in pred_list])\n","                    writer.write(f\"{index}\\t{pred_line}\\n\")\n","\n","    # Clean up checkpoints\n","    checkpoints = [filepath for filepath in glob.glob(f'{training_args.output_dir}/*/') if '/checkpoint' in filepath]\n","    for checkpoint in checkpoints:\n","        shutil.rmtree(checkpoint)\n","\n","\n","if __name__ == \"__main__\":\n","    training_args = TrainingArguments(\n","        do_train = True,\n","        do_eval = True,\n","        do_predict = True,\n","        output_dir=os.getcwd(),\n","        overwrite_output_dir=True,\n","        num_train_epochs=2,\n","        per_device_train_batch_size=2,\n","        per_device_eval_batch_size=2,\n","        save_steps=500,\n","        save_total_limit=2,\n","        fp16=False,\n","        logging_dir=\"./logs\",\n","        logging_steps=100,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=500,\n","        logging_first_step=False,\n","        load_best_model_at_end = True,\n","        metric_for_best_model=\"micro-f1\",\n","    )\n","    main(training_args)\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyP2/MAuBm80vgZr66SeUALd","machine_shape":"hm","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"00d177aaf1104375b098b6a170806d96":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"06948cbf67de48a9924ff4a07a4d24e2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d2a8032fa4ad4966bc96c8cea8d9ddad","IPY_MODEL_d56fe25f05824af9a2bbb6afae778158","IPY_MODEL_adfcccce68b6404097eb816c86830087"],"layout":"IPY_MODEL_b56d701d18be4a10916580793a1d36c3"}},"0cb3d264988745ea9c0d0de940fc52b3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b56751c11074e479f990b17a1b24eb8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba6dd5abcb7544049f37365788ffc6fe","max":9101,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5b777474b7594b28b0e1d4a209929919","value":9101}},"20c5f08cdc9c4e84a2a84a9b97032b09":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2564f82134b64c60bf6072afdcda4e43":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a4d90290cdbf4de2aa17e5d66037522d","IPY_MODEL_8b328eb4fbcc4d4092d7375d31c37da1","IPY_MODEL_cae60777b8d24bb6804cf8af07997444"],"layout":"IPY_MODEL_e9cf806f9d744e0bb76bd8dad4cc64c5"}},"307f754b03dd487480bd37bd76f00885":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"351fdbc3100546478c5f5311e144f0e9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f0fcb5c21934ddfaee71977ee9ea35c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b777474b7594b28b0e1d4a209929919":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"657e11f92d234465b4e18a6536a1539e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74f21101de6f4445a82a8f62aefaad56":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b328eb4fbcc4d4092d7375d31c37da1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_20c5f08cdc9c4e84a2a84a9b97032b09","max":101,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c3423b1c11be44f5a922da9c49395fa6","value":101}},"92d398bb305d4d4886d82014b016607e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_351fdbc3100546478c5f5311e144f0e9","placeholder":"​","style":"IPY_MODEL_d98b402b1b7049e2a459ada8d4198a65","value":"Running tokenizer on train dataset: 100%"}},"93f8ddf3a029460c9f2a70ed65ffb1c7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"941a8b4ff05b4e5ca54638232b555f9d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d0c9a8945854e639e9b7e3193d8204f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a4d90290cdbf4de2aa17e5d66037522d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_657e11f92d234465b4e18a6536a1539e","placeholder":"​","style":"IPY_MODEL_00d177aaf1104375b098b6a170806d96","value":"Map: 100%"}},"adfcccce68b6404097eb816c86830087":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0cb3d264988745ea9c0d0de940fc52b3","placeholder":"​","style":"IPY_MODEL_e2107ee66a304d77af6ae82b4860c225","value":" 9101/9101 [00:50\u0026lt;00:00, 167.23 examples/s]"}},"b54124909107459db551dd7254bb46b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b56d701d18be4a10916580793a1d36c3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba6dd5abcb7544049f37365788ffc6fe":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3423b1c11be44f5a922da9c49395fa6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c7a2483e39454b79ab12db160738c1aa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_92d398bb305d4d4886d82014b016607e","IPY_MODEL_1b56751c11074e479f990b17a1b24eb8","IPY_MODEL_ffbfca6216a9403fab408c3103bfd451"],"layout":"IPY_MODEL_941a8b4ff05b4e5ca54638232b555f9d"}},"cae60777b8d24bb6804cf8af07997444":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_307f754b03dd487480bd37bd76f00885","placeholder":"​","style":"IPY_MODEL_cff6ffcb3b5245a29710137ddb89c164","value":" 101/101 [00:00\u0026lt;00:00, 4291.48 examples/s]"}},"cff6ffcb3b5245a29710137ddb89c164":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d2a8032fa4ad4966bc96c8cea8d9ddad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2154618441145acb2a7cfe6bf3decc6","placeholder":"​","style":"IPY_MODEL_b54124909107459db551dd7254bb46b7","value":"Running tokenizer on train dataset: 100%"}},"d56fe25f05824af9a2bbb6afae778158":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_74f21101de6f4445a82a8f62aefaad56","max":9101,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9d0c9a8945854e639e9b7e3193d8204f","value":9101}},"d98b402b1b7049e2a459ada8d4198a65":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e2107ee66a304d77af6ae82b4860c225":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e9cf806f9d744e0bb76bd8dad4cc64c5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2154618441145acb2a7cfe6bf3decc6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffbfca6216a9403fab408c3103bfd451":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f0fcb5c21934ddfaee71977ee9ea35c","placeholder":"​","style":"IPY_MODEL_93f8ddf3a029460c9f2a70ed65ffb1c7","value":" 9101/9101 [00:49\u0026lt;00:00, 174.01 examples/s]"}}}}},"nbformat":4,"nbformat_minor":0}