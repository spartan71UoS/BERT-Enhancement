{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52145,"status":"ok","timestamp":1691368074742,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"},"user_tz":-60},"id":"4dhnto7I5WWd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch) (16.0.6)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch) (2.1.3)\n","Requirement already satisfied: mpmath\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-\u003etorch) (1.3.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,\u003c0.14,\u003e=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors\u003e=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.14.1-\u003etransformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.14.1-\u003etransformers) (4.7.1)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (1.26.16)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (2023.7.22)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (2.0.12)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (3.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy\u003e=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.22.4)\n","Requirement already satisfied: scipy\u003e=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n","Requirement already satisfied: joblib\u003e=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.1)\n","Requirement already satisfied: threadpoolctl\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n"]}],"source":["! pip install torch\n","! pip install transformers\n","! pip install scikit-learn\n","! pip install tqdm\n","! pip install numpy\n","! pip install datasets\n","! pip install nltk\n","import nltk\n","nltk.download('stopwords')\n","! pip install scipy\n","! pip install transformers[torch] accelerate\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":233},"executionInfo":{"elapsed":46246,"status":"ok","timestamp":1691368120968,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"},"user_tz":-60},"id":"pR1BTjor5nCw","outputId":"8628e655-9e1d-48d5-e25b-578aacfacdb1"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c57fda0b1b874c24bb731f4b0c3a8b36","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"59230e94aec740789e13407c12601684","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c5f2d684dc0945979d8935577ba6b816","version_major":2,"version_minor":0},"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"60ee65a9b68043b5b522aeb3c8fb9b1a","version_major":2,"version_minor":0},"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1a86128d20024e2290e36cb2132ea9fd","version_major":2,"version_minor":0},"text/plain":["Downloading model.safetensors:   0%|          | 0.00/440M [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from dataclasses import dataclass\n","from typing import Optional, Tuple\n","\n","import torch\n","import numpy as np\n","from torch import nn\n","from transformers.file_utils import ModelOutput\n","\n","\n","@dataclass\n","class SimpleOutput(ModelOutput):\n","    last_hidden_state: torch.FloatTensor = None\n","    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n","    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n","    attentions: Optional[Tuple[torch.FloatTensor]] = None\n","    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n","\n","\n","def sinusoidal_init(num_embeddings: int, embedding_dim: int):\n","    # keep dim 0 for padding token position encoding zero vector\n","    position_enc = np.array([\n","        [pos / np.power(10000, 2 * i / embedding_dim) for i in range(embedding_dim)]\n","        if pos != 0 else np.zeros(embedding_dim) for pos in range(num_embeddings)])\n","\n","    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2])  # dim 2i\n","    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2])  # dim 2i+1\n","    return torch.from_numpy(position_enc).type(torch.FloatTensor)\n","\n","\n","class HierarchicalBert(nn.Module):\n","\n","    def __init__(self, encoder, max_segments=64, max_segment_length=128):\n","        super(HierarchicalBert, self).__init__()\n","        supported_models = ['bert', 'roberta', 'deberta']\n","        assert encoder.config.model_type in supported_models  # other model types are not supported so far\n","        # Pre-trained segment (token-wise) encoder, e.g., BERT\n","        self.encoder = encoder\n","        # Specs for the segment-wise encoder\n","        self.hidden_size = encoder.config.hidden_size\n","        self.max_segments = max_segments\n","        self.max_segment_length = max_segment_length\n","        # Init sinusoidal positional embeddings\n","        self.seg_pos_embeddings = nn.Embedding(max_segments + 1, encoder.config.hidden_size,\n","                                               padding_idx=0,\n","                                               _weight=sinusoidal_init(max_segments + 1, encoder.config.hidden_size))\n","        # Init segment-wise transformer-based encoder\n","        self.seg_encoder = nn.Transformer(d_model=encoder.config.hidden_size,\n","                                          nhead=encoder.config.num_attention_heads,\n","                                          batch_first=True, dim_feedforward=encoder.config.intermediate_size,\n","                                          activation=encoder.config.hidden_act,\n","                                          dropout=encoder.config.hidden_dropout_prob,\n","                                          layer_norm_eps=encoder.config.layer_norm_eps,\n","                                          num_encoder_layers=2, num_decoder_layers=0).encoder\n","\n","    def forward(self,\n","                input_ids=None,\n","                attention_mask=None,\n","                token_type_ids=None,\n","                position_ids=None,\n","                head_mask=None,\n","                inputs_embeds=None,\n","                labels=None,\n","                output_attentions=None,\n","                output_hidden_states=None,\n","                return_dict=None,\n","                ):\n","        # Hypothetical Example\n","        # Batch of 4 documents: (batch_size, n_segments, max_segment_length) --\u003e (4, 64, 128)\n","        # BERT-BASE encoder: 768 hidden units\n","\n","        # Squash samples and segments into a single axis (batch_size * n_segments, max_segment_length) --\u003e (256, 128)\n","        input_ids_reshape = input_ids.contiguous().view(-1, input_ids.size(-1))\n","        attention_mask_reshape = attention_mask.contiguous().view(-1, attention_mask.size(-1))\n","        if token_type_ids is not None:\n","            token_type_ids_reshape = token_type_ids.contiguous().view(-1, token_type_ids.size(-1))\n","        else:\n","            token_type_ids_reshape = None\n","\n","        # Encode segments with BERT --\u003e (256, 128, 768)\n","        encoder_outputs = self.encoder(input_ids=input_ids_reshape,\n","                                       attention_mask=attention_mask_reshape,\n","                                       token_type_ids=token_type_ids_reshape)[0]\n","\n","        # Reshape back to (batch_size, n_segments, max_segment_length, output_size) --\u003e (4, 64, 128, 768)\n","        encoder_outputs = encoder_outputs.contiguous().view(input_ids.size(0), self.max_segments,\n","                                                            self.max_segment_length,\n","                                                            self.hidden_size)\n","\n","        # Gather CLS outputs per segment --\u003e (4, 64, 768)\n","        encoder_outputs = encoder_outputs[:, :, 0]\n","\n","        # Infer real segments, i.e., mask paddings\n","        seg_mask = (torch.sum(input_ids, 2) != 0).to(input_ids.dtype)\n","        # Infer and collect segment positional embeddings\n","        seg_positions = torch.arange(1, self.max_segments + 1).to(input_ids.device) * seg_mask\n","        # Add segment positional embeddings to segment inputs\n","        encoder_outputs += self.seg_pos_embeddings(seg_positions)\n","\n","        # Encode segments with segment-wise transformer\n","        seg_encoder_outputs = self.seg_encoder(encoder_outputs)\n","\n","        # Collect document representation\n","        outputs, _ = torch.max(seg_encoder_outputs, 1)\n","\n","        return SimpleOutput(last_hidden_state=outputs, hidden_states=outputs)\n","\n","\n","if __name__ == \"__main__\":\n","    from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n","    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","\n","    # Use as a stand-alone encoder\n","    bert = AutoModel.from_pretrained('bert-base-uncased')\n","    model = HierarchicalBert(encoder=bert, max_segments=64, max_segment_length=128)\n","\n","    fake_inputs = {'input_ids': [], 'attention_mask': [], 'token_type_ids': []}\n","    for i in range(4):\n","        # Tokenize segment\n","        temp_inputs = tokenizer(['dog ' * 126] * 64)\n","        fake_inputs['input_ids'].append(temp_inputs['input_ids'])\n","        fake_inputs['attention_mask'].append(temp_inputs['attention_mask'])\n","        fake_inputs['token_type_ids'].append(temp_inputs['token_type_ids'])\n","\n","    fake_inputs['input_ids'] = torch.as_tensor(fake_inputs['input_ids'])\n","    fake_inputs['attention_mask'] = torch.as_tensor(fake_inputs['attention_mask'])\n","    fake_inputs['token_type_ids'] = torch.as_tensor(fake_inputs['token_type_ids'])\n","\n","    output = model(fake_inputs['input_ids'], fake_inputs['attention_mask'], fake_inputs['token_type_ids'])\n","\n","    # 4 document representations of 768 features are expected\n","    assert output[0].shape == torch.Size([4, 768])\n","\n","    # Use with HuggingFace AutoModelForSequenceClassification and Trainer API\n","\n","    # Init Classifier\n","    model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=10)\n","    # Replace flat BERT encoder with hierarchical BERT encoder\n","    model.bert = HierarchicalBert(encoder=model.bert, max_segments=64, max_segment_length=128)\n","    output = model(fake_inputs['input_ids'], fake_inputs['attention_mask'], fake_inputs['token_type_ids'])\n","\n","    # 4 document outputs with 10 (num_labels) logits are expected\n","    assert output.logits.shape == torch.Size([4, 10])\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":490,"status":"ok","timestamp":1691368121400,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"},"user_tz":-60},"id":"VlT9XruS9585"},"outputs":[],"source":["from torch import nn\n","from transformers import Trainer\n","\n","\n","class MultilabelTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.pop(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","        loss_fct = nn.BCEWithLogitsLoss()\n","        loss = loss_fct(logits.view(-1, self.model.config.num_labels),\n","                        labels.float().view(-1, self.model.config.num_labels))\n","        return (loss, outputs) if return_outputs else loss\n","\n","\n","\n"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2488,"status":"ok","timestamp":1691379134362,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"},"user_tz":-60},"id":"q1wbwYUpZBpY","outputId":"6a7088f3-4147-44b5-9c0b-fd3ec0ce02fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'text': 'we may make changes to this agreement and to the services from time to time . \\n', 'labels': [2]}\n"]}],"source":["from datasets import load_dataset\n","dataset_dict = load_dataset(\"lex_glue\", 'unfair_tos')\n","#print(dataset)\n","#Divide into train,dev,test\n","\n","from sklearn.model_selection import train_test_split\n","\n","#data_list = list(dataset_dict.items())\n","\n","train_set_dict = dataset_dict['train']\n","print(train_set_dict[10])"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":361},"executionInfo":{"elapsed":78961,"status":"ok","timestamp":1691377271108,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"},"user_tz":-60},"id":"hEtwZ93652sh","outputId":"05f296ed-54d6-4697-8c1f-d9cf2b1afd2f"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n","[WARNING|modeling_utils.py:3331] 2023-08-07 03:00:01,904 \u003e\u003e Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"09e5278974a549328e6ec54929aed6dd","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on validation dataset:   0%|          | 0/2275 [00:00\u003c?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    \u003cdiv\u003e\n","      \n","      \u003cprogress value='346' max='346' style='width:300px; height:20px; vertical-align: middle;'\u003e\u003c/progress\u003e\n","      [346/346 01:00, Epoch 2/2]\n","    \u003c/div\u003e\n","    \u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n"," \u003ctr style=\"text-align: left;\"\u003e\n","      \u003cth\u003eStep\u003c/th\u003e\n","      \u003cth\u003eTraining Loss\u003c/th\u003e\n","      \u003cth\u003eValidation Loss\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\u003cp\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["***** train metrics *****\n","  epoch                    =        2.0\n","  total_flos               =   677820GF\n","  train_loss               =      0.087\n","  train_runtime            = 0:01:00.88\n","  train_samples            =       5532\n","  train_samples_per_second =    181.723\n","  train_steps_per_second   =      5.683\n"]},{"data":{"text/html":[],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["#!/usr/bin/env python\n","# coding=utf-8\n","\"\"\" Finetuning models on UNFAIR-ToC (e.g. Bert, RoBERTa, LEGAL-BERT).\"\"\"\n","import pandas as pd\n","import logging\n","import os\n","import random\n","import sys\n","from dataclasses import dataclass, field\n","from typing import Optional\n","\n","import datasets\n","from datasets import load_dataset\n","from sklearn.metrics import f1_score\n","# from trainer import MultilabelTrainer\n","from scipy.special import expit\n","import glob\n","import shutil\n","import numpy as np\n","import csv\n","\n","import transformers\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    DataCollatorWithPadding,\n","    EvalPrediction,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    default_data_collator,\n","    set_seed,\n","    EarlyStoppingCallback,\n",")\n","from transformers.trainer_utils import get_last_checkpoint\n","from transformers.utils import check_min_version\n","from transformers.utils.versions import require_version\n","\n","\n","# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n","check_min_version(\"4.9.0\")\n","\n","require_version(\"datasets\u003e=1.8.0\", \"To fix: pip install -r examples/pytorch/text-classification/requirements.txt\")\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","@dataclass\n","class DataTrainingArguments:\n","    \"\"\"\n","    Arguments pertaining to what data we are going to input our model for training and eval.\n","\n","    Using `HfArgumentParser` we can turn this class\n","    into argparse arguments to be able to specify them on\n","    the command line.\n","    \"\"\"\n","\n","    max_seq_length: Optional[int] = field(\n","        default=128,\n","        metadata={\n","            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n","            \"than this will be truncated, sequences shorter will be padded.\"\n","        },\n","    )\n","    overwrite_cache: bool = field(\n","        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n","    )\n","    pad_to_max_length: bool = field(\n","        default=True,\n","        metadata={\n","            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n","            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n","        },\n","    )\n","    max_train_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_eval_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_predict_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    server_ip: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n","    server_port: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n","\n","\n","@dataclass\n","class ModelArguments:\n","    \"\"\"\n","    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n","    \"\"\"\n","\n","    model_name_or_path: str = field(\n","        default=None, metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n","    )\n","    config_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n","    )\n","    tokenizer_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n","    )\n","    cache_dir: Optional[str] = field(\n","        default=None,\n","        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n","    )\n","    do_lower_case: Optional[bool] = field(\n","        default=True,\n","        metadata={\"help\": \"arg to indicate if tokenizer should do lower case in AutoTokenizer.from_pretrained()\"},\n","    )\n","    use_fast_tokenizer: bool = field(\n","        default=True,\n","        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n","    )\n","    model_revision: str = field(\n","        default=\"main\",\n","        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n","    )\n","    use_auth_token: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n","            \"with private models).\"\n","        },\n","    )\n","def compute_test_predictions(trainer, test_dataset):\n","    logger.info(\"*** Test Predictions ***\")\n","    predictions, labels, metrics = trainer.predict(test_dataset, metric_key_prefix=\"test\")\n","\n","    # Get the actual predicted indexes (class labels)\n","    threshold = 0.5  # Set the threshold for probability values to be considered as positive predictions\n","    actual_predictions = (predictions \u003e threshold).astype(int)  # Convert probabilities to binary predictions\n","\n","    # Get the input text from the dataset\n","    input_texts = test_dataset['text']\n","\n","    # Create a DataFrame to hold the predictions, labels, and input text\n","    with open(\"predictions.txt\", \"w\") as f:\n","        for index, input_text, prediction, label in zip(range(len(input_texts)), input_texts, actual_predictions, labels):\n","            f.write(f\"Index: {index}\\n\")\n","            f.write(f\"Input Text: {input_text}\\n\")\n","            f.write(f\"Predictions: {prediction}\\n\")\n","            f.write(f\"Labels: {label}\\n\")\n","            f.write(\"\\n\")\n","\n","def main(training_args):\n","    # See all possible arguments in src/transformers/training_args.py\n","    # or by passing the --help flag to this script.\n","    # We now keep distinct sets of args, for a cleaner separation of concerns.\n","\n","    model_args = ModelArguments(\n","        model_name_or_path=\"roberta-base\",\n","        # hierarchical=True,\n","        do_lower_case=True,\n","        use_fast_tokenizer=True,\n","    )\n","    data_args = DataTrainingArguments(\n","        max_seq_length=128,\n","        # max_segments=64,\n","        # max_seg_length=128,\n","        overwrite_cache=False,\n","        pad_to_max_length=True,\n","    )\n","\n","    # Setup distant debugging if needed\n","    if data_args.server_ip and data_args.server_port:\n","        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n","        import ptvsd\n","\n","        print(\"Waiting for debugger attach\")\n","        ptvsd.enable_attach(address=(data_args.server_ip, data_args.server_port), redirect_output=True)\n","        ptvsd.wait_for_attach()\n","\n","    # Fix boolean parameter\n","    if model_args.do_lower_case == 'False' or not model_args.do_lower_case:\n","        model_args.do_lower_case = False\n","        'Tokenizer do_lower_case False'\n","    else:\n","        model_args.do_lower_case = True\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        handlers=[logging.StreamHandler(sys.stdout)],\n","    )\n","\n","    log_level = training_args.get_process_log_level()\n","    logger.setLevel(log_level)\n","    datasets.utils.logging.set_verbosity(log_level)\n","    transformers.utils.logging.set_verbosity(log_level)\n","    transformers.utils.logging.enable_default_handler()\n","    transformers.utils.logging.enable_explicit_format()\n","\n","    # Log on each process the small summary:\n","    logger.warning(\n","        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n","        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n","    )\n","    logger.info(f\"Training/evaluation parameters {training_args}\")\n","\n","    # Detecting last checkpoint.\n","    last_checkpoint = None\n","    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n","        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n","        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) \u003e 0:\n","            raise ValueError(\n","                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n","                \"Use --overwrite_output_dir to overcome.\"\n","            )\n","        elif last_checkpoint is not None:\n","            logger.info(\n","                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n","                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n","            )\n","\n","    # Set seed before initializing model.\n","    set_seed(training_args.seed)\n","\n","    # In distributed training, the load_dataset function guarantees that only one local process can concurrently\n","    # download the dataset.\n","    # Downloading and loading eurlex dataset from the hub.\n","    if training_args.do_train:\n","        train_dataset = load_dataset(\"lex_glue\", \"unfair_tos\", split=\"train\", data_dir='data', cache_dir=model_args.cache_dir)\n","\n","    if training_args.do_eval:\n","        eval_dataset = load_dataset(\"lex_glue\", \"unfair_tos\", split=\"validation\", data_dir='data', cache_dir=model_args.cache_dir)\n","\n","    if training_args.do_predict:\n","        predict_dataset = load_dataset(\"lex_glue\", \"unfair_tos\", split=\"test\", data_dir='data', cache_dir=model_args.cache_dir)\n","\n","    # Labels\n","    label_list = list(range(8))\n","    num_labels = len(label_list)\n","\n","    # Load pretrained model and tokenizer\n","    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n","    # download model \u0026 vocab.\n","    config = AutoConfig.from_pretrained(\n","        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n","        num_labels=num_labels,\n","        finetuning_task=\"unfair_toc\",\n","        cache_dir=model_args.cache_dir,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","\n","    if config.model_type == 'big_bird':\n","        config.attention_type = 'original_full'\n","\n","    if config.model_type == 'longformer':\n","        config.attention_window = [128] * config.num_hidden_layers\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n","        do_lower_case=model_args.do_lower_case,\n","        cache_dir=model_args.cache_dir,\n","        use_fast=model_args.use_fast_tokenizer,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        model_args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","        config=config,\n","        cache_dir=model_args.cache_dir,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","\n","    # Preprocessing the datasets\n","    # Padding strategy\n","    if data_args.pad_to_max_length:\n","        padding = \"max_length\"\n","    else:\n","        # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n","        padding = False\n","\n","    def preprocess_function(examples):\n","        # Tokenize the texts\n","        batch = tokenizer(\n","            examples[\"text\"],\n","            padding=padding,\n","            max_length=data_args.max_seq_length,\n","            truncation=True,\n","        )\n","        batch[\"labels\"] = [[1 if label in labels else 0 for label in label_list] for labels in examples[\"labels\"]]\n","\n","        return batch\n","\n","    if training_args.do_train:\n","        if data_args.max_train_samples is not None:\n","            train_dataset = train_dataset.select(range(data_args.max_train_samples))\n","        with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n","            train_dataset = train_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on train dataset\",\n","            )\n","        # Log a few random samples from the training set:\n","        for index in random.sample(range(len(train_dataset)), 3):\n","            logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n","\n","    if training_args.do_eval:\n","        if data_args.max_eval_samples is not None:\n","            eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))\n","        with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n","            eval_dataset = eval_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on validation dataset\",\n","            )\n","\n","    if training_args.do_predict:\n","        if data_args.max_predict_samples is not None:\n","           predict_dataset = predict_dataset.select(range(data_args.max_predict_samples))\n","        with training_args.main_process_first(desc=\"prediction dataset map pre-processing\"):\n","            predict_dataset = predict_dataset.map(\n","            preprocess_function,\n","            batched=True,\n","            load_from_cache_file=not data_args.overwrite_cache,\n","            desc=\"Running tokenizer on prediction dataset\",\n","        )\n","\n","\n","\n","    # Compute predictions for the test dataset\n","\n","\n","\n","    # You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n","    # predictions and label_ids field) and has to return a dictionary string to float.\n","    def compute_metrics(p: EvalPrediction):\n","        # Fix gold labels\n","        y_true = np.zeros((p.label_ids.shape[0], p.label_ids.shape[1] + 1), dtype=np.int32)\n","        y_true[:, :-1] = p.label_ids\n","        y_true[:, -1] = (np.sum(p.label_ids, axis=1) == 0).astype('int32')\n","        # Fix predictions\n","        logits = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","        preds = (expit(logits) \u003e 0.5).astype('int32')\n","        y_pred = np.zeros((p.label_ids.shape[0], p.label_ids.shape[1] + 1), dtype=np.int32)\n","        y_pred[:, :-1] = preds\n","        y_pred[:, -1] = (np.sum(preds, axis=1) == 0).astype('int32')\n","        # Compute scores\n","        macro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='macro', zero_division=0)\n","        micro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='micro', zero_division=0)\n","        return {'macro-f1': macro_f1, 'micro-f1': micro_f1}\n","\n","\n","\n","    # Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n","    if data_args.pad_to_max_length:\n","        data_collator = default_data_collator\n","    elif training_args.fp16:\n","        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n","    else:\n","        data_collator = None\n","\n","    # Initialize our Trainer\n","    trainer = MultilabelTrainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset if training_args.do_train else None,\n","        eval_dataset=eval_dataset if training_args.do_eval else None,\n","        compute_metrics=compute_metrics,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n","    )\n","    compute_test_predictions(trainer, predict_dataset)\n","    # Training\n","    if training_args.do_train:\n","        checkpoint = None\n","        if training_args.resume_from_checkpoint is not None:\n","            checkpoint = training_args.resume_from_checkpoint\n","        elif last_checkpoint is not None:\n","            checkpoint = last_checkpoint\n","        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n","        metrics = train_result.metrics\n","        max_train_samples = (\n","            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n","        )\n","        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n","\n","        trainer.save_model()  # Saves the tokenizer too for easy upload\n","\n","        trainer.log_metrics(\"train\", metrics)\n","        trainer.save_metrics(\"train\", metrics)\n","        trainer.save_state()\n","\n","    # Evaluation\n","    if training_args.do_eval:\n","        logger.info(\"*** Predict ***\")\n","        predictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix=\"predict\")\n","\n","        # Get the actual predicted indexes (class labels)\n","        threshold = 0.5  # Set the threshold for probability values to be considered as positive predictions\n","        actual_predictions = (predictions \u003e threshold).astype(int)  # Convert probabilities to binary predictions\n","\n","        # Get the input text from the dataset\n","        input_texts = predict_dataset['text']\n","\n","        # Create a DataFrame to hold the predictions, labels, and input text\n","        with open(\"predictions.txt\", \"w\") as f:\n","            for index, input_text, prediction, label in zip(range(len(input_texts)), input_texts, actual_predictions, labels):\n","                f.write(f\"Index: {index}\\n\")\n","                f.write(f\"Input Text: {input_text}\\n\")\n","                f.write(f\"Predictions: {prediction}\\n\")\n","                f.write(f\"Labels: {label}\\n\")\n","                f.write(\"\\n\")\n","\n","    # ... Your existing code ...\n","\n","    # Access the rows from the dataset associated with correct predictions\n","    # ... Your existing code ...\n","\n","# Access the rows from the dataset associated with correct predictions\n","    correct_predictions_dataset = predict_dataset.filter(\n","    lambda example, idx: all(actual_predictions[idx] == example[\"labels\"]),\n","    with_indices=True\n",")\n","\n","# Access the rows from the dataset associated with incorrect predictions\n","    incorrect_predictions_dataset = predict_dataset.filter(\n","    lambda example, idx: any(actual_predictions[idx] != example[\"labels\"]),\n","    with_indices=True\n",")\n","\n","# Get the input texts and input IDs from the datasets\n","    correct_input_texts = correct_predictions_dataset[\"text\"]\n","    correct_input_ids = correct_predictions_dataset[\"input_ids\"]\n","    incorrect_input_texts = incorrect_predictions_dataset[\"text\"]\n","    incorrect_input_ids = incorrect_predictions_dataset[\"input_ids\"]\n","\n","# Save the correct predictions to a file\n","    with open(\"correct_predictions.txt\", \"w\") as f:\n","       for idx, input_text, input_id in zip(range(len(correct_input_texts)), correct_input_texts, correct_input_ids):\n","         f.write(f\"Index: {idx}\\n\")\n","         f.write(f\"Input Text: {input_text}\\n\")\n","         f.write(f\"Input IDs: {input_id}\\n\")\n","         f.write(\"\\n\")\n","\n","# Save the incorrect predictions to a file\n","    # Save the incorrect predictions to a file\n","    with open(\"incorrect_predictions.txt\", \"w\") as f:\n","       for idx, input_text, input_id, labels in zip(\n","         range(len(incorrect_input_texts)),\n","         incorrect_input_texts,\n","         incorrect_input_ids,\n","         incorrect_predictions_dataset[\"labels\"]\n","    ):\n","         f.write(f\"Index: {idx}\\n\")\n","         f.write(f\"Input Text: {input_text}\\n\")\n","         f.write(f\"Input IDs: {input_id}\\n\")\n","         f.write(f\"Labels: {labels}\\n\")  # Write the labels to the file\n","         f.write(\"\\n\")\n","\n","\n","\n","\n","\n","\n","if __name__ == \"__main__\":\n","    training_args = TrainingArguments(\n","        do_train=True,\n","        do_eval=True,\n","        do_predict=True,\n","        output_dir=os.getcwd(),\n","        overwrite_output_dir=True,\n","        num_train_epochs=2,\n","        per_device_train_batch_size=32,\n","        per_device_eval_batch_size=32,\n","        save_steps=500,\n","        save_total_limit=2,\n","        fp16=False,\n","        logging_dir=\"./logs\",\n","        logging_steps=200,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=500,\n","        logging_first_step=False,\n","        load_best_model_at_end=True,\n","        metric_for_best_model=\"macro-f1\",\n","    )\n","    main(training_args)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a3XXuwGlc4pA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":60,"status":"ok","timestamp":1691377561082,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"},"user_tz":-60},"id":"VTfFbXbRc4wT","outputId":"e8e066f7-9406-47f7-e807-214fc746669e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Converted incorrect predictions saved to incorrect_predictions_dataset.json\n"]}],"source":["import json\n","import re\n","\n","# Read the \"incorrect-predictions.txt\" file\n","with open(\"/content/incorrect_predictions.txt\", \"r\") as f:\n","    lines = f.readlines()\n","\n","# Initialize variables\n","current_entry = {}\n","data_entries = []\n","\n","# Process the lines and create data entries\n","for line in lines:\n","    line = line.strip()\n","    if line.startswith(\"Index: \"):\n","        current_entry[\"Index\"] = int(line.split(\": \")[1])\n","    elif line.startswith(\"Input Text: \"):\n","        current_entry[\"text\"] = line.split(\": \")[1]\n","    elif line.startswith(\"Input IDs: \"):\n","        input_ids_str = line.split(\": \")[1]\n","        input_ids = [int(id_str) for id_str in re.findall(r'\\d+', input_ids_str)]\n","        current_entry[\"input_ids\"] = input_ids\n","        current_entry[\"labels\"] = []  # Assuming no labels for incorrect predictions\n","        data_entries.append(current_entry)\n","        current_entry = {}\n","    elif line.startswith(\"Labels: \"):  # Process labels\n","        labels_str = line.split(\": \")[1]\n","        labels = [int(label_str) for label_str in re.findall(r'\\d+', labels_str)]\n","        current_entry[\"labels\"] = labels\n","        data_entries.append(current_entry)\n","        current_entry = {}\n","\n","# Save the data entries to a JSON file\n","output_filename = \"incorrect_predictions_dataset.json\"\n","with open(output_filename, \"w\") as json_file:\n","    json.dump(data_entries, json_file, indent=4)\n","\n","print(f\"Converted incorrect predictions saved to {output_filename}\")\n"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2794,"status":"ok","timestamp":1691378459762,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"},"user_tz":-60},"id":"oa9JyZpaei2G","outputId":"47179f27-8173-4a81-cdfc-baff74b4b822"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'text': ['notice to california subscribers : you may cancel your subscription , without penalty or obligation , at any time prior to midnight of the third business day following the date you subscribed . \\n', 'if you subscribed using your apple id , refunds are handled by apple , not tinder . \\n', 'if you wish to request a refund , please visit https://getsupport.apple.com . \\n', 'if you subscribed using your google play store account or through tinder online : contact customer support \\n', \"key changes in this version : we 've included a legal notice required under california law regarding refunds and updated our legal name to match group , llc \\n\"], 'labels': [[], [], [], [], []]}\n"]}],"source":["import json\n","from datasets import Dataset\n","\n","# Load your existing dataset\n","from datasets import load_dataset\n","dataset_name = \"lex_glue\"  # Update with your dataset name\n","existing_dataset = load_dataset(dataset_name, 'unfair_tos')\n","\n","# Load the data from the \"incorrect_predictions_dataset.json\" file\n","with open(\"/content/incorrect_predictions_dataset.json\", \"r\") as json_file:\n","    incorrect_predictions_data = json.load(json_file)\n","\n","# Prepare the data entries for merging\n","merged_texts = []\n","merged_labels = []\n","\n","for entry in incorrect_predictions_data:\n","    if \"text\" in entry:\n","        text = entry[\"text\"]\n","        labels = entry[\"labels\"]\n","        num_labels = len(labels)\n","        merged_texts.extend([text] * num_labels)\n","        merged_labels.extend(labels)\n","\n","# Convert the data to the dataset format\n","merged_dataset = Dataset.from_dict({\n","    \"text\": existing_dataset[\"train\"][\"text\"] + merged_texts,\n","    \"labels\": existing_dataset[\"train\"][\"labels\"] + merged_labels,\n","})\n","\n","# Print a few samples from the merged dataset\n","print(merged_dataset[:5])\n"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":574},"executionInfo":{"elapsed":15051,"status":"error","timestamp":1691379429370,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"},"user_tz":-60},"id":"-KJgklubhxhl","outputId":"f0fa6c3f-7bf2-4592-bd46-1ee3b506593b"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n","[WARNING|modeling_utils.py:3331] 2023-08-07 03:37:02,622 \u003e\u003e Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aebdde5429ce4dc29fb0875de98107b0","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on prediction dataset:   0%|          | 0/1607 [00:00\u003c?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    \u003cdiv\u003e\n","      \n","      \u003cprogress value='85' max='13840' style='width:300px; height:20px; vertical-align: middle;'\u003e\u003c/progress\u003e\n","      [   85/13840 00:05 \u003c 14:58, 15.32 it/s, Epoch 0.12/20]\n","    \u003c/div\u003e\n","    \u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n"," \u003ctr style=\"text-align: left;\"\u003e\n","      \u003cth\u003eStep\u003c/th\u003e\n","      \u003cth\u003eTraining Loss\u003c/th\u003e\n","      \u003cth\u003eValidation Loss\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\u003cp\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-44-6acafc06170d\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 421\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mmetric_for_best_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"micro-f1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     )\n\u001b[0;32m--\u003e 443\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m\u003cipython-input-44-6acafc06170d\u003e\u001b[0m in \u001b[0;36mmain\u001b[0;34m(training_args)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlast_checkpoint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 366\u001b[0;31m         \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         max_train_samples = (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m         )\n\u001b[0;32m-\u003e 1539\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1540\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1812\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m                     \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1814\u001b[0;31m                     \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1815\u001b[0m                 ):\n\u001b[1;32m   1816\u001b[0m                     \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["#!/usr/bin/env python\n","# coding=utf-8\n","\"\"\" Finetuning models on UNFAIR-ToC (e.g. Bert, RoBERTa, LEGAL-BERT).\"\"\"\n","\n","import logging\n","import os\n","import random\n","import sys\n","from dataclasses import dataclass, field\n","from typing import Optional\n","\n","import datasets\n","from datasets import load_dataset\n","from sklearn.metrics import f1_score\n","\n","from scipy.special import expit\n","import glob\n","import shutil\n","import numpy as np\n","\n","import transformers\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    DataCollatorWithPadding,\n","    EvalPrediction,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    default_data_collator,\n","    set_seed,\n","    EarlyStoppingCallback,\n",")\n","from transformers.trainer_utils import get_last_checkpoint\n","from transformers.utils import check_min_version\n","from transformers.utils.versions import require_version\n","\n","\n","# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n","check_min_version(\"4.9.0\")\n","\n","require_version(\"datasets\u003e=1.8.0\", \"To fix: pip install -r examples/pytorch/text-classification/requirements.txt\")\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","@dataclass\n","class DataTrainingArguments:\n","    \"\"\"\n","    Arguments pertaining to what data we are going to input our model for training and eval.\n","\n","    Using `HfArgumentParser` we can turn this class\n","    into argparse arguments to be able to specify them on\n","    the command line.\n","    \"\"\"\n","\n","    max_seq_length: Optional[int] = field(\n","        default=128,\n","        metadata={\n","            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n","            \"than this will be truncated, sequences shorter will be padded.\"\n","        },\n","    )\n","    overwrite_cache: bool = field(\n","        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n","    )\n","    pad_to_max_length: bool = field(\n","        default=True,\n","        metadata={\n","            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n","            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n","        },\n","    )\n","    max_train_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_eval_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_predict_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    server_ip: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n","    server_port: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n","\n","\n","@dataclass\n","class ModelArguments:\n","    \"\"\"\n","    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n","    \"\"\"\n","\n","    model_name_or_path: str = field(\n","        default=None, metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n","    )\n","    config_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n","    )\n","    tokenizer_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n","    )\n","    cache_dir: Optional[str] = field(\n","        default=None,\n","        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n","    )\n","    do_lower_case: Optional[bool] = field(\n","        default=True,\n","        metadata={\"help\": \"arg to indicate if tokenizer should do lower case in AutoTokenizer.from_pretrained()\"},\n","    )\n","    use_fast_tokenizer: bool = field(\n","        default=True,\n","        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n","    )\n","    model_revision: str = field(\n","        default=\"main\",\n","        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n","    )\n","    use_auth_token: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n","            \"with private models).\"\n","        },\n","    )\n","\n","\n","def main(training_args):\n","    # See all possible arguments in src/transformers/training_args.py\n","    # or by passing the --help flag to this script.\n","    # We now keep distinct sets of args, for a cleaner separation of concerns.\n","\n","    model_args = ModelArguments(\n","        model_name_or_path=\"bert-base-uncased\",\n","        #hierarchical=True,\n","        do_lower_case=True,\n","        use_fast_tokenizer=True,\n","    )\n","    data_args = DataTrainingArguments(\n","        max_seq_length=128,\n","        #max_segments=64,\n","        #max_seg_length=128,\n","        overwrite_cache=False,\n","        pad_to_max_length=True,\n","    )\n","\n","    # Setup distant debugging if needed\n","    if data_args.server_ip and data_args.server_port:\n","        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n","        import ptvsd\n","\n","        print(\"Waiting for debugger attach\")\n","        ptvsd.enable_attach(address=(data_args.server_ip, data_args.server_port), redirect_output=True)\n","        ptvsd.wait_for_attach()\n","\n","    # Fix boolean parameter\n","    if model_args.do_lower_case == 'False' or not model_args.do_lower_case:\n","        model_args.do_lower_case = False\n","        'Tokenizer do_lower_case False'\n","    else:\n","        model_args.do_lower_case = True\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        handlers=[logging.StreamHandler(sys.stdout)],\n","    )\n","\n","    log_level = training_args.get_process_log_level()\n","    logger.setLevel(log_level)\n","    datasets.utils.logging.set_verbosity(log_level)\n","    transformers.utils.logging.set_verbosity(log_level)\n","    transformers.utils.logging.enable_default_handler()\n","    transformers.utils.logging.enable_explicit_format()\n","\n","    # Log on each process the small summary:\n","    logger.warning(\n","        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n","        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n","    )\n","    logger.info(f\"Training/evaluation parameters {training_args}\")\n","\n","    # Detecting last checkpoint.\n","    last_checkpoint = None\n","    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n","        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n","        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) \u003e 0:\n","            raise ValueError(\n","                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n","                \"Use --overwrite_output_dir to overcome.\"\n","            )\n","        elif last_checkpoint is not None:\n","            logger.info(\n","                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n","                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n","            )\n","\n","    # Set seed before initializing model.\n","    set_seed(training_args.seed)\n","\n","    # In distributed training, the load_dataset function guarantees that only one local process can concurrently\n","    # download the dataset.\n","    # Downloading and loading eurlex dataset from the hub.\n","    if training_args.do_train:\n","        train_dataset = load_dataset(\"lex_glue\", \"unfair_tos\", split=\"train\", data_dir='data', cache_dir=model_args.cache_dir)\n","\n","    if training_args.do_eval:\n","        eval_dataset = load_dataset(\"lex_glue\", \"unfair_tos\", split=\"validation\", data_dir='data', cache_dir=model_args.cache_dir)\n","\n","    if training_args.do_predict:\n","        predict_dataset = load_dataset(\"lex_glue\", \"unfair_tos\", split=\"test\", data_dir='data', cache_dir=model_args.cache_dir)\n","\n","    # Labels\n","    label_list = list(range(8))\n","    num_labels = len(label_list)\n","\n","    # Load pretrained model and tokenizer\n","    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n","    # download model \u0026 vocab.\n","    config = AutoConfig.from_pretrained(\n","        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n","        num_labels=num_labels,\n","        finetuning_task=\"unfair_toc\",\n","        cache_dir=model_args.cache_dir,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","\n","    if config.model_type == 'big_bird':\n","        config.attention_type = 'original_full'\n","\n","    if config.model_type == 'longformer':\n","        config.attention_window = [128] * config.num_hidden_layers\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n","        do_lower_case=model_args.do_lower_case,\n","        cache_dir=model_args.cache_dir,\n","        use_fast=model_args.use_fast_tokenizer,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        model_args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","        config=config,\n","        cache_dir=model_args.cache_dir,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","\n","    # Preprocessing the datasets\n","    # Padding strategy\n","    if data_args.pad_to_max_length:\n","        padding = \"max_length\"\n","    else:\n","        # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n","        padding = False\n","\n","    def preprocess_function(examples):\n","        # Tokenize the texts\n","        batch = tokenizer(\n","            examples[\"text\"],\n","            padding=padding,\n","            max_length=data_args.max_seq_length,\n","            truncation=True,\n","        )\n","        batch[\"labels\"] = [[1 if label in labels else 0 for label in label_list] for labels in\n","                              examples[\"labels\"]]\n","\n","        return batch\n","\n","    if training_args.do_train:\n","        if data_args.max_train_samples is not None:\n","            train_dataset = train_dataset.select(range(data_args.max_train_samples))\n","        with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n","            train_dataset = train_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on train dataset\",\n","            )\n","        # Log a few random samples from the training set:\n","        for index in random.sample(range(len(train_dataset)), 3):\n","            logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n","\n","    if training_args.do_eval:\n","        if data_args.max_eval_samples is not None:\n","            eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))\n","        with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n","            eval_dataset = eval_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on validation dataset\",\n","            )\n","\n","    if training_args.do_predict:\n","        if data_args.max_predict_samples is not None:\n","            predict_dataset = predict_dataset.select(range(data_args.max_predict_samples))\n","        with training_args.main_process_first(desc=\"prediction dataset map pre-processing\"):\n","            predict_dataset = predict_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on prediction dataset\",\n","            )\n","\n","    # You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n","    # predictions and label_ids field) and has to return a dictionary string to float.\n","    def compute_metrics(p: EvalPrediction):\n","        # Fix gold labels\n","        y_true = np.zeros((p.label_ids.shape[0], p.label_ids.shape[1] + 1), dtype=np.int32)\n","        y_true[:, :-1] = p.label_ids\n","        y_true[:, -1] = (np.sum(p.label_ids, axis=1) == 0).astype('int32')\n","        # Fix predictions\n","        logits = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","        preds = (expit(logits) \u003e 0.5).astype('int32')\n","        y_pred = np.zeros((p.label_ids.shape[0], p.label_ids.shape[1] + 1), dtype=np.int32)\n","        y_pred[:, :-1] = preds\n","        y_pred[:, -1] = (np.sum(preds, axis=1) == 0).astype('int32')\n","        # Compute scores\n","        macro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='macro', zero_division=0)\n","        micro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='micro', zero_division=0)\n","        return {'macro-f1': macro_f1, 'micro-f1': micro_f1}\n","\n","    # Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n","    if data_args.pad_to_max_length:\n","        data_collator = default_data_collator\n","    elif training_args.fp16:\n","        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n","    else:\n","        data_collator = None\n","\n","    # Initialize our Trainer\n","    trainer = MultilabelTrainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset if training_args.do_train else None,\n","        eval_dataset=eval_dataset if training_args.do_eval else None,\n","        compute_metrics=compute_metrics,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n","    )\n","\n","    # Training\n","    if training_args.do_train:\n","        checkpoint = None\n","        if training_args.resume_from_checkpoint is not None:\n","            checkpoint = training_args.resume_from_checkpoint\n","        elif last_checkpoint is not None:\n","            checkpoint = last_checkpoint\n","        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n","        metrics = train_result.metrics\n","        max_train_samples = (\n","            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n","        )\n","        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n","\n","        trainer.save_model()  # Saves the tokenizer too for easy upload\n","\n","        trainer.log_metrics(\"train\", metrics)\n","        trainer.save_metrics(\"train\", metrics)\n","        trainer.save_state()\n","\n","    # Evaluation\n","    if training_args.do_eval:\n","        logger.info(\"*** Evaluate ***\")\n","        metrics = trainer.evaluate(eval_dataset=eval_dataset)\n","\n","        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n","        metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n","\n","        trainer.log_metrics(\"eval\", metrics)\n","        trainer.save_metrics(\"eval\", metrics)\n","\n","    # Prediction\n","    if training_args.do_predict:\n","        logger.info(\"*** Predict ***\")\n","        predictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix=\"predict\")\n","\n","        max_predict_samples = (\n","            data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)\n","        )\n","        metrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n","\n","        trainer.log_metrics(\"predict\", metrics)\n","        trainer.save_metrics(\"predict\", metrics)\n","\n","        output_predict_file = os.path.join(training_args.output_dir, \"test_predictions.csv\")\n","        if trainer.is_world_process_zero():\n","            with open(output_predict_file, \"w\") as writer:\n","                if isinstance(predictions[0], np.float32):\n","                   pred_line = f'{predictions[0]:.5f}'\n","                   writer.write(pred_line)\n","                else:\n","                   for index, pred_probs in enumerate(predictions[0]):\n","                       pred_line = '\\t'.join([f'{pred:.5f}' for pred in pred_probs])\n","                       writer.write(f\"{index}\\t{pred_line}\\n\")\n","\n","\n","    # Clean up checkpoints\n","    checkpoints = [filepath for filepath in glob.glob(f'{training_args.output_dir}/*/') if '/checkpoint' in filepath]\n","    for checkpoint in checkpoints:\n","        shutil.rmtree(checkpoint)\n","\n","\n","if __name__ == \"__main__\":\n"," #    For Evaluation\n","    training_args = TrainingArguments(\n","        do_train = True,\n","        do_eval = True,\n","        do_predict = True,\n","        output_dir=os.getcwd(),\n","        overwrite_output_dir=True,\n","        num_train_epochs=20,\n","        per_device_train_batch_size=8,\n","        per_device_eval_batch_size=8,\n","        save_steps=500,\n","        save_total_limit=2,\n","        fp16=False,\n","        logging_dir=\"./logs\",\n","        logging_steps=200,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=500,\n","        logging_first_step=False,\n","        load_best_model_at_end = True,\n","        metric_for_best_model=\"micro-f1\",\n","    )\n","    main(training_args)\n"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":538363,"status":"error","timestamp":1691379989291,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"},"user_tz":-60},"id":"P9ToKgj6hA90","outputId":"fcf3840d-895c-49be-bec1-d828620b1ba8"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n","[WARNING|modeling_utils.py:3331] 2023-08-07 03:37:36,623 \u003e\u003e Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8e9564a4b1fe4c258278f29f606545a1","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on train dataset:   0%|          | 0/5532 [00:00\u003c?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    \u003cdiv\u003e\n","      \n","      \u003cprogress value='6500' max='13840' style='width:300px; height:20px; vertical-align: middle;'\u003e\u003c/progress\u003e\n","      [ 6500/13840 08:41 \u003c 09:49, 12.46 it/s, Epoch 9/20]\n","    \u003c/div\u003e\n","    \u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n"," \u003ctr style=\"text-align: left;\"\u003e\n","      \u003cth\u003eStep\u003c/th\u003e\n","      \u003cth\u003eTraining Loss\u003c/th\u003e\n","      \u003cth\u003eValidation Loss\u003c/th\u003e\n","      \u003cth\u003eMacro-f1\u003c/th\u003e\n","      \u003cth\u003eMicro-f1\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e500\u003c/td\u003e\n","      \u003ctd\u003e0.071900\u003c/td\u003e\n","      \u003ctd\u003e0.060420\u003c/td\u003e\n","      \u003ctd\u003e0.105195\u003c/td\u003e\n","      \u003ctd\u003e0.895163\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e1000\u003c/td\u003e\n","      \u003ctd\u003e0.054400\u003c/td\u003e\n","      \u003ctd\u003e0.043522\u003c/td\u003e\n","      \u003ctd\u003e0.148524\u003c/td\u003e\n","      \u003ctd\u003e0.897789\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e1500\u003c/td\u003e\n","      \u003ctd\u003e0.044000\u003c/td\u003e\n","      \u003ctd\u003e0.037218\u003c/td\u003e\n","      \u003ctd\u003e0.454809\u003c/td\u003e\n","      \u003ctd\u003e0.923679\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e2000\u003c/td\u003e\n","      \u003ctd\u003e0.029300\u003c/td\u003e\n","      \u003ctd\u003e0.035990\u003c/td\u003e\n","      \u003ctd\u003e0.400300\u003c/td\u003e\n","      \u003ctd\u003e0.928665\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e2500\u003c/td\u003e\n","      \u003ctd\u003e0.024400\u003c/td\u003e\n","      \u003ctd\u003e0.038192\u003c/td\u003e\n","      \u003ctd\u003e0.453754\u003c/td\u003e\n","      \u003ctd\u003e0.928105\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e3000\u003c/td\u003e\n","      \u003ctd\u003e0.019400\u003c/td\u003e\n","      \u003ctd\u003e0.033721\u003c/td\u003e\n","      \u003ctd\u003e0.607740\u003c/td\u003e\n","      \u003ctd\u003e0.934645\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e3500\u003c/td\u003e\n","      \u003ctd\u003e0.018400\u003c/td\u003e\n","      \u003ctd\u003e0.035959\u003c/td\u003e\n","      \u003ctd\u003e0.665165\u003c/td\u003e\n","      \u003ctd\u003e0.940637\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e4000\u003c/td\u003e\n","      \u003ctd\u003e0.017200\u003c/td\u003e\n","      \u003ctd\u003e0.032155\u003c/td\u003e\n","      \u003ctd\u003e0.658253\u003c/td\u003e\n","      \u003ctd\u003e0.937527\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e4500\u003c/td\u003e\n","      \u003ctd\u003e0.012400\u003c/td\u003e\n","      \u003ctd\u003e0.033207\u003c/td\u003e\n","      \u003ctd\u003e0.714500\u003c/td\u003e\n","      \u003ctd\u003e0.946129\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e5000\u003c/td\u003e\n","      \u003ctd\u003e0.012400\u003c/td\u003e\n","      \u003ctd\u003e0.034416\u003c/td\u003e\n","      \u003ctd\u003e0.744933\u003c/td\u003e\n","      \u003ctd\u003e0.946542\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e5500\u003c/td\u003e\n","      \u003ctd\u003e0.008700\u003c/td\u003e\n","      \u003ctd\u003e0.033138\u003c/td\u003e\n","      \u003ctd\u003e0.730534\u003c/td\u003e\n","      \u003ctd\u003e0.945098\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e6000\u003c/td\u003e\n","      \u003ctd\u003e0.008500\u003c/td\u003e\n","      \u003ctd\u003e0.037744\u003c/td\u003e\n","      \u003ctd\u003e0.708073\u003c/td\u003e\n","      \u003ctd\u003e0.938428\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e6500\u003c/td\u003e\n","      \u003ctd\u003e0.003700\u003c/td\u003e\n","      \u003ctd\u003e0.040912\u003c/td\u003e\n","      \u003ctd\u003e0.704874\u003c/td\u003e\n","      \u003ctd\u003e0.939817\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\u003cp\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["***** train metrics *****\n","  epoch                    =       9.39\n","  total_flos               =  3183502GF\n","  train_loss               =     0.0271\n","  train_runtime            = 0:08:41.58\n","  train_samples            =       5532\n","  train_samples_per_second =    212.124\n","  train_steps_per_second   =     26.535\n"]},{"data":{"text/html":[],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["***** eval metrics *****\n","  epoch                   =       9.39\n","  eval_loss               =     0.0344\n","  eval_macro-f1           =     0.7449\n","  eval_micro-f1           =     0.9465\n","  eval_runtime            = 0:00:05.14\n","  eval_samples            =       2275\n","  eval_samples_per_second =    441.885\n","  eval_steps_per_second   =     55.357\n","***** predict metrics *****\n","  predict_loss               =     0.0323\n","  predict_macro-f1           =     0.7677\n","  predict_micro-f1           =     0.9456\n","  predict_runtime            = 0:00:03.85\n","  predict_samples            =       1607\n","  predict_samples_per_second =    417.285\n","  predict_steps_per_second   =     52.193\n"]},{"ename":"TypeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-45-567cada72e38\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 421\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mmetric_for_best_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"micro-f1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     )\n\u001b[0;32m--\u003e 443\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m\u003cipython-input-45-567cada72e38\u003e\u001b[0m in \u001b[0;36mmain\u001b[0;34m(training_args)\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                    \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_probs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 411\u001b[0;31m                        \u001b[0mpred_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\t'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'{pred:.5f}'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_probs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m                        \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{index}\\t{pred_line}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'numpy.float32' object is not iterable"]}],"source":["#!/usr/bin/env python\n","# coding=utf-8\n","\"\"\" Finetuning models on UNFAIR-ToC (e.g. Bert, RoBERTa, LEGAL-BERT).\"\"\"\n","\n","import logging\n","import os\n","import random\n","import sys\n","from dataclasses import dataclass, field\n","from typing import Optional\n","\n","import datasets\n","from datasets import load_dataset\n","from sklearn.metrics import f1_score\n","\n","from scipy.special import expit\n","import glob\n","import shutil\n","import numpy as np\n","\n","import transformers\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    DataCollatorWithPadding,\n","    EvalPrediction,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    default_data_collator,\n","    set_seed,\n","    EarlyStoppingCallback,\n",")\n","from transformers.trainer_utils import get_last_checkpoint\n","from transformers.utils import check_min_version\n","from transformers.utils.versions import require_version\n","\n","\n","# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n","check_min_version(\"4.9.0\")\n","\n","require_version(\"datasets\u003e=1.8.0\", \"To fix: pip install -r examples/pytorch/text-classification/requirements.txt\")\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","@dataclass\n","class DataTrainingArguments:\n","    \"\"\"\n","    Arguments pertaining to what data we are going to input our model for training and eval.\n","\n","    Using `HfArgumentParser` we can turn this class\n","    into argparse arguments to be able to specify them on\n","    the command line.\n","    \"\"\"\n","\n","    max_seq_length: Optional[int] = field(\n","        default=128,\n","        metadata={\n","            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n","            \"than this will be truncated, sequences shorter will be padded.\"\n","        },\n","    )\n","    overwrite_cache: bool = field(\n","        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n","    )\n","    pad_to_max_length: bool = field(\n","        default=True,\n","        metadata={\n","            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n","            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n","        },\n","    )\n","    max_train_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_eval_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_predict_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    server_ip: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n","    server_port: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n","\n","\n","@dataclass\n","class ModelArguments:\n","    \"\"\"\n","    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n","    \"\"\"\n","\n","    model_name_or_path: str = field(\n","        default=None, metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n","    )\n","    config_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n","    )\n","    tokenizer_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n","    )\n","    cache_dir: Optional[str] = field(\n","        default=None,\n","        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n","    )\n","    do_lower_case: Optional[bool] = field(\n","        default=True,\n","        metadata={\"help\": \"arg to indicate if tokenizer should do lower case in AutoTokenizer.from_pretrained()\"},\n","    )\n","    use_fast_tokenizer: bool = field(\n","        default=True,\n","        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n","    )\n","    model_revision: str = field(\n","        default=\"main\",\n","        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n","    )\n","    use_auth_token: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n","            \"with private models).\"\n","        },\n","    )\n","\n","\n","def main(training_args):\n","    # See all possible arguments in src/transformers/training_args.py\n","    # or by passing the --help flag to this script.\n","    # We now keep distinct sets of args, for a cleaner separation of concerns.\n","\n","    model_args = ModelArguments(\n","        model_name_or_path=\"bert-base-uncased\",\n","        #hierarchical=True,\n","        do_lower_case=True,\n","        use_fast_tokenizer=True,\n","    )\n","    data_args = DataTrainingArguments(\n","        max_seq_length=128,\n","        #max_segments=64,\n","        #max_seg_length=128,\n","        overwrite_cache=False,\n","        pad_to_max_length=True,\n","    )\n","\n","    # Setup distant debugging if needed\n","    if data_args.server_ip and data_args.server_port:\n","        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n","        import ptvsd\n","\n","        print(\"Waiting for debugger attach\")\n","        ptvsd.enable_attach(address=(data_args.server_ip, data_args.server_port), redirect_output=True)\n","        ptvsd.wait_for_attach()\n","\n","    # Fix boolean parameter\n","    if model_args.do_lower_case == 'False' or not model_args.do_lower_case:\n","        model_args.do_lower_case = False\n","        'Tokenizer do_lower_case False'\n","    else:\n","        model_args.do_lower_case = True\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        handlers=[logging.StreamHandler(sys.stdout)],\n","    )\n","\n","    log_level = training_args.get_process_log_level()\n","    logger.setLevel(log_level)\n","    datasets.utils.logging.set_verbosity(log_level)\n","    transformers.utils.logging.set_verbosity(log_level)\n","    transformers.utils.logging.enable_default_handler()\n","    transformers.utils.logging.enable_explicit_format()\n","\n","    # Log on each process the small summary:\n","    logger.warning(\n","        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n","        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n","    )\n","    logger.info(f\"Training/evaluation parameters {training_args}\")\n","\n","    # Detecting last checkpoint.\n","    last_checkpoint = None\n","    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n","        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n","        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) \u003e 0:\n","            raise ValueError(\n","                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n","                \"Use --overwrite_output_dir to overcome.\"\n","            )\n","        elif last_checkpoint is not None:\n","            logger.info(\n","                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n","                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n","            )\n","\n","    # Set seed before initializing model.\n","    set_seed(training_args.seed)\n","\n","    # In distributed training, the load_dataset function guarantees that only one local process can concurrently\n","    # download the dataset.\n","    # Downloading and loading eurlex dataset from the hub.\n","    if training_args.do_train:\n","        train_dataset = merged_dataset\n","\n","    if training_args.do_eval:\n","        eval_dataset = load_dataset(\"lex_glue\", \"unfair_tos\", split=\"validation\", data_dir='data', cache_dir=model_args.cache_dir)\n","\n","    if training_args.do_predict:\n","        predict_dataset = load_dataset(\"lex_glue\", \"unfair_tos\", split=\"test\", data_dir='data', cache_dir=model_args.cache_dir)\n","\n","    # Labels\n","    label_list = list(range(8))\n","    num_labels = len(label_list)\n","\n","    # Load pretrained model and tokenizer\n","    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n","    # download model \u0026 vocab.\n","    config = AutoConfig.from_pretrained(\n","        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n","        num_labels=num_labels,\n","        finetuning_task=\"unfair_toc\",\n","        cache_dir=model_args.cache_dir,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","\n","    if config.model_type == 'big_bird':\n","        config.attention_type = 'original_full'\n","\n","    if config.model_type == 'longformer':\n","        config.attention_window = [128] * config.num_hidden_layers\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n","        do_lower_case=model_args.do_lower_case,\n","        cache_dir=model_args.cache_dir,\n","        use_fast=model_args.use_fast_tokenizer,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        model_args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","        config=config,\n","        cache_dir=model_args.cache_dir,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","\n","    # Preprocessing the datasets\n","    # Padding strategy\n","    if data_args.pad_to_max_length:\n","        padding = \"max_length\"\n","    else:\n","        # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n","        padding = False\n","\n","    def preprocess_function(examples):\n","        # Tokenize the texts\n","        batch = tokenizer(\n","            examples[\"text\"],\n","            padding=padding,\n","            max_length=data_args.max_seq_length,\n","            truncation=True,\n","        )\n","        batch[\"labels\"] = [[1 if label in labels else 0 for label in label_list] for labels in\n","                              examples[\"labels\"]]\n","\n","        return batch\n","\n","    if training_args.do_train:\n","        if data_args.max_train_samples is not None:\n","            train_dataset = train_dataset.select(range(data_args.max_train_samples))\n","        with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n","            train_dataset = train_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on train dataset\",\n","            )\n","        # Log a few random samples from the training set:\n","        for index in random.sample(range(len(train_dataset)), 3):\n","            logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n","\n","    if training_args.do_eval:\n","        if data_args.max_eval_samples is not None:\n","            eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))\n","        with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n","            eval_dataset = eval_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on validation dataset\",\n","            )\n","\n","    if training_args.do_predict:\n","        if data_args.max_predict_samples is not None:\n","            predict_dataset = predict_dataset.select(range(data_args.max_predict_samples))\n","        with training_args.main_process_first(desc=\"prediction dataset map pre-processing\"):\n","            predict_dataset = predict_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on prediction dataset\",\n","            )\n","\n","    # You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n","    # predictions and label_ids field) and has to return a dictionary string to float.\n","    def compute_metrics(p: EvalPrediction):\n","        # Fix gold labels\n","        y_true = np.zeros((p.label_ids.shape[0], p.label_ids.shape[1] + 1), dtype=np.int32)\n","        y_true[:, :-1] = p.label_ids\n","        y_true[:, -1] = (np.sum(p.label_ids, axis=1) == 0).astype('int32')\n","        # Fix predictions\n","        logits = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","        preds = (expit(logits) \u003e 0.5).astype('int32')\n","        y_pred = np.zeros((p.label_ids.shape[0], p.label_ids.shape[1] + 1), dtype=np.int32)\n","        y_pred[:, :-1] = preds\n","        y_pred[:, -1] = (np.sum(preds, axis=1) == 0).astype('int32')\n","        # Compute scores\n","        macro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='macro', zero_division=0)\n","        micro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='micro', zero_division=0)\n","        return {'macro-f1': macro_f1, 'micro-f1': micro_f1}\n","\n","    # Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n","    if data_args.pad_to_max_length:\n","        data_collator = default_data_collator\n","    elif training_args.fp16:\n","        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n","    else:\n","        data_collator = None\n","\n","    # Initialize our Trainer\n","    trainer = MultilabelTrainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset if training_args.do_train else None,\n","        eval_dataset=eval_dataset if training_args.do_eval else None,\n","        compute_metrics=compute_metrics,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n","    )\n","\n","    # Training\n","    if training_args.do_train:\n","        checkpoint = None\n","        if training_args.resume_from_checkpoint is not None:\n","            checkpoint = training_args.resume_from_checkpoint\n","        elif last_checkpoint is not None:\n","            checkpoint = last_checkpoint\n","        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n","        metrics = train_result.metrics\n","        max_train_samples = (\n","            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n","        )\n","        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n","\n","        trainer.save_model()  # Saves the tokenizer too for easy upload\n","\n","        trainer.log_metrics(\"train\", metrics)\n","        trainer.save_metrics(\"train\", metrics)\n","        trainer.save_state()\n","\n","    # Evaluation\n","    if training_args.do_eval:\n","        logger.info(\"*** Evaluate ***\")\n","        metrics = trainer.evaluate(eval_dataset=eval_dataset)\n","\n","        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n","        metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n","\n","        trainer.log_metrics(\"eval\", metrics)\n","        trainer.save_metrics(\"eval\", metrics)\n","\n","    # Prediction\n","    if training_args.do_predict:\n","        logger.info(\"*** Predict ***\")\n","        predictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix=\"predict\")\n","\n","        max_predict_samples = (\n","            data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)\n","        )\n","        metrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n","\n","        trainer.log_metrics(\"predict\", metrics)\n","        trainer.save_metrics(\"predict\", metrics)\n","\n","        output_predict_file = os.path.join(training_args.output_dir, \"test_predictions.csv\")\n","        if trainer.is_world_process_zero():\n","            with open(output_predict_file, \"w\") as writer:\n","                if isinstance(predictions[0], np.float32):\n","                   pred_line = f'{predictions[0]:.5f}'\n","                   writer.write(pred_line)\n","                else:\n","                   for index, pred_probs in enumerate(predictions[0]):\n","                       pred_line = '\\t'.join([f'{pred:.5f}' for pred in pred_probs])\n","                       writer.write(f\"{index}\\t{pred_line}\\n\")\n","\n","\n","    # Clean up checkpoints\n","    checkpoints = [filepath for filepath in glob.glob(f'{training_args.output_dir}/*/') if '/checkpoint' in filepath]\n","    for checkpoint in checkpoints:\n","        shutil.rmtree(checkpoint)\n","\n","\n","if __name__ == \"__main__\":\n"," #    For Evaluation\n","    training_args = TrainingArguments(\n","        do_train = True,\n","        do_eval = True,\n","        do_predict = True,\n","        output_dir=os.getcwd(),\n","        overwrite_output_dir=True,\n","        num_train_epochs=20,\n","        per_device_train_batch_size=8,\n","        per_device_eval_batch_size=8,\n","        save_steps=500,\n","        save_total_limit=2,\n","        fp16=False,\n","        logging_dir=\"./logs\",\n","        logging_steps=200,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=500,\n","        logging_first_step=False,\n","        load_best_model_at_end = True,\n","        metric_for_best_model=\"micro-f1\",\n","    )\n","    main(training_args)\n"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":590691,"status":"error","timestamp":1691383345278,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"},"user_tz":-60},"id":"1xPcWGA0fkns","outputId":"01d13e2f-52b8-400d-8d43-8fd14114e541"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n","[WARNING|modeling_utils.py:3331] 2023-08-07 04:32:40,522 \u003e\u003e Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e6ccd16827ce4ec898e02e5335dbdbc8","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on train dataset:   0%|          | 0/5532 [00:00\u003c?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    \u003cdiv\u003e\n","      \n","      \u003cprogress value='7500' max='13830' style='width:300px; height:20px; vertical-align: middle;'\u003e\u003c/progress\u003e\n","      [ 7500/13830 09:25 \u003c 07:57, 13.26 it/s, Epoch 5/10]\n","    \u003c/div\u003e\n","    \u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n"," \u003ctr style=\"text-align: left;\"\u003e\n","      \u003cth\u003eStep\u003c/th\u003e\n","      \u003cth\u003eTraining Loss\u003c/th\u003e\n","      \u003cth\u003eValidation Loss\u003c/th\u003e\n","      \u003cth\u003eMacro-f1\u003c/th\u003e\n","      \u003cth\u003eMicro-f1\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e500\u003c/td\u003e\n","      \u003ctd\u003e0.094800\u003c/td\u003e\n","      \u003ctd\u003e0.071137\u003c/td\u003e\n","      \u003ctd\u003e0.105195\u003c/td\u003e\n","      \u003ctd\u003e0.895163\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e1000\u003c/td\u003e\n","      \u003ctd\u003e0.061500\u003c/td\u003e\n","      \u003ctd\u003e0.052649\u003c/td\u003e\n","      \u003ctd\u003e0.105195\u003c/td\u003e\n","      \u003ctd\u003e0.895163\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e1500\u003c/td\u003e\n","      \u003ctd\u003e0.051100\u003c/td\u003e\n","      \u003ctd\u003e0.036915\u003c/td\u003e\n","      \u003ctd\u003e0.295095\u003c/td\u003e\n","      \u003ctd\u003e0.923616\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e2000\u003c/td\u003e\n","      \u003ctd\u003e0.033600\u003c/td\u003e\n","      \u003ctd\u003e0.034057\u003c/td\u003e\n","      \u003ctd\u003e0.528733\u003c/td\u003e\n","      \u003ctd\u003e0.929337\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e2500\u003c/td\u003e\n","      \u003ctd\u003e0.032500\u003c/td\u003e\n","      \u003ctd\u003e0.027980\u003c/td\u003e\n","      \u003ctd\u003e0.572500\u003c/td\u003e\n","      \u003ctd\u003e0.943314\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e3000\u003c/td\u003e\n","      \u003ctd\u003e0.024400\u003c/td\u003e\n","      \u003ctd\u003e0.026893\u003c/td\u003e\n","      \u003ctd\u003e0.665441\u003c/td\u003e\n","      \u003ctd\u003e0.943801\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e3500\u003c/td\u003e\n","      \u003ctd\u003e0.023600\u003c/td\u003e\n","      \u003ctd\u003e0.028723\u003c/td\u003e\n","      \u003ctd\u003e0.578236\u003c/td\u003e\n","      \u003ctd\u003e0.930070\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e4000\u003c/td\u003e\n","      \u003ctd\u003e0.018400\u003c/td\u003e\n","      \u003ctd\u003e0.025038\u003c/td\u003e\n","      \u003ctd\u003e0.719819\u003c/td\u003e\n","      \u003ctd\u003e0.946702\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e4500\u003c/td\u003e\n","      \u003ctd\u003e0.018500\u003c/td\u003e\n","      \u003ctd\u003e0.024972\u003c/td\u003e\n","      \u003ctd\u003e0.744091\u003c/td\u003e\n","      \u003ctd\u003e0.949552\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e5000\u003c/td\u003e\n","      \u003ctd\u003e0.012900\u003c/td\u003e\n","      \u003ctd\u003e0.025080\u003c/td\u003e\n","      \u003ctd\u003e0.729051\u003c/td\u003e\n","      \u003ctd\u003e0.946955\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e5500\u003c/td\u003e\n","      \u003ctd\u003e0.020100\u003c/td\u003e\n","      \u003ctd\u003e0.025731\u003c/td\u003e\n","      \u003ctd\u003e0.749196\u003c/td\u003e\n","      \u003ctd\u003e0.943231\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e6000\u003c/td\u003e\n","      \u003ctd\u003e0.011900\u003c/td\u003e\n","      \u003ctd\u003e0.025620\u003c/td\u003e\n","      \u003ctd\u003e0.770990\u003c/td\u003e\n","      \u003ctd\u003e0.953275\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e6500\u003c/td\u003e\n","      \u003ctd\u003e0.007400\u003c/td\u003e\n","      \u003ctd\u003e0.027329\u003c/td\u003e\n","      \u003ctd\u003e0.736068\u003c/td\u003e\n","      \u003ctd\u003e0.941305\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e7000\u003c/td\u003e\n","      \u003ctd\u003e0.010300\u003c/td\u003e\n","      \u003ctd\u003e0.026546\u003c/td\u003e\n","      \u003ctd\u003e0.764990\u003c/td\u003e\n","      \u003ctd\u003e0.951883\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e7500\u003c/td\u003e\n","      \u003ctd\u003e0.007900\u003c/td\u003e\n","      \u003ctd\u003e0.027250\u003c/td\u003e\n","      \u003ctd\u003e0.753929\u003c/td\u003e\n","      \u003ctd\u003e0.949389\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\u003cp\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["***** train metrics *****\n","  epoch                    =       5.42\n","  total_flos               =  1837908GF\n","  train_loss               =     0.0345\n","  train_runtime            = 0:09:25.52\n","  train_samples            =       5532\n","  train_samples_per_second =      97.82\n","  train_steps_per_second   =     24.455\n"]},{"data":{"text/html":[],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["***** eval metrics *****\n","  epoch                   =       5.42\n","  eval_loss               =     0.0256\n","  eval_macro-f1           =      0.771\n","  eval_micro-f1           =     0.9533\n","  eval_runtime            = 0:00:08.34\n","  eval_samples            =       2275\n","  eval_samples_per_second =    272.651\n","  eval_steps_per_second   =     68.193\n","***** predict metrics *****\n","  predict_loss               =     0.0252\n","  predict_macro-f1           =     0.7968\n","  predict_micro-f1           =     0.9549\n","  predict_runtime            = 0:00:06.43\n","  predict_samples            =       1607\n","  predict_samples_per_second =    249.776\n","  predict_steps_per_second   =     62.483\n"]},{"ename":"TypeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-49-4b057f0f192b\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 425\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;31m# Check if this configuration is the best so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 461\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcurrent_metric\u001b[0m \u001b[0;34m\u003e\u001b[0m \u001b[0mbest_metric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m             \u001b[0mbest_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: '\u003e' not supported between instances of 'NoneType' and 'float'"]}],"source":["#!/usr/bin/env python\n","# coding=utf-8\n","\"\"\" Finetuning models on UNFAIR-ToC (e.g. Bert, RoBERTa, LEGAL-BERT).\"\"\"\n","\n","import logging\n","import os\n","import random\n","import sys\n","from dataclasses import dataclass, field\n","from sklearn.model_selection import ParameterGrid\n","from typing import Optional\n","\n","import datasets\n","from datasets import load_dataset\n","from sklearn.metrics import f1_score\n","#from trainer import MultilabelTrainer\n","from scipy.special import expit\n","import glob\n","import shutil\n","import numpy as np\n","\n","import transformers\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    DataCollatorWithPadding,\n","    EvalPrediction,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    default_data_collator,\n","    set_seed,\n","    EarlyStoppingCallback,\n",")\n","from transformers.trainer_utils import get_last_checkpoint\n","from transformers.utils import check_min_version\n","from transformers.utils.versions import require_version\n","\n","\n","# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n","check_min_version(\"4.9.0\")\n","\n","require_version(\"datasets\u003e=1.8.0\", \"To fix: pip install -r examples/pytorch/text-classification/requirements.txt\")\n","\n","logger = logging.getLogger(__name__)\n","\n","param_grid = {\n","    'learning_rate': [1e-5, 2e-5, 3e-5],  # Learning rates to try\n","    'num_train_epochs': [10, 15, 20],        # Number of training epochs to try\n","    'per_device_train_batch_size': [4, 8, 16],  # Batch sizes for training\n","    'per_device_eval_batch_size': [4, 8, 16],   # Batch sizes for evaluation\n","}\n","\n","\n","@dataclass\n","class DataTrainingArguments:\n","    \"\"\"\n","    Arguments pertaining to what data we are going to input our model for training and eval.\n","\n","    Using `HfArgumentParser` we can turn this class\n","    into argparse arguments to be able to specify them on\n","    the command line.\n","    \"\"\"\n","\n","    max_seq_length: Optional[int] = field(\n","        default=128,\n","        metadata={\n","            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n","            \"than this will be truncated, sequences shorter will be padded.\"\n","        },\n","    )\n","    overwrite_cache: bool = field(\n","        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n","    )\n","    pad_to_max_length: bool = field(\n","        default=True,\n","        metadata={\n","            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n","            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n","        },\n","    )\n","    max_train_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_eval_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_predict_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    server_ip: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n","    server_port: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n","\n","\n","@dataclass\n","class ModelArguments:\n","    \"\"\"\n","    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n","    \"\"\"\n","\n","    model_name_or_path: str = field(\n","        default=None, metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n","    )\n","    config_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n","    )\n","    tokenizer_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n","    )\n","    cache_dir: Optional[str] = field(\n","        default=None,\n","        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n","    )\n","    do_lower_case: Optional[bool] = field(\n","        default=True,\n","        metadata={\"help\": \"arg to indicate if tokenizer should do lower case in AutoTokenizer.from_pretrained()\"},\n","    )\n","    use_fast_tokenizer: bool = field(\n","        default=True,\n","        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n","    )\n","    model_revision: str = field(\n","        default=\"main\",\n","        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n","    )\n","    use_auth_token: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n","            \"with private models).\"\n","        },\n","    )\n","\n","\n","def main(training_args):\n","    # See all possible arguments in src/transformers/training_args.py\n","    # or by passing the --help flag to this script.\n","    # We now keep distinct sets of args, for a cleaner separation of concerns.\n","\n","    model_args = ModelArguments(\n","        model_name_or_path=\"nlpaueb/legal-bert-base-uncased\",\n","        #hierarchical=True,\n","        do_lower_case=True,\n","        use_fast_tokenizer=True,\n","    )\n","    data_args = DataTrainingArguments(\n","        max_seq_length=128,\n","        #max_segments=64,\n","        #max_seg_length=128,\n","        overwrite_cache=False,\n","        pad_to_max_length=True,\n","    )\n","\n","\n","    # Setup distant debugging if needed\n","    if data_args.server_ip and data_args.server_port:\n","        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n","        import ptvsd\n","\n","        print(\"Waiting for debugger attach\")\n","        ptvsd.enable_attach(address=(data_args.server_ip, data_args.server_port), redirect_output=True)\n","        ptvsd.wait_for_attach()\n","\n","    # Fix boolean parameter\n","    if model_args.do_lower_case == 'False' or not model_args.do_lower_case:\n","        model_args.do_lower_case = False\n","        'Tokenizer do_lower_case False'\n","    else:\n","        model_args.do_lower_case = True\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        handlers=[logging.StreamHandler(sys.stdout)],\n","    )\n","\n","    log_level = training_args.get_process_log_level()\n","    logger.setLevel(log_level)\n","    datasets.utils.logging.set_verbosity(log_level)\n","    transformers.utils.logging.set_verbosity(log_level)\n","    transformers.utils.logging.enable_default_handler()\n","    transformers.utils.logging.enable_explicit_format()\n","\n","    # Log on each process the small summary:\n","    logger.warning(\n","        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n","        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n","    )\n","    logger.info(f\"Training/evaluation parameters {training_args}\")\n","\n","    # Detecting last checkpoint.\n","    last_checkpoint = None\n","    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n","        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n","        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) \u003e 0:\n","            raise ValueError(\n","                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n","                \"Use --overwrite_output_dir to overcome.\"\n","            )\n","        elif last_checkpoint is not None:\n","            logger.info(\n","                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n","                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n","            )\n","\n","    # Set seed before initializing model.\n","    set_seed(training_args.seed)\n","\n","    # In distributed training, the load_dataset function guarantees that only one local process can concurrently\n","    # download the dataset.\n","    # Downloading and loading eurlex dataset from the hub.\n","    if training_args.do_train:\n","        train_dataset = merged_dataset\n","\n","    if training_args.do_eval:\n","        eval_dataset = load_dataset(\"lex_glue\", \"unfair_tos\", split=\"validation\", data_dir='data', cache_dir=model_args.cache_dir)\n","\n","    if training_args.do_predict:\n","        predict_dataset = load_dataset(\"lex_glue\", \"unfair_tos\", split=\"test\", data_dir='data', cache_dir=model_args.cache_dir)\n","\n","    # Labels\n","    label_list = list(range(8))\n","    num_labels = len(label_list)\n","\n","    # Load pretrained model and tokenizer\n","    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n","    # download model \u0026 vocab.\n","    config = AutoConfig.from_pretrained(\n","        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n","        num_labels=num_labels,\n","        finetuning_task=\"unfair_toc\",\n","        cache_dir=model_args.cache_dir,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","\n","    if config.model_type == 'big_bird':\n","        config.attention_type = 'original_full'\n","\n","    if config.model_type == 'longformer':\n","        config.attention_window = [128] * config.num_hidden_layers\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n","        do_lower_case=model_args.do_lower_case,\n","        cache_dir=model_args.cache_dir,\n","        use_fast=model_args.use_fast_tokenizer,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        model_args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","        config=config,\n","        cache_dir=model_args.cache_dir,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","\n","    # Preprocessing the datasets\n","    # Padding strategy\n","    if data_args.pad_to_max_length:\n","        padding = \"max_length\"\n","    else:\n","        # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n","        padding = False\n","\n","    def preprocess_function(examples):\n","        # Tokenize the texts\n","        batch = tokenizer(\n","            examples[\"text\"],\n","            padding=padding,\n","            max_length=data_args.max_seq_length,\n","            truncation=True,\n","        )\n","        batch[\"labels\"] = [[1 if label in labels else 0 for label in label_list] for labels in\n","                              examples[\"labels\"]]\n","\n","        return batch\n","\n","    if training_args.do_train:\n","        if data_args.max_train_samples is not None:\n","            train_dataset = train_dataset.select(range(data_args.max_train_samples))\n","        with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n","            train_dataset = train_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on train dataset\",\n","            )\n","        # Log a few random samples from the training set:\n","        for index in random.sample(range(len(train_dataset)), 3):\n","            logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n","\n","    if training_args.do_eval:\n","        if data_args.max_eval_samples is not None:\n","            eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))\n","        with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n","            eval_dataset = eval_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on validation dataset\",\n","            )\n","\n","    if training_args.do_predict:\n","        if data_args.max_predict_samples is not None:\n","            predict_dataset = predict_dataset.select(range(data_args.max_predict_samples))\n","        with training_args.main_process_first(desc=\"prediction dataset map pre-processing\"):\n","            predict_dataset = predict_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on prediction dataset\",\n","            )\n","\n","    # You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n","    # predictions and label_ids field) and has to return a dictionary string to float.\n","    def compute_metrics(p: EvalPrediction):\n","        # Fix gold labels\n","        y_true = np.zeros((p.label_ids.shape[0], p.label_ids.shape[1] + 1), dtype=np.int32)\n","        y_true[:, :-1] = p.label_ids\n","        y_true[:, -1] = (np.sum(p.label_ids, axis=1) == 0).astype('int32')\n","        # Fix predictions\n","        logits = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","        preds = (expit(logits) \u003e 0.5).astype('int32')\n","        y_pred = np.zeros((p.label_ids.shape[0], p.label_ids.shape[1] + 1), dtype=np.int32)\n","        y_pred[:, :-1] = preds\n","        y_pred[:, -1] = (np.sum(preds, axis=1) == 0).astype('int32')\n","        # Compute scores\n","        macro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='macro', zero_division=0)\n","        micro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='micro', zero_division=0)\n","        return {'macro-f1': macro_f1, 'micro-f1': micro_f1}\n","\n","    # Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n","    if data_args.pad_to_max_length:\n","        data_collator = default_data_collator\n","    elif training_args.fp16:\n","        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n","    else:\n","        data_collator = None\n","\n","    # Initialize our Trainer\n","    trainer = MultilabelTrainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset if training_args.do_train else None,\n","        eval_dataset=eval_dataset if training_args.do_eval else None,\n","        compute_metrics=compute_metrics,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n","    )\n","\n","    # Training\n","    if training_args.do_train:\n","        checkpoint = None\n","        if training_args.resume_from_checkpoint is not None:\n","            checkpoint = training_args.resume_from_checkpoint\n","        elif last_checkpoint is not None:\n","            checkpoint = last_checkpoint\n","        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n","        metrics = train_result.metrics\n","        max_train_samples = (\n","            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n","        )\n","        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n","\n","        trainer.save_model()  # Saves the tokenizer too for easy upload\n","\n","        trainer.log_metrics(\"train\", metrics)\n","        trainer.save_metrics(\"train\", metrics)\n","        trainer.save_state()\n","\n","    # Evaluation\n","    if training_args.do_eval:\n","        logger.info(\"*** Evaluate ***\")\n","        metrics = trainer.evaluate(eval_dataset=eval_dataset)\n","\n","        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n","        metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n","\n","        trainer.log_metrics(\"eval\", metrics)\n","        trainer.save_metrics(\"eval\", metrics)\n","\n","    # Prediction\n","    if training_args.do_predict:\n","        logger.info(\"*** Predict ***\")\n","        predictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix=\"predict\")\n","\n","        max_predict_samples = (\n","            data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)\n","        )\n","        metrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n","\n","        trainer.log_metrics(\"predict\", metrics)\n","        trainer.save_metrics(\"predict\", metrics)\n","\n","        output_predict_file = os.path.join(training_args.output_dir, \"test_predictions.csv\")\n","        if trainer.is_world_process_zero():\n","            with open(output_predict_file, \"w\") as writer:\n","                for index, pred_list in enumerate(predictions[0]):\n","                    pred_line = f'{pred_list:.5f}'\n","                    writer.write(f\"{index}\\t{pred_line}\\n\")\n","\n","    # Clean up checkpoints\n","    checkpoints = [filepath for filepath in glob.glob(f'{training_args.output_dir}/*/') if '/checkpoint' in filepath]\n","    for checkpoint in checkpoints:\n","        shutil.rmtree(checkpoint)\n","\n","\n","if __name__ == \"__main__\":\n"," #    For Evaluation\n","    training_args = TrainingArguments(\n","        do_train = True,\n","        do_eval = True,\n","        do_predict = True,\n","        output_dir=os.getcwd(),\n","        overwrite_output_dir=True,\n","        num_train_epochs=2,\n","        per_device_train_batch_size=8,\n","        per_device_eval_batch_size=8,\n","        save_steps=500,\n","        save_total_limit=2,\n","        fp16=False,\n","        logging_dir=\"./logs\",\n","        logging_steps=200,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=500,\n","        logging_first_step=False,\n","        load_best_model_at_end = True,\n","        metric_for_best_model=\"macro-f1\",\n","    )\n","    best_metric = 0.0\n","    best_params = None\n","\n","    for params in ParameterGrid(param_grid):\n","        #training_args = base_training_args.clone()  # Clone the base training args\n","        training_args.learning_rate = params['learning_rate']\n","        training_args.num_train_epochs = params['num_train_epochs']\n","        training_args.per_device_train_batch_size = params['per_device_train_batch_size']\n","        training_args.per_device_eval_batch_size = params['per_device_eval_batch_size']\n","\n","        # Run training\n","        current_metric = main(training_args)\n","\n","        # Check if this configuration is the best so far\n","        if current_metric \u003e best_metric:\n","            best_metric = current_metric\n","            best_params = params\n","\n","    print(\"Best hyperparameters:\", best_params)\n","    print(\"Best macro F1:\", best_metric)\n"]},{"cell_type":"markdown","metadata":{"id":"Xv8JE2qedxwI"},"source":["# New section"]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":430,"status":"ok","timestamp":1690685692787,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"},"user_tz":-60},"id":"iwRd_Hb-uAA8"},"outputs":[],"source":["incorrect_indices = []\n","with open('incorrect_predictions.txt', 'r') as f:\n","    for line in f:\n","        if line.startswith(\"Index:\"):\n","            incorrect_indices.append(int(line.strip().split(\": \")[1]))\n","\n","# Read all predictions from 'predictions.txt'\n","predictions_file = \"predictions.txt\"\n","with open(predictions_file, \"r\") as f:\n","    predictions = [line.strip().split(\": \")[1] for line in f if line.startswith(\"Predictions\")]\n","\n","# Filter out the incorrect predictions based on indices\n","incorrect_predictions = [prediction for index, prediction in enumerate(predictions) if index in incorrect_indices]\n","\n","# Save the incorrect predictions to a new file 'incorrect_only_predictions.txt'\n","with open(\"incorrect_only_predictions.txt\", \"w\") as f:\n","    for pred in incorrect_predictions:\n","        f.write(f\"Predictions: {pred}\\n\")"]},{"cell_type":"code","execution_count":63,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":368},"executionInfo":{"elapsed":449,"status":"error","timestamp":1690688413245,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"},"user_tz":-60},"id":"Nita_eW-wRWQ","outputId":"63e61c9c-8d65-4522-90e1-a7d4ee298b8c"},"outputs":[{"ename":"ValueError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-63-361ce84088b8\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 10\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predictions:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mpredictions_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\": \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 18\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mpredictions_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m\u003cipython-input-63-361ce84088b8\u003e\u001b[0m in \u001b[0;36m\u003clistcomp\u003e\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predictions:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mpredictions_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\": \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 18\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mpredictions_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '[0'"]}],"source":["import pandas as pd\n","import re\n","import ast\n","\n","# Read the incorrect_predictions.txt file and create a DataFrame\n","df = pd.read_csv(\"incorrect_predictions.txt\", delimiter=\"\\t\", names=[\"Index\", \"Input_Text\", \"Input_IDs\"])\n","\n","# Read predictions from predictions.txt and store them in a dictionary\n","predictions_dict = {}\n","with open(\"incorrect_only_predictions.txt\", \"r\") as f:\n","    lines = f.readlines()\n","    for line in lines:\n","        if line.startswith(\"Index:\"):\n","            current_index = int(line.strip().split(\": \")[1])\n","            predictions_dict[current_index] = []\n","        elif line.startswith(\"Predictions:\"):\n","            predictions_str = line.strip().split(\": \")[1]\n","            predictions = [int(val) for val in predictions_str.split()]\n","            predictions_dict[current_index].extend(predictions)\n","\n","# Create lists to store the extracted data\n","indexes = []\n","input_texts = []\n","input_ids = []\n","predictions_list = []\n","\n","# Populate the lists with the data from the DataFrame and predictions dictionary\n","for index, row in df.iterrows():\n","    current_index = row[\"Index\"]\n","    indexes.append(current_index)\n","    input_texts.append(row[\"Input_Text\"])\n","    input_ids.append(row[\"Input_IDs\"])\n","    predictions_list.append(predictions_dict.get(current_index, []))\n","\n","# Create a new DataFrame with the extracted data\n","data = {\"Index\": indexes, \"Input_Text\": input_texts, \"Input_IDs\": input_ids, \"Predictions\": predictions_list}\n","df_repeated = pd.DataFrame(data)\n","\n","# Save the modified DataFrame back to the file\n","df_repeated.to_csv(\"incorrect_predictions_with_predictions.txt\", sep=\"\\t\", index=False)\n"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":242},"executionInfo":{"elapsed":402,"status":"error","timestamp":1690685740678,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"},"user_tz":-60},"id":"hLt3v_YfR-6P","outputId":"7dfe458e-8300-4aae-8376-81ea9d91f0c1"},"outputs":[{"ename":"ValueError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-39-6adfa47559e6\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 16\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Check if the number of predictions matches the number of rows in the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 17\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of predictions does not match the number of rows in the dataset.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Remove the last prediction as it might be an extra one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Number of predictions does not match the number of rows in the dataset."]}],"source":["\n","\n","def analyze_incorrect_predictions(incorrect_predictions_file):\n","    with open(incorrect_predictions_file, \"r\") as f:\n","        lines = f.readlines()\n","\n","    incorrect_predictions = []\n","    current_example = {}\n","    for line in lines:\n","        line = line.strip()  # Remove leading/trailing whitespaces\n","        if line.startswith(\"Index: \"):\n","            if current_example:\n","                incorrect_predictions.append(current_example)\n","                current_example = {}\n","            current_example[\"Index\"] = int(line.split(\": \")[1])\n","        elif line.startswith(\"Input Text: \"):\n","            current_example[\"Input Text\"] = line.split(\": \")[1]\n","        elif line.startswith(\"Predictions: \"):\n","            current_example[\"Predictions\"] = list(map(int, line.split(\": \")[1].split()))\n","        elif line.startswith(\"Labels: \"):\n","            current_example[\"Labels\"] = list(map(int, line.split(\": \")[1].split()))\n","\n","    # Add the last example\n","    if current_example:\n","        incorrect_predictions.append(current_example)\n","\n","    return incorrect_predictions\n","\n","incorrect_predictions_file = \"incorrect_predictions.txt\"\n","incorrect_predictions_data = analyze_incorrect_predictions(incorrect_predictions_file)\n","\n","# Example analysis\n","for example in incorrect_predictions_data:\n","    print(\"Index:\", example[\"Index\"])\n","    print(\"Input Text:\", example[\"Input Text\"])\n","    if \"Predictions\" in example:\n","        print(\"Predictions:\", example[\"Predictions\"])\n","    else:\n","        print(\"No Predictions\")\n","    if \"Labels\" in example:\n","        print(\"Labels:\", example[\"Labels\"])\n","    else:\n","        print(\"No Labels\")\n","    print()\n"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":423,"status":"ok","timestamp":1690681401415,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"},"user_tz":-60},"id":"GjA-kOF5bFc1","outputId":"b17fe576-faf6-4b54-a63b-900a1c87d2df"},"outputs":[{"name":"stdout","output_type":"stream","text":["Top 5 Unpredicted Labels:\n"]}],"source":["def analyze_incorrect_predictions(incorrect_predictions_file):\n","    with open(incorrect_predictions_file, \"r\") as f:\n","        lines = f.readlines()\n","\n","    incorrect_predictions = []\n","    current_example = {}\n","    for line in lines:\n","        line = line.strip()  # Remove leading/trailing whitespaces\n","        if line.startswith(\"Index: \"):\n","            if current_example:\n","                incorrect_predictions.append(current_example)\n","                current_example = {}\n","            current_example[\"Index\"] = int(line.split(\": \")[1])\n","        elif line.startswith(\"Input Text: \"):\n","            current_example[\"Input Text\"] = line.split(\": \")[1]\n","        elif line.startswith(\"Predictions: \"):\n","            current_example[\"Predictions\"] = list(map(int, line.split(\": \")[1].split()))\n","        elif line.startswith(\"Labels: \"):\n","            current_example[\"Labels\"] = list(map(int, line.split(\": \")[1].split()))\n","\n","    # Add the last example\n","    if current_example:\n","        incorrect_predictions.append(current_example)\n","\n","    return incorrect_predictions\n","\n","incorrect_predictions_file = \"incorrect_predictions.txt\"\n","incorrect_predictions_data = analyze_incorrect_predictions(incorrect_predictions_file)\n","\n","# Now you can use the incorrect_predictions_data for analysis and printing as before\n","\n","\n","\n","\n","def analyze_unpredicted_labels(incorrect_predictions_data):\n","    # Create a dictionary to store the count of each label being predicted as 1 or 0\n","    label_counts = {}\n","\n","    for example in incorrect_predictions_data:\n","        if \"Predictions\" in example and \"Labels\" in example:\n","            predictions = example[\"Predictions\"]\n","            labels = example[\"Labels\"]\n","\n","            for label, prediction in zip(labels, predictions):\n","                if label not in label_counts:\n","                    label_counts[label] = {\"Predicted_1\": 0, \"Predicted_0\": 0}\n","\n","                if prediction == 1:\n","                    label_counts[label][\"Predicted_1\"] += 1\n","                else:\n","                    label_counts[label][\"Predicted_0\"] += 1\n","\n","    return label_counts\n","\n","# Assuming you have already loaded \"incorrect_predictions_data\" from the file or function\n","label_counts = analyze_unpredicted_labels(incorrect_predictions_data)\n","\n","# Sort the labels by the difference in count between Predicted_1 and Predicted_0\n","sorted_labels = sorted(label_counts.keys(), key=lambda label: abs(label_counts[label][\"Predicted_1\"] - label_counts[label][\"Predicted_0\"]), reverse=True)\n","\n","# Print the top 5 unpredicted labels and their corresponding words\n","print(\"Top 5 Unpredicted Labels:\")\n","for label in sorted_labels[:5]:\n","    count_1 = label_counts[label][\"Predicted_1\"]\n","    count_0 = label_counts[label][\"Predicted_0\"]\n","    total_count = count_1 + count_0\n","    print(f\"Label: {label}, Predicted as 1: {count_1}, Predicted as 0: {count_0}, Total Examples: {total_count}\")\n"]},{"cell_type":"markdown","metadata":{"id":"pEiITtEjaTKP"},"source":["# New section"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":484},"executionInfo":{"elapsed":11516,"status":"error","timestamp":1690595430232,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"},"user_tz":-60},"id":"EGT4Uf8aPc83","outputId":"6c001ea3-6f03-4fba-a5d2-adad237c0e43"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n","[WARNING|modeling_utils.py:3331] 2023-07-29 01:50:29,076 \u003e\u003e Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"ename":"ValueError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-32-4cbd3fbdc55d\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 445\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0mmetric_for_best_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"macro-f1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     )\n\u001b[0;32m--\u003e 466\u001b[0;31m    \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m\u003cipython-input-32-4cbd3fbdc55d\u003e\u001b[0m in \u001b[0;36mmain\u001b[0;34m(training_args)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlast_checkpoint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 390\u001b[0;31m         \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         max_train_samples = (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m         )\n\u001b[0;32m-\u003e 1539\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1540\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1809\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 2654\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m\u003e\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2677\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2678\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 2679\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2680\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1228\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"single_label_classification\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1230\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1231\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multi_label_classification\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u003e\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1174\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                label_smoothing=self.label_smoothing)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 3029\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Expected input batch_size (32) to match target batch_size (256)."]}],"source":["#!/usr/bin/env python\n","# coding=utf-8\n","\"\"\" Finetuning models on UNFAIR-ToC (e.g. Bert, RoBERTa, LEGAL-BERT).\"\"\"\n","import pandas as pd\n","import logging\n","import os\n","import random\n","import sys\n","from dataclasses import dataclass, field\n","from typing import Optional\n","\n","import datasets\n","from datasets import load_dataset\n","from sklearn.metrics import f1_score\n","#from trainer import MultilabelTrainer\n","from scipy.special import expit\n","import glob\n","import shutil\n","import numpy as np\n","import csv\n","\n","import transformers\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    DataCollatorWithPadding,\n","    EvalPrediction,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    default_data_collator,\n","    set_seed,\n","    EarlyStoppingCallback,\n",")\n","from transformers.trainer_utils import get_last_checkpoint\n","from transformers.utils import check_min_version\n","from transformers.utils.versions import require_version\n","\n","\n","# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n","check_min_version(\"4.9.0\")\n","\n","require_version(\"datasets\u003e=1.8.0\", \"To fix: pip install -r examples/pytorch/text-classification/requirements.txt\")\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","@dataclass\n","class DataTrainingArguments:\n","    \"\"\"\n","    Arguments pertaining to what data we are going to input our model for training and eval.\n","\n","    Using `HfArgumentParser` we can turn this class\n","    into argparse arguments to be able to specify them on\n","    the command line.\n","    \"\"\"\n","\n","    max_seq_length: Optional[int] = field(\n","        default=128,\n","        metadata={\n","            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n","            \"than this will be truncated, sequences shorter will be padded.\"\n","        },\n","    )\n","    overwrite_cache: bool = field(\n","        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n","    )\n","    pad_to_max_length: bool = field(\n","        default=True,\n","        metadata={\n","            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n","            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n","        },\n","    )\n","    max_train_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_eval_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_predict_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    server_ip: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n","    server_port: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n","\n","\n","@dataclass\n","class ModelArguments:\n","    \"\"\"\n","    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n","    \"\"\"\n","\n","    model_name_or_path: str = field(\n","        default=None, metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n","    )\n","    config_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n","    )\n","    tokenizer_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n","    )\n","    cache_dir: Optional[str] = field(\n","        default=None,\n","        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n","    )\n","    do_lower_case: Optional[bool] = field(\n","        default=True,\n","        metadata={\"help\": \"arg to indicate if tokenizer should do lower case in AutoTokenizer.from_pretrained()\"},\n","    )\n","    use_fast_tokenizer: bool = field(\n","        default=True,\n","        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n","    )\n","    model_revision: str = field(\n","        default=\"main\",\n","        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n","    )\n","    use_auth_token: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n","            \"with private models).\"\n","        },\n","    )\n","\n","\n","def main(training_args):\n","    # See all possible arguments in src/transformers/training_args.py\n","    # or by passing the --help flag to this script.\n","    # We now keep distinct sets of args, for a cleaner separation of concerns.\n","\n","    model_args = ModelArguments(\n","        model_name_or_path=\"roberta-base\",\n","        #hierarchical=True,\n","        do_lower_case=True,\n","        use_fast_tokenizer=True,\n","    )\n","    data_args = DataTrainingArguments(\n","        max_seq_length=128,\n","        #max_segments=64,\n","        #max_seg_length=128,\n","        overwrite_cache=False,\n","        pad_to_max_length=True,\n","    )\n","\n","    # Setup distant debugging if needed\n","    if data_args.server_ip and data_args.server_port:\n","        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n","        import ptvsd\n","\n","        print(\"Waiting for debugger attach\")\n","        ptvsd.enable_attach(address=(data_args.server_ip, data_args.server_port), redirect_output=True)\n","        ptvsd.wait_for_attach()\n","\n","    # Fix boolean parameter\n","    if model_args.do_lower_case == 'False' or not model_args.do_lower_case:\n","        model_args.do_lower_case = False\n","        'Tokenizer do_lower_case False'\n","    else:\n","        model_args.do_lower_case = True\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        handlers=[logging.StreamHandler(sys.stdout)],\n","    )\n","\n","    log_level = training_args.get_process_log_level()\n","    logger.setLevel(log_level)\n","    datasets.utils.logging.set_verbosity(log_level)\n","    transformers.utils.logging.set_verbosity(log_level)\n","    transformers.utils.logging.enable_default_handler()\n","    transformers.utils.logging.enable_explicit_format()\n","\n","    # Log on each process the small summary:\n","    logger.warning(\n","        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n","        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n","    )\n","    logger.info(f\"Training/evaluation parameters {training_args}\")\n","\n","    # Detecting last checkpoint.\n","    last_checkpoint = None\n","    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n","        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n","        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) \u003e 0:\n","            raise ValueError(\n","                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n","                \"Use --overwrite_output_dir to overcome.\"\n","            )\n","        elif last_checkpoint is not None:\n","            logger.info(\n","                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n","                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n","            )\n","\n","    # Set seed before initializing model.\n","    set_seed(training_args.seed)\n","\n","    # In distributed training, the load_dataset function guarantees that only one local process can concurrently\n","    # download the dataset.\n","    # Downloading and loading eurlex dataset from the hub.\n","    if training_args.do_train:\n","        train_dataset = load_dataset(\"lex_glue\", \"unfair_tos\", split=\"train\", data_dir='data', cache_dir=model_args.cache_dir)\n","\n","    if training_args.do_eval:\n","        eval_dataset = load_dataset(\"lex_glue\", \"unfair_tos\", split=\"validation\", data_dir='data', cache_dir=model_args.cache_dir)\n","\n","    if training_args.do_predict:\n","        predict_dataset = load_dataset(\"lex_glue\", \"unfair_tos\", split=\"test\", data_dir='data', cache_dir=model_args.cache_dir)\n","\n","    # Labels\n","    label_list = list(range(8))\n","    num_labels = len(label_list)\n","\n","    # Load pretrained model and tokenizer\n","    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n","    # download model \u0026 vocab.\n","    config = AutoConfig.from_pretrained(\n","        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n","        num_labels=num_labels,\n","        finetuning_task=\"unfair_toc\",\n","        cache_dir=model_args.cache_dir,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","\n","    if config.model_type == 'big_bird':\n","        config.attention_type = 'original_full'\n","\n","    if config.model_type == 'longformer':\n","        config.attention_window = [128] * config.num_hidden_layers\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n","        do_lower_case=model_args.do_lower_case,\n","        cache_dir=model_args.cache_dir,\n","        use_fast=model_args.use_fast_tokenizer,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        model_args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","        config=config,\n","        cache_dir=model_args.cache_dir,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","\n","    # Preprocessing the datasets\n","    # Padding strategy\n","    if data_args.pad_to_max_length:\n","        padding = \"max_length\"\n","    else:\n","        # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n","        padding = False\n","\n","    def preprocess_function(examples):\n","        # Tokenize the texts\n","        batch = tokenizer(\n","            examples[\"text\"],\n","            padding=padding,\n","            max_length=data_args.max_seq_length,\n","            truncation=True,\n","        )\n","        batch[\"labels\"] = [[1 if label in labels else 0 for label in label_list] for labels in\n","                              examples[\"labels\"]]\n","\n","        return batch\n","\n","    if training_args.do_train:\n","        if data_args.max_train_samples is not None:\n","            train_dataset = train_dataset.select(range(data_args.max_train_samples))\n","        with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n","            train_dataset = train_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on train dataset\",\n","            )\n","        # Log a few random samples from the training set:\n","        for index in random.sample(range(len(train_dataset)), 3):\n","            logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n","\n","    if training_args.do_eval:\n","        if data_args.max_eval_samples is not None:\n","            eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))\n","        with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n","            eval_dataset = eval_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on validation dataset\",\n","            )\n","\n","    if training_args.do_predict:\n","        if data_args.max_predict_samples is not None:\n","            predict_dataset = predict_dataset.select(range(data_args.max_predict_samples))\n","        with training_args.main_process_first(desc=\"prediction dataset map pre-processing\"):\n","            predict_dataset = predict_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on prediction dataset\",\n","            )\n","\n","    # You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n","    # predictions and label_ids field) and has to return a dictionary string to float.\n","    def compute_metrics(p: EvalPrediction):\n","        # Fix gold labels\n","        y_true = np.zeros((p.label_ids.shape[0], p.label_ids.shape[1] + 1), dtype=np.int32)\n","        y_true[:, :-1] = p.label_ids\n","        y_true[:, -1] = (np.sum(p.label_ids, axis=1) == 0).astype('int32')\n","        # Fix predictions\n","        logits = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","        preds = (expit(logits) \u003e 0.5).astype('int32')\n","        y_pred = np.zeros((p.label_ids.shape[0], p.label_ids.shape[1] + 1), dtype=np.int32)\n","        y_pred[:, :-1] = preds\n","        y_pred[:, -1] = (np.sum(preds, axis=1) == 0).astype('int32')\n","        # Compute scores\n","        macro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='macro', zero_division=0)\n","        micro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='micro', zero_division=0)\n","        return {'macro-f1': macro_f1, 'micro-f1': micro_f1}\n","\n","    # Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n","    if data_args.pad_to_max_length:\n","        data_collator = default_data_collator\n","    elif training_args.fp16:\n","        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n","    else:\n","        data_collator = None\n","\n","    # Initialize our Trainer\n","    trainer = MultilabelTrainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset if training_args.do_train else None,\n","        eval_dataset=eval_dataset if training_args.do_eval else None,\n","        compute_metrics=compute_metrics,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n","    )\n","\n","    # Training\n","    if training_args.do_train:\n","        checkpoint = None\n","        if training_args.resume_from_checkpoint is not None:\n","            checkpoint = training_args.resume_from_checkpoint\n","        elif last_checkpoint is not None:\n","            checkpoint = last_checkpoint\n","        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n","        metrics = train_result.metrics\n","        max_train_samples = (\n","            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n","        )\n","        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n","\n","        trainer.save_model()  # Saves the tokenizer too for easy upload\n","\n","        trainer.log_metrics(\"train\", metrics)\n","        trainer.save_metrics(\"train\", metrics)\n","        trainer.save_state()\n","\n","    # Evaluation\n","    if training_args.do_eval:\n","        logger.info(\"*** Evaluate ***\")\n","        metrics = trainer.evaluate(eval_dataset=eval_dataset)\n","\n","        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n","        metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n","\n","        trainer.log_metrics(\"eval\", metrics)\n","        trainer.save_metrics(\"eval\", metrics)\n","\n","    # Prediction\n","    if training_args.do_predict:\n","        logger.info(\"*** Predict ***\")\n","    predictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix=\"predict\")\n","\n","    # Get the actual predicted indexes (class labels)\n","    threshold = 0.5  # Set the threshold for probability values to be considered as positive predictions\n","    actual_predictions = (predictions \u003e threshold).astype(int)  # Convert probabilities to binary predictions\n","\n","    # Get the input text from the dataset\n","    input_texts = predict_dataset['text']\n","\n","    # Create a DataFrame to hold the predictions, labels, and input text\n","    with open(\"predictions.txt\", \"w\") as f:\n","        for index, input_text, prediction, label in zip(range(len(input_texts)), input_texts, actual_predictions, labels):\n","            f.write(f\"Index: {index}\\n\")\n","            f.write(f\"Input Text: {input_text}\\n\")\n","            f.write(f\"Predictions: {prediction}\\n\")\n","            f.write(f\"Labels: {label}\\n\")\n","            f.write(\"\\n\")\n","\n","    correct_predictions_dataset = predict_dataset.filter(lambda example, idx: all(actual_predictions[idx] == example[\"labels\"]), with_indices=True)\n","\n","# Access the rows from the dataset associated with correct predictions\n","    correct_predictions_rows = correct_predictions_dataset[\"text\"]\n","\n","# Print or use the correct_predictions_rows as needed\n","    print(correct_predictions_rows)\n","\n","    # Clean up checkpoints\n","    checkpoints = [filepath for filepath in glob.glob(f'{training_args.output_dir}/*/') if '/checkpoint' in filepath]\n","    for checkpoint in checkpoints:\n","        shutil.rmtree(checkpoint)\n","\n","\n","if __name__ == \"__main__\":\n","    training_args = TrainingArguments(\n","        do_train = True,\n","        do_eval = True,\n","        do_predict = True,\n","        output_dir=os.getcwd(),\n","        overwrite_output_dir=True,\n","        num_train_epochs=2,\n","        per_device_train_batch_size=32,\n","        per_device_eval_batch_size=32,\n","        save_steps=500,\n","        save_total_limit=2,\n","        fp16=False,\n","        logging_dir=\"./logs\",\n","        logging_steps=200,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=500,\n","        logging_first_step=False,\n","        load_best_model_at_end = True,\n","        metric_for_best_model=\"macro-f1\",\n","    )\n","    main(training_args)\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMCSmYQj/RYX83lSG0LWSyB","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"013fcd89af4141dd8a2e7e3f12917446":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2fec99ac144c4e419d23b2558bd0f804","placeholder":"​","style":"IPY_MODEL_5ec0b7c2154244e3b1872ef73fb41d36","value":" 5532/5532 [00:00\u0026lt;00:00, 11638.28 examples/s]"}},"029f59e8e7a8441daee665e2238355bf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"032a1b4ee9314d8299efea86fa7f178e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03fddcc558864984850e146285b66bc2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09e5278974a549328e6ec54929aed6dd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5113ca21ed8a4b6e81887eed0c5291bf","IPY_MODEL_3a4b894fa4b948849f86991cbce32eb2","IPY_MODEL_a17d46ebd02a4640b27b8035959afea9"],"layout":"IPY_MODEL_92d9da1d5a12456687f77b0b184d8588"}},"09ec5e12deff4e3992b9df19e0325a60":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_998668785c684f9383622fd9012cdb97","placeholder":"​","style":"IPY_MODEL_5d431fe31c6944aa894640c9e2e2e86b","value":" 5532/5532 [00:00\u0026lt;00:00, 11766.34 examples/s]"}},"0a94e51ed3994bef87747976062ecf97":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a8af1f4ac3c4938b1ee111f066dd316","placeholder":"​","style":"IPY_MODEL_5488becff92a4b99a64610583d1b5bd6","value":"Downloading (…)lve/main/config.json: 100%"}},"0da9ff408d11457bb41668d446af0050":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed4ff56ec9b0489eb2c7742a70e8b4c4","placeholder":"​","style":"IPY_MODEL_b482bdbe95a0474ea1d01f867801f753","value":"Downloading model.safetensors: 100%"}},"0fab79d6878f4634b688d14caa9a0d05":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0fb376d8669e47e380e9caf44549d8ed":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10a8ef3a52bb4aa1bcf8c0ed2c783200":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"11c8be94df5045ad9778c5de766d2aea":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a86128d20024e2290e36cb2132ea9fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0da9ff408d11457bb41668d446af0050","IPY_MODEL_33a3a26ca2664cc0b63ef83f30c53df2","IPY_MODEL_2f906dcd9f4042398739b6a2fcbd1df3"],"layout":"IPY_MODEL_029f59e8e7a8441daee665e2238355bf"}},"1e8dcd3e9dba43d087109db0346b9c04":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1f36f4cf3c1f43a99d527f3b7d1887eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae46ffb44da04cf18eb733cb339666e0","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8d77f36f9bc94dd99c2cc5f0dc06f941","value":466062}},"2f906dcd9f4042398739b6a2fcbd1df3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c223b98d8da4444eabc51cf159ec7182","placeholder":"​","style":"IPY_MODEL_c80429f834ca441b81a7cd4e19543812","value":" 440M/440M [00:00\u0026lt;00:00, 575MB/s]"}},"2fec99ac144c4e419d23b2558bd0f804":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33a3a26ca2664cc0b63ef83f30c53df2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eba17b0f915e4421a74d8b27c5f6b26d","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8d0707264bea463ab22536c90def1c15","value":440449768}},"39a9fa4ef97c424ba79fa3c4668bc321":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_734707e1987b4fa084bd8931cce90600","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6ec6d863c9dc43e9aea472f074cb6980","value":570}},"3a4b894fa4b948849f86991cbce32eb2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a22be71cff69488da7ff1d86634b8750","max":2275,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f7171acc7b3e408999d984876f397077","value":2275}},"3badc2afed8f4f9290a340da363b5640":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a09700d2cb9f4bc294d8fed82be5be35","placeholder":"​","style":"IPY_MODEL_0fab79d6878f4634b688d14caa9a0d05","value":" 232k/232k [00:00\u0026lt;00:00, 14.1MB/s]"}},"3fcbca63ff0b4855a303b47a43012f59":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e3514d3a052433982b1ff574c2a4a98","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_be137a9bd9354caa92716af49045278c","value":28}},"423148d25b164c2b993b10094136edd5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b12d25f68deb4d9dbd7472fb2f5c6783","placeholder":"​","style":"IPY_MODEL_a93a2c2b11ad435ab7b1d5f7dbdd0411","value":" 570/570 [00:00\u0026lt;00:00, 47.2kB/s]"}},"46768c19d7fa4a589dbe3bf3f50bdebb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"48518cac133d47fea576846dc365f2e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ca8c368134042e4908d923bd04e629b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca3984f2759a46a1a5523dd8854a57f6","placeholder":"​","style":"IPY_MODEL_48518cac133d47fea576846dc365f2e7","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"4ea9f6285fac4c97894659683e7c8d90":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f837df47c9b4523b10075894cb17f2e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fa184743d85450ea4fb784c577a4dac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5113ca21ed8a4b6e81887eed0c5291bf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c8edc38046d47af8550010d60947345","placeholder":"​","style":"IPY_MODEL_10a8ef3a52bb4aa1bcf8c0ed2c783200","value":"Running tokenizer on validation dataset: 100%"}},"5488becff92a4b99a64610583d1b5bd6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"549756494abe4030b33879671892cec7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_62fdec7f1f5245dda5cb03e6bb978afd","placeholder":"​","style":"IPY_MODEL_687724d36475407f844382ce0531b665","value":"Running tokenizer on train dataset: 100%"}},"59230e94aec740789e13407c12601684":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0a94e51ed3994bef87747976062ecf97","IPY_MODEL_39a9fa4ef97c424ba79fa3c4668bc321","IPY_MODEL_423148d25b164c2b993b10094136edd5"],"layout":"IPY_MODEL_90d22e347e2348dfa718ee36ed1f4047"}},"5c8edc38046d47af8550010d60947345":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d431fe31c6944aa894640c9e2e2e86b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5e799a00394546bbbf15ff2fbf6361a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ec0b7c2154244e3b1872ef73fb41d36":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"60ee65a9b68043b5b522aeb3c8fb9b1a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e6ea716498324618ad5946c273ed9a1e","IPY_MODEL_1f36f4cf3c1f43a99d527f3b7d1887eb","IPY_MODEL_f1747469e9cb431da7316317320ebdc4"],"layout":"IPY_MODEL_03fddcc558864984850e146285b66bc2"}},"61d962e342b84f8d88db2e3cc32d2751":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62fdec7f1f5245dda5cb03e6bb978afd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6705312309b04223a9e9087dd4b42c0c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"687724d36475407f844382ce0531b665":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ec6d863c9dc43e9aea472f074cb6980":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"734707e1987b4fa084bd8931cce90600":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"759a3984eda74fdfb6fa272b8cff1ee2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"797e52a0dd1940f28043cb086a740dd4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79dfa54c0b4c46cdb8e5bbc563c35a6b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8275e092d82e47c798a1d3e603d23c6d","placeholder":"​","style":"IPY_MODEL_d10a34b13da244d496d8c6ac34163550","value":" 1607/1607 [00:00\u0026lt;00:00, 10785.62 examples/s]"}},"7a8af1f4ac3c4938b1ee111f066dd316":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b3757ad2da24ac092fcebaf4b2902eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7e3514d3a052433982b1ff574c2a4a98":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8275e092d82e47c798a1d3e603d23c6d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85b1756ff0c54e65a69295e0a603a5d4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d0707264bea463ab22536c90def1c15":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8d77f36f9bc94dd99c2cc5f0dc06f941":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8e9564a4b1fe4c258278f29f606545a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ceafd83aa9454bc4a060b6c2ca9470ef","IPY_MODEL_ae26b51429d44fb29297e9eef3e4a390","IPY_MODEL_09ec5e12deff4e3992b9df19e0325a60"],"layout":"IPY_MODEL_759a3984eda74fdfb6fa272b8cff1ee2"}},"90d22e347e2348dfa718ee36ed1f4047":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92d9da1d5a12456687f77b0b184d8588":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96856a1fb3be4939826ed16d0968cde9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"984dc4b589ba46ff9adb4d3f7831eaf2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2b5d76f6fbf4ae9b8dc4c6fa5bceef2","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_46768c19d7fa4a589dbe3bf3f50bdebb","value":231508}},"990d5a5e54d64c729b1a461c5f134843":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f837df47c9b4523b10075894cb17f2e","max":1607,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ffceeaba0d2841e087dadfbcaa52b85f","value":1607}},"998668785c684f9383622fd9012cdb97":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9da2b17c9f754ab48c59ee9e479e20a1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f341317beab45fe8606c1a0bcdf97f3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a09700d2cb9f4bc294d8fed82be5be35":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a17d46ebd02a4640b27b8035959afea9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_797e52a0dd1940f28043cb086a740dd4","placeholder":"​","style":"IPY_MODEL_d92f3c1e76034d6dbe870169bd9a3db0","value":" 2275/2275 [00:00\u0026lt;00:00, 13900.18 examples/s]"}},"a22be71cff69488da7ff1d86634b8750":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a93a2c2b11ad435ab7b1d5f7dbdd0411":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ae26b51429d44fb29297e9eef3e4a390":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e09fdf39088b4a4b90a685f0f8ee7b18","max":5532,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1e8dcd3e9dba43d087109db0346b9c04","value":5532}},"ae46ffb44da04cf18eb733cb339666e0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aebdde5429ce4dc29fb0875de98107b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bfcf389494bb4ec2b94132414071eb58","IPY_MODEL_990d5a5e54d64c729b1a461c5f134843","IPY_MODEL_79dfa54c0b4c46cdb8e5bbc563c35a6b"],"layout":"IPY_MODEL_85b1756ff0c54e65a69295e0a603a5d4"}},"b12d25f68deb4d9dbd7472fb2f5c6783":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b482bdbe95a0474ea1d01f867801f753":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bd6502f8ce544cfeb429e2991897166e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_61d962e342b84f8d88db2e3cc32d2751","placeholder":"​","style":"IPY_MODEL_f5adfbdbda4d4b92ad074b1b795924af","value":" 28.0/28.0 [00:00\u0026lt;00:00, 2.20kB/s]"}},"be137a9bd9354caa92716af49045278c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"beaaee19612b4306943acea7b6481f46":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_032a1b4ee9314d8299efea86fa7f178e","max":5532,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7b3757ad2da24ac092fcebaf4b2902eb","value":5532}},"bfcf389494bb4ec2b94132414071eb58":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9da2b17c9f754ab48c59ee9e479e20a1","placeholder":"​","style":"IPY_MODEL_5e799a00394546bbbf15ff2fbf6361a7","value":"Running tokenizer on prediction dataset: 100%"}},"c223b98d8da4444eabc51cf159ec7182":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c57fda0b1b874c24bb731f4b0c3a8b36":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c58af834d3504296ae3e1ec514e93520","IPY_MODEL_3fcbca63ff0b4855a303b47a43012f59","IPY_MODEL_bd6502f8ce544cfeb429e2991897166e"],"layout":"IPY_MODEL_f4814ac54215417e92fb8267871e9614"}},"c58af834d3504296ae3e1ec514e93520":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfb1afaba41e4d119630a732c3697303","placeholder":"​","style":"IPY_MODEL_96856a1fb3be4939826ed16d0968cde9","value":"Downloading (…)okenizer_config.json: 100%"}},"c5f2d684dc0945979d8935577ba6b816":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4ca8c368134042e4908d923bd04e629b","IPY_MODEL_984dc4b589ba46ff9adb4d3f7831eaf2","IPY_MODEL_3badc2afed8f4f9290a340da363b5640"],"layout":"IPY_MODEL_6705312309b04223a9e9087dd4b42c0c"}},"c733d77bebd7459a80b5a5dab50c7064":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c80429f834ca441b81a7cd4e19543812":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ca3984f2759a46a1a5523dd8854a57f6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ceafd83aa9454bc4a060b6c2ca9470ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7027ecbb3944d38b8a77abf071b6ac3","placeholder":"​","style":"IPY_MODEL_9f341317beab45fe8606c1a0bcdf97f3","value":"Running tokenizer on train dataset: 100%"}},"d10a34b13da244d496d8c6ac34163550":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d92f3c1e76034d6dbe870169bd9a3db0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dfb1afaba41e4d119630a732c3697303":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e09fdf39088b4a4b90a685f0f8ee7b18":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6ccd16827ce4ec898e02e5335dbdbc8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_549756494abe4030b33879671892cec7","IPY_MODEL_beaaee19612b4306943acea7b6481f46","IPY_MODEL_013fcd89af4141dd8a2e7e3f12917446"],"layout":"IPY_MODEL_0fb376d8669e47e380e9caf44549d8ed"}},"e6ea716498324618ad5946c273ed9a1e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ea9f6285fac4c97894659683e7c8d90","placeholder":"​","style":"IPY_MODEL_c733d77bebd7459a80b5a5dab50c7064","value":"Downloading (…)/main/tokenizer.json: 100%"}},"e7027ecbb3944d38b8a77abf071b6ac3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eba17b0f915e4421a74d8b27c5f6b26d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed4ff56ec9b0489eb2c7742a70e8b4c4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1747469e9cb431da7316317320ebdc4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_11c8be94df5045ad9778c5de766d2aea","placeholder":"​","style":"IPY_MODEL_4fa184743d85450ea4fb784c577a4dac","value":" 466k/466k [00:00\u0026lt;00:00, 34.6MB/s]"}},"f2b5d76f6fbf4ae9b8dc4c6fa5bceef2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4814ac54215417e92fb8267871e9614":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5adfbdbda4d4b92ad074b1b795924af":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7171acc7b3e408999d984876f397077":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ffceeaba0d2841e087dadfbcaa52b85f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}