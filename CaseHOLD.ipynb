{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyMtW9XpCR9oownaD0E8iw6E"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"ae72a1ce74a54d80b30774f9d858e2d8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a12807eb3f1b4dbd8f952ffbb94f5576","IPY_MODEL_b09bd26d1a1e476a82f338b63c045d8a","IPY_MODEL_ff2cc63c390b450582ab15af099a9cd8"],"layout":"IPY_MODEL_2ec99028b3174a32bfa8c3f6e6f4b167"}},"a12807eb3f1b4dbd8f952ffbb94f5576":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eabe6f0f988b479a98ef2cbca97c9229","placeholder":"​","style":"IPY_MODEL_c2a28971e2434125a1bcf7e3f4027832","value":"Downloading (…)okenizer_config.json: 100%"}},"b09bd26d1a1e476a82f338b63c045d8a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8fdd3d176f242de917fd2bcfaa42c85","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0d2aa40de64b4dbd84dd063b915df802","value":28}},"ff2cc63c390b450582ab15af099a9cd8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e418dbf101e944e5b14e4f04c136f0f8","placeholder":"​","style":"IPY_MODEL_84de5eb0decc495799c030eb5e0bbc6a","value":" 28.0/28.0 [00:00&lt;00:00, 2.30kB/s]"}},"2ec99028b3174a32bfa8c3f6e6f4b167":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eabe6f0f988b479a98ef2cbca97c9229":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2a28971e2434125a1bcf7e3f4027832":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a8fdd3d176f242de917fd2bcfaa42c85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d2aa40de64b4dbd84dd063b915df802":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e418dbf101e944e5b14e4f04c136f0f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84de5eb0decc495799c030eb5e0bbc6a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f157cacc24124e44a14b6e03fd14acfe":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_844810d9577840d797cf7392ef5a5b78","IPY_MODEL_95abd7f154be4eb89c7d787b8402e156","IPY_MODEL_638361bc7b1d47a5b9daf752c864be41"],"layout":"IPY_MODEL_d219943071574aeb92dcf88d43dd6d74"}},"844810d9577840d797cf7392ef5a5b78":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c6ecb3c8c514eff822d4227edf467cf","placeholder":"​","style":"IPY_MODEL_5e2b82d321314ed2aedbb3b75f315d0c","value":"Downloading (…)lve/main/config.json: 100%"}},"95abd7f154be4eb89c7d787b8402e156":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5419157828846d18d4eca914d81656e","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a324cb8d913a4899bb5fb64b865bfbbe","value":570}},"638361bc7b1d47a5b9daf752c864be41":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1e5f8dbe13147959c963c154001a34a","placeholder":"​","style":"IPY_MODEL_994a247f75f54d748ae0cbdb88982e9b","value":" 570/570 [00:00&lt;00:00, 39.6kB/s]"}},"d219943071574aeb92dcf88d43dd6d74":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c6ecb3c8c514eff822d4227edf467cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e2b82d321314ed2aedbb3b75f315d0c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5419157828846d18d4eca914d81656e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a324cb8d913a4899bb5fb64b865bfbbe":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e1e5f8dbe13147959c963c154001a34a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"994a247f75f54d748ae0cbdb88982e9b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f31151e42454cb98d4375ab76ad8d7f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d765bdc40edf498dbc2bd1e923eef217","IPY_MODEL_25ae1c6dbada44bfbfc663cbc0e23cf1","IPY_MODEL_96bdaf70586740ad85e9cb1d8dd86787"],"layout":"IPY_MODEL_3bf0f5e80a6041d2b81ac1f9f8912eb0"}},"d765bdc40edf498dbc2bd1e923eef217":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f4658fd27e647a4ba561d34c5c00194","placeholder":"​","style":"IPY_MODEL_4ce98c3aa063423591fbd7c788815365","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"25ae1c6dbada44bfbfc663cbc0e23cf1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6406fccc650f46d19cd18e6118b9d8c5","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ce10211bccd948f69260821f7e7e6c0f","value":231508}},"96bdaf70586740ad85e9cb1d8dd86787":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_080fc23c3cd1444dbb5eed08eb17f9d3","placeholder":"​","style":"IPY_MODEL_8716d30567d74b029decd99ebd0f522e","value":" 232k/232k [00:00&lt;00:00, 13.1MB/s]"}},"3bf0f5e80a6041d2b81ac1f9f8912eb0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f4658fd27e647a4ba561d34c5c00194":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ce98c3aa063423591fbd7c788815365":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6406fccc650f46d19cd18e6118b9d8c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce10211bccd948f69260821f7e7e6c0f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"080fc23c3cd1444dbb5eed08eb17f9d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8716d30567d74b029decd99ebd0f522e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6b464328ed7c47788dd93c1d637a56f5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f514b16a8d8d46c48ba8c29006e70a25","IPY_MODEL_83d035a4a2f9427e9ad1dad5eb575eef","IPY_MODEL_3bfc7fb54f244e088b3788c6a13b3227"],"layout":"IPY_MODEL_a58336669d1e4ab1b71f9d54caaf0f52"}},"f514b16a8d8d46c48ba8c29006e70a25":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f52fb9f5e25d4c53936186c889d9a616","placeholder":"​","style":"IPY_MODEL_2aa7b3965d254fda93939e7ee73e1bf4","value":"Downloading (…)/main/tokenizer.json: 100%"}},"83d035a4a2f9427e9ad1dad5eb575eef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b68cd75868b4893b9445588e493d5fc","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e2fa504a285c49c1905af9933ac3c575","value":466062}},"3bfc7fb54f244e088b3788c6a13b3227":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_903d3a5d4aad4538b5b715502169fafc","placeholder":"​","style":"IPY_MODEL_28f14d4ef6df40fbbf70fd475d035946","value":" 466k/466k [00:00&lt;00:00, 27.0MB/s]"}},"a58336669d1e4ab1b71f9d54caaf0f52":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f52fb9f5e25d4c53936186c889d9a616":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2aa7b3965d254fda93939e7ee73e1bf4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8b68cd75868b4893b9445588e493d5fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2fa504a285c49c1905af9933ac3c575":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"903d3a5d4aad4538b5b715502169fafc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28f14d4ef6df40fbbf70fd475d035946":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee855712759e462ba1a6e8ae1bcd5c88":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_57c8c568ac7141f399b186524c2597f8","IPY_MODEL_67076958d0dd4607a6cb5089cec58956","IPY_MODEL_12969f4afd674e63843981eadd7ca2d3"],"layout":"IPY_MODEL_3c502e3f0d4543c19c1e193cb56dd40e"}},"57c8c568ac7141f399b186524c2597f8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d92c4e6339df4c05bc2c5610293eb477","placeholder":"​","style":"IPY_MODEL_01ec4651c0c54d18b7293dcbc3778b04","value":"Downloading model.safetensors: 100%"}},"67076958d0dd4607a6cb5089cec58956":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e31549d595f4bb3866d03a0f5ac3b0a","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d24d64116db34550ac82f6e608e66296","value":440449768}},"12969f4afd674e63843981eadd7ca2d3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_763bdf5f44494f9da1d82ffdace41d9a","placeholder":"​","style":"IPY_MODEL_992f22966a384ba586e6678118ea3860","value":" 440M/440M [00:00&lt;00:00, 518MB/s]"}},"3c502e3f0d4543c19c1e193cb56dd40e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d92c4e6339df4c05bc2c5610293eb477":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01ec4651c0c54d18b7293dcbc3778b04":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e31549d595f4bb3866d03a0f5ac3b0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d24d64116db34550ac82f6e608e66296":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"763bdf5f44494f9da1d82ffdace41d9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"992f22966a384ba586e6678118ea3860":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9IJDknlHv4NK","executionInfo":{"status":"ok","timestamp":1692682977516,"user_tz":-60,"elapsed":49616,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"}},"outputId":"861cc302-51a0-4581-b773-a2680c38a0c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Collecting transformers\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n","Collecting datasets\n","  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, dill, multiprocess, datasets\n","Successfully installed datasets-2.14.4 dill-0.3.7 multiprocess-0.70.15 xxhash-3.3.0\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.10.1)\n","Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.23.5)\n","Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.31.0)\n","Collecting accelerate\n","  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.3.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n","Requirement already satisfied: torch!=1.12.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (16.0.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.21.0\n"]}],"source":["! pip install torch\n","! pip install transformers\n","! pip install scikit-learn\n","! pip install tqdm\n","! pip install numpy\n","! pip install datasets\n","! pip install nltk\n","import nltk\n","nltk.download('stopwords')\n","! pip install scipy\n","! pip install transformers[torch] accelerate\n"]},{"cell_type":"code","source":["from datasets import load_dataset\n","dataset_dict = load_dataset(\"lex_glue\", 'case_hold')\n","#print(dataset)\n","#Divide into train,dev,test\n","\n","from sklearn.model_selection import train_test_split\n","\n","#data_list = list(dataset_dict.items())\n","\n","train_set_dict = dataset_dict['test']\n","print(train_set_dict[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3dCvU7rYxvD3","executionInfo":{"status":"ok","timestamp":1692149039863,"user_tz":-60,"elapsed":1340,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"}},"outputId":"33592c56-1194-440a-f764-7e8976abfafe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'context': 'of § 3583(e)(3) was reasonably foreseeable and provided the defendant with a fair warning. Thus, it was not unconstitutional to apply Johnson retroactively. Although Seals is unpublished, and thus not binding, Seals is authoritative and persuasive. Therefore, applying Johnson retroactively to Martinez’s 1993 conviction does not violate the Due Process Clause, and the district court did not plainly err in reimposing supervised release after the first revocation. Accordingly, Martinez’s sentence is affirmed. AFFIRMED; MOTION DISMISSED AS MOOT. 1 . See, e.g., United States v. Golding, 739 F.2d 183, 184 (5th Cir.1984). 2 . Ketchum v. Gulf Oil Corp., 798 F.2d 159, 162 (5th Cir.1986). 3 . See Eberhart v. United States, 546 U.S. 12, 126 S.Ct. 403, 406-07, 163 L.Ed.2d 14 (2005) (per curiam) (<HOLDING>); Kontrick v. Ryan, 540 U.S. 443, 455-56, 124', 'endings': ['holding that the defendants evidence did not qualify as newly discovered evidence', 'holding seventh state petition for postconviction relief which was based on newly discovered evidence but rejected by the state courts because the evidence was not newly discovered was properly filed', 'holding that rules setting forth time limits for a defendants motion for a new trial grounded on a reason other than newly discovered evidence are not jurisdictional but instead are nonjurisdictional claimprocessing rules', 'holding that newly discovered evidence must be that which existed at the time of trial but for an excusable reason was not discoverable until later', 'holding that the district court did not abuse its discretion in denying motion for new trial based on newly discovered evidence where the evidence would serve only to impeach  testimony'], 'label': 2}\n"]}]},{"cell_type":"code","source":["from dataclasses import dataclass\n","from typing import Optional, Tuple\n","\n","import torch\n","import numpy as np\n","from torch import nn\n","from transformers.file_utils import ModelOutput\n","\n","\n","@dataclass\n","class SimpleOutput(ModelOutput):\n","    last_hidden_state: torch.FloatTensor = None\n","    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n","    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n","    attentions: Optional[Tuple[torch.FloatTensor]] = None\n","    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n","\n","\n","def sinusoidal_init(num_embeddings: int, embedding_dim: int):\n","    # keep dim 0 for padding token position encoding zero vector\n","    position_enc = np.array([\n","        [pos / np.power(10000, 2 * i / embedding_dim) for i in range(embedding_dim)]\n","        if pos != 0 else np.zeros(embedding_dim) for pos in range(num_embeddings)])\n","\n","    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2])  # dim 2i\n","    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2])  # dim 2i+1\n","    return torch.from_numpy(position_enc).type(torch.FloatTensor)\n","\n","\n","class HierarchicalBert(nn.Module):\n","\n","    def __init__(self, encoder, max_segments=64, max_segment_length=128):\n","        super(HierarchicalBert, self).__init__()\n","        supported_models = ['bert', 'roberta', 'deberta']\n","        assert encoder.config.model_type in supported_models  # other model types are not supported so far\n","        # Pre-trained segment (token-wise) encoder, e.g., BERT\n","        self.encoder = encoder\n","        # Specs for the segment-wise encoder\n","        self.hidden_size = encoder.config.hidden_size\n","        self.max_segments = max_segments\n","        self.max_segment_length = max_segment_length\n","        # Init sinusoidal positional embeddings\n","        self.seg_pos_embeddings = nn.Embedding(max_segments + 1, encoder.config.hidden_size,\n","                                               padding_idx=0,\n","                                               _weight=sinusoidal_init(max_segments + 1, encoder.config.hidden_size))\n","        # Init segment-wise transformer-based encoder\n","        self.seg_encoder = nn.Transformer(d_model=encoder.config.hidden_size,\n","                                          nhead=encoder.config.num_attention_heads,\n","                                          batch_first=True, dim_feedforward=encoder.config.intermediate_size,\n","                                          activation=encoder.config.hidden_act,\n","                                          dropout=encoder.config.hidden_dropout_prob,\n","                                          layer_norm_eps=encoder.config.layer_norm_eps,\n","                                          num_encoder_layers=2, num_decoder_layers=0).encoder\n","\n","    def forward(self,\n","                input_ids=None,\n","                attention_mask=None,\n","                token_type_ids=None,\n","                position_ids=None,\n","                head_mask=None,\n","                inputs_embeds=None,\n","                labels=None,\n","                output_attentions=None,\n","                output_hidden_states=None,\n","                return_dict=None,\n","                ):\n","        # Hypothetical Example\n","        # Batch of 4 documents: (batch_size, n_segments, max_segment_length) --> (4, 64, 128)\n","        # BERT-BASE encoder: 768 hidden units\n","\n","        # Squash samples and segments into a single axis (batch_size * n_segments, max_segment_length) --> (256, 128)\n","        input_ids_reshape = input_ids.contiguous().view(-1, input_ids.size(-1))\n","        attention_mask_reshape = attention_mask.contiguous().view(-1, attention_mask.size(-1))\n","        if token_type_ids is not None:\n","            token_type_ids_reshape = token_type_ids.contiguous().view(-1, token_type_ids.size(-1))\n","        else:\n","            token_type_ids_reshape = None\n","\n","        # Encode segments with BERT --> (256, 128, 768)\n","        encoder_outputs = self.encoder(input_ids=input_ids_reshape,\n","                                       attention_mask=attention_mask_reshape,\n","                                       token_type_ids=token_type_ids_reshape)[0]\n","\n","        # Reshape back to (batch_size, n_segments, max_segment_length, output_size) --> (4, 64, 128, 768)\n","        encoder_outputs = encoder_outputs.contiguous().view(input_ids.size(0), self.max_segments,\n","                                                            self.max_segment_length,\n","                                                            self.hidden_size)\n","\n","        # Gather CLS outputs per segment --> (4, 64, 768)\n","        encoder_outputs = encoder_outputs[:, :, 0]\n","\n","        # Infer real segments, i.e., mask paddings\n","        seg_mask = (torch.sum(input_ids, 2) != 0).to(input_ids.dtype)\n","        # Infer and collect segment positional embeddings\n","        seg_positions = torch.arange(1, self.max_segments + 1).to(input_ids.device) * seg_mask\n","        # Add segment positional embeddings to segment inputs\n","        encoder_outputs += self.seg_pos_embeddings(seg_positions)\n","\n","        # Encode segments with segment-wise transformer\n","        seg_encoder_outputs = self.seg_encoder(encoder_outputs)\n","\n","        # Collect document representation\n","        outputs, _ = torch.max(seg_encoder_outputs, 1)\n","\n","        return SimpleOutput(last_hidden_state=outputs, hidden_states=outputs)\n","\n","\n","if __name__ == \"__main__\":\n","    from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n","    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","\n","    # Use as a stand-alone encoder\n","    bert = AutoModel.from_pretrained('bert-base-uncased')\n","    model = HierarchicalBert(encoder=bert, max_segments=64, max_segment_length=128)\n","\n","    fake_inputs = {'input_ids': [], 'attention_mask': [], 'token_type_ids': []}\n","    for i in range(4):\n","        # Tokenize segment\n","        temp_inputs = tokenizer(['dog ' * 126] * 64)\n","        fake_inputs['input_ids'].append(temp_inputs['input_ids'])\n","        fake_inputs['attention_mask'].append(temp_inputs['attention_mask'])\n","        fake_inputs['token_type_ids'].append(temp_inputs['token_type_ids'])\n","\n","    fake_inputs['input_ids'] = torch.as_tensor(fake_inputs['input_ids'])\n","    fake_inputs['attention_mask'] = torch.as_tensor(fake_inputs['attention_mask'])\n","    fake_inputs['token_type_ids'] = torch.as_tensor(fake_inputs['token_type_ids'])\n","\n","    output = model(fake_inputs['input_ids'], fake_inputs['attention_mask'], fake_inputs['token_type_ids'])\n","\n","    # 4 document representations of 768 features are expected\n","    assert output[0].shape == torch.Size([4, 768])\n","\n","    # Use with HuggingFace AutoModelForSequenceClassification and Trainer API\n","\n","    # Init Classifier\n","    model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=10)\n","    # Replace flat BERT encoder with hierarchical BERT encoder\n","    model.bert = HierarchicalBert(encoder=model.bert, max_segments=64, max_segment_length=128)\n","    output = model(fake_inputs['input_ids'], fake_inputs['attention_mask'], fake_inputs['token_type_ids'])\n","\n","    # 4 document outputs with 10 (num_labels) logits are expected\n","    assert output.logits.shape == torch.Size([4, 10])\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":233,"referenced_widgets":["ae72a1ce74a54d80b30774f9d858e2d8","a12807eb3f1b4dbd8f952ffbb94f5576","b09bd26d1a1e476a82f338b63c045d8a","ff2cc63c390b450582ab15af099a9cd8","2ec99028b3174a32bfa8c3f6e6f4b167","eabe6f0f988b479a98ef2cbca97c9229","c2a28971e2434125a1bcf7e3f4027832","a8fdd3d176f242de917fd2bcfaa42c85","0d2aa40de64b4dbd84dd063b915df802","e418dbf101e944e5b14e4f04c136f0f8","84de5eb0decc495799c030eb5e0bbc6a","f157cacc24124e44a14b6e03fd14acfe","844810d9577840d797cf7392ef5a5b78","95abd7f154be4eb89c7d787b8402e156","638361bc7b1d47a5b9daf752c864be41","d219943071574aeb92dcf88d43dd6d74","8c6ecb3c8c514eff822d4227edf467cf","5e2b82d321314ed2aedbb3b75f315d0c","a5419157828846d18d4eca914d81656e","a324cb8d913a4899bb5fb64b865bfbbe","e1e5f8dbe13147959c963c154001a34a","994a247f75f54d748ae0cbdb88982e9b","0f31151e42454cb98d4375ab76ad8d7f","d765bdc40edf498dbc2bd1e923eef217","25ae1c6dbada44bfbfc663cbc0e23cf1","96bdaf70586740ad85e9cb1d8dd86787","3bf0f5e80a6041d2b81ac1f9f8912eb0","2f4658fd27e647a4ba561d34c5c00194","4ce98c3aa063423591fbd7c788815365","6406fccc650f46d19cd18e6118b9d8c5","ce10211bccd948f69260821f7e7e6c0f","080fc23c3cd1444dbb5eed08eb17f9d3","8716d30567d74b029decd99ebd0f522e","6b464328ed7c47788dd93c1d637a56f5","f514b16a8d8d46c48ba8c29006e70a25","83d035a4a2f9427e9ad1dad5eb575eef","3bfc7fb54f244e088b3788c6a13b3227","a58336669d1e4ab1b71f9d54caaf0f52","f52fb9f5e25d4c53936186c889d9a616","2aa7b3965d254fda93939e7ee73e1bf4","8b68cd75868b4893b9445588e493d5fc","e2fa504a285c49c1905af9933ac3c575","903d3a5d4aad4538b5b715502169fafc","28f14d4ef6df40fbbf70fd475d035946","ee855712759e462ba1a6e8ae1bcd5c88","57c8c568ac7141f399b186524c2597f8","67076958d0dd4607a6cb5089cec58956","12969f4afd674e63843981eadd7ca2d3","3c502e3f0d4543c19c1e193cb56dd40e","d92c4e6339df4c05bc2c5610293eb477","01ec4651c0c54d18b7293dcbc3778b04","9e31549d595f4bb3866d03a0f5ac3b0a","d24d64116db34550ac82f6e608e66296","763bdf5f44494f9da1d82ffdace41d9a","992f22966a384ba586e6678118ea3860"]},"id":"eRtaFQT6v9xN","executionInfo":{"status":"ok","timestamp":1692683039593,"user_tz":-60,"elapsed":62082,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"}},"outputId":"2a008331-c281-4995-93ff-d141a1c26b6b"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae72a1ce74a54d80b30774f9d858e2d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f157cacc24124e44a14b6e03fd14acfe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f31151e42454cb98d4375ab76ad8d7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b464328ed7c47788dd93c1d637a56f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee855712759e462ba1a6e8ae1bcd5c88"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["from torch import nn\n","from transformers import Trainer\n","\n","\n","class MultilabelTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.pop(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","        loss_fct = nn.BCEWithLogitsLoss()\n","        loss = loss_fct(logits.view(-1, self.model.config.num_labels),\n","                        labels.float().view(-1, self.model.config.num_labels))\n","        return (loss, outputs) if return_outputs else loss\n","\n","\n","\n"],"metadata":{"id":"6OBH6a0-wDtc","executionInfo":{"status":"ok","timestamp":1692683040065,"user_tz":-60,"elapsed":502,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import logging\n","import os\n","from dataclasses import dataclass\n","from enum import Enum\n","from typing import List, Optional\n","\n","import tqdm\n","import re\n","\n","from filelock import FileLock\n","from transformers import PreTrainedTokenizer, is_tf_available, is_torch_available\n","import datasets\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","@dataclass(frozen=True)\n","class InputFeatures:\n","    \"\"\"\n","    A single set of features of data.\n","    Property names are the same names as the corresponding inputs to a model.\n","    \"\"\"\n","\n","    input_ids: List[List[int]]\n","    attention_mask: Optional[List[List[int]]]\n","    token_type_ids: Optional[List[List[int]]]\n","    label: Optional[int]\n","\n","\n","class Split(Enum):\n","    train = \"train\"\n","    dev = \"dev\"\n","    test = \"test\"\n","\n","\n","if is_torch_available():\n","    import torch\n","    from torch.utils.data.dataset import Dataset\n","\n","    class MultipleChoiceDataset(Dataset):\n","        \"\"\"\n","        PyTorch multiple choice dataset class\n","        \"\"\"\n","\n","        features: List[InputFeatures]\n","\n","        def __init__(\n","            self,\n","            tokenizer: PreTrainedTokenizer,\n","            task: str,\n","            max_seq_length: Optional[int] = None,\n","            overwrite_cache=False,\n","            mode: Split = Split.train,\n","        ):\n","            dataset = datasets.load_dataset('lex_glue', task)\n","            tokenizer_name = re.sub('[^a-z]+', ' ', tokenizer.name_or_path).title().replace(' ', '')\n","            cached_features_file = os.path.join(\n","                '.cache',\n","                task,\n","                \"cached_{}_{}_{}_{}\".format(\n","                    mode.value,\n","                    tokenizer_name,\n","                    str(max_seq_length),\n","                    task,\n","                ),\n","            )\n","\n","            # Make sure only the first process in distributed training processes the dataset,\n","            # and the others will use the cache.\n","            lock_path = cached_features_file + \".lock\"\n","            if not os.path.exists(os.path.join('.cache', task)):\n","                if not os.path.exists('.cache'):\n","                    os.mkdir('.cache')\n","                os.mkdir(os.path.join('.cache', task))\n","            with FileLock(lock_path):\n","\n","                if os.path.exists(cached_features_file) and not overwrite_cache:\n","                    logger.info(f\"Loading features from cached file {cached_features_file}\")\n","                    self.features = torch.load(cached_features_file)\n","                else:\n","                    logger.info(f\"Creating features from dataset file at {task}\")\n","                    if mode == Split.dev:\n","                        examples = dataset['validation']\n","                    elif mode == Split.test:\n","                        examples = dataset['test']\n","                    elif mode == Split.train:\n","                        examples = dataset['train']\n","                    logger.info(\"Training examples: %s\", len(examples))\n","                    self.features = convert_examples_to_features(\n","                        examples,\n","                        max_seq_length,\n","                        tokenizer,\n","                    )\n","                    logger.info(\"Saving features into cached file %s\", cached_features_file)\n","                    torch.save(self.features, cached_features_file)\n","\n","        def __len__(self):\n","            return len(self.features)\n","\n","        def __getitem__(self, i) -> InputFeatures:\n","            return self.features[i]\n","\n","\n","if is_tf_available():\n","    import tensorflow as tf\n","\n","    class TFMultipleChoiceDataset:\n","        \"\"\"\n","        TensorFlow multiple choice dataset class\n","        \"\"\"\n","\n","        features: List[InputFeatures]\n","\n","        def __init__(\n","            self,\n","            tokenizer: PreTrainedTokenizer,\n","            task: str,\n","            max_seq_length: Optional[int] = 256,\n","            overwrite_cache=False,\n","            mode: Split = Split.train,\n","        ):\n","            dataset = datasets.load_dataset('lex_glue')\n","\n","            logger.info(f\"Creating features from dataset file at {task}\")\n","            if mode == Split.dev:\n","                examples = dataset['validation']\n","            elif mode == Split.test:\n","                examples = dataset['test']\n","            else:\n","                examples = dataset['train']\n","            logger.info(f\"{mode.name.title()} examples: %s\", len(examples))\n","\n","            self.features = convert_examples_to_features(\n","                examples,\n","                max_seq_length,\n","                tokenizer,\n","            )\n","\n","            def gen():\n","                for (ex_index, ex) in tqdm.tqdm(enumerate(self.features), desc=\"convert examples to features\"):\n","                    if ex_index % 10000 == 0:\n","                        logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n","\n","                    yield (\n","                        {\n","                            \"input_ids\": ex.input_ids,\n","                            \"attention_mask\": ex.attention_mask,\n","                            \"token_type_ids\": ex.token_type_ids,\n","                        },\n","                        ex.label,\n","                    )\n","\n","            self.dataset = tf.data.Dataset.from_generator(\n","                gen,\n","                (\n","                    {\n","                        \"input_ids\": tf.int32,\n","                        \"attention_mask\": tf.int32,\n","                        \"token_type_ids\": tf.int32,\n","                    },\n","                    tf.int64,\n","                ),\n","                (\n","                    {\n","                        \"input_ids\": tf.TensorShape([None, None]),\n","                        \"attention_mask\": tf.TensorShape([None, None]),\n","                        \"token_type_ids\": tf.TensorShape([None, None]),\n","                    },\n","                    tf.TensorShape([]),\n","                ),\n","            )\n","\n","        def get_dataset(self):\n","            self.dataset = self.dataset.apply(tf.data.experimental.assert_cardinality(len(self.features)))\n","\n","            return self.dataset\n","\n","        def __len__(self):\n","            return len(self.features)\n","\n","        def __getitem__(self, i) -> InputFeatures:\n","            return self.features[i]\n","\n","\n","def convert_examples_to_features(\n","    examples: datasets.Dataset,\n","    max_length: int,\n","    tokenizer: PreTrainedTokenizer,\n",") -> List[InputFeatures]:\n","    \"\"\"\n","    Loads a data file into a list of `InputFeatures`\n","    \"\"\"\n","    features = []\n","    feature_list = []\n","    context_params = []  # Initialize context_params list\n","    ending_params = []   # Initialize ending_params list\n","    for (ex_index, example) in tqdm.tqdm(enumerate(examples), desc=\"convert examples to features\"):\n","        if ex_index % 10000 == 0:\n","            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n","        choices_inputs = []\n","        for ending_idx, ending in enumerate(example['endings']):\n","            context = example['context']\n","            inputs = tokenizer(\n","                context,\n","                ending,\n","                add_special_tokens=True,\n","                max_length=max_length,\n","                padding=\"max_length\",\n","                truncation=True,\n","            )\n","\n","            choices_inputs.append(inputs)\n","            context_params.append(context)  # Append context_params to list\n","            ending_params.append(ending)\n","        label = example['label']\n","\n","        input_ids = [x[\"input_ids\"] for x in choices_inputs]\n","        attention_mask = (\n","            [x[\"attention_mask\"] for x in choices_inputs] if \"attention_mask\" in choices_inputs[0] else None\n","        )\n","        token_type_ids = (\n","            [x[\"token_type_ids\"] for x in choices_inputs] if \"token_type_ids\" in choices_inputs[0] else None\n","        )\n","\n","\n","        feature_dict = {\n","                \"input_ids\": inputs['input_ids'],\n","                \"attention_mask\": inputs['attention_mask'],\n","                \"token_type_ids\": inputs['token_type_ids'],\n","                \"label\": label,\n","                \"context_params\": context_params,\n","                \"ending_params\": ending_params,\n","            }\n","        #feature_list.append(feature_dict)\n","        #features.append(feature_dict)\n","\n","    for f in feature_dict[:2]:\n","        logger.info(\"*** Example ***\")\n","        logger.info(\"feature: %s\" % f)\n","\n","    return feature_dict\n"],"metadata":{"id":"AbVxAq2-wNLK","executionInfo":{"status":"ok","timestamp":1692683040066,"user_tz":-60,"elapsed":7,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#!/usr/bin/env python\n","# coding=utf-8\n","\"\"\" Finetuning models on CaseHOLD (e.g. Bert, RoBERTa, LEGAL-BERT).\"\"\"\n","\n","import logging\n","import os\n","from dataclasses import dataclass, field\n","from typing import Optional\n","from sklearn.model_selection import ParameterGrid\n","import numpy as np\n","import random\n","import shutil\n","import glob\n","import os\n","\n","import transformers\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","\t\tAutoModelForMultipleChoice,\n","    DataCollatorWithPadding,\n","    EvalPrediction,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    default_data_collator,\n","    set_seed,\n","    EarlyStoppingCallback,\n","    Trainer\n",")\n","from transformers.trainer_utils import is_main_process\n","from transformers import EarlyStoppingCallback\n","# from casehold_helpers import MultipleChoiceDataset, Split\n","from sklearn.metrics import f1_score\n","# from models.deberta import DebertaForMultipleChoice\n","\n","logger = logging.getLogger(__name__)\n","\n","param_grid = {\n","    'learning_rate': [1e-5, 2e-5],  # Learning rates to try\n","    'num_train_epochs': [1, 2],        # Number of training epochs to try\n","    'per_device_train_batch_size': [2, 4],  # Batch sizes for training\n","    'per_device_eval_batch_size': [2, 4],   # Batch sizes for evaluation\n","}\n","\n","\n","@dataclass\n","class ModelArguments:\n","    \"\"\"\n","    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n","    \"\"\"\n","\n","    model_name_or_path: str = field(\n","        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n","    )\n","    config_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n","    )\n","    tokenizer_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n","    )\n","    cache_dir: Optional[str] = field(\n","        default=None,\n","        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n","    )\n","\n","\n","@dataclass\n","class DataTrainingArguments:\n","    \"\"\"\n","    Arguments pertaining to what data we are going to input our model for training and eval.\n","    \"\"\"\n","\n","    task_name: str = field(default=\"case_hold\", metadata={\"help\": \"The name of the task to train on\"})\n","    max_seq_length: int = field(\n","        default=256,\n","        metadata={\n","            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n","            \"than this will be truncated, sequences shorter will be padded.\"\n","        },\n","    )\n","    pad_to_max_length: bool = field(\n","        default=True,\n","        metadata={\n","            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n","            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n","        },\n","    )\n","    max_train_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_eval_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_predict_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    overwrite_cache: bool = field(\n","        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n","    )\n","\n","\n","def main(training_args):\n","    # See all possible arguments in src/transformers/training_args.py\n","    # or by passing the --help flag to this script.\n","    # We now keep distinct sets of args, for a cleaner separation of concerns.\n","    data_args = DataTrainingArguments(\n","        max_seq_length=128,\n","        # max_segments=64,\n","        # max_seg_length=128,\n","        overwrite_cache=False,\n","        pad_to_max_length=True,\n","    )\n","    model_args = ModelArguments(\n","        model_name_or_path=\"bert-base-uncased\",\n","        #\"microsoft/deberta-base\",\n","        # hierarchical=True,\n","        #do_lower_case=True,\n","        #use_fast_tokenizer=True,\n","    )\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n","    )\n","    logger.warning(\n","        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n","        training_args.local_rank,\n","        training_args.device,\n","        training_args.n_gpu,\n","        bool(training_args.local_rank != -1),\n","        training_args.fp16,\n","    )\n","    # Set the verbosity to info of the Transformers logger (on main process only):\n","    if is_main_process(training_args.local_rank):\n","        transformers.utils.logging.set_verbosity_info()\n","        transformers.utils.logging.enable_default_handler()\n","        transformers.utils.logging.enable_explicit_format()\n","    logger.info(\"Training/evaluation parameters %s\", training_args)\n","\n","    # Set seed\n","    set_seed(training_args.seed)\n","\n","    # Load pretrained model and tokenizer\n","    config = AutoConfig.from_pretrained(\n","        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n","        num_labels=5,\n","        finetuning_task=data_args.task_name,\n","        cache_dir=model_args.cache_dir,\n","    )\n","\n","    if config.model_type == 'big_bird':\n","        config.attention_type = 'original_full'\n","    elif config.model_type == 'longformer':\n","        config.attention_window = [data_args.max_seq_length] * config.num_hidden_layers\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n","        cache_dir=model_args.cache_dir,\n","        # Default fast tokenizer is buggy on CaseHOLD task, switch to legacy tokenizer\n","        use_fast=True,\n","    )\n","\n","    if config.model_type != 'deberta':\n","        model = AutoModelForMultipleChoice.from_pretrained(\n","            model_args.model_name_or_path,\n","            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","            config=config,\n","            cache_dir=model_args.cache_dir,\n","        )\n","    else:\n","        model = DebertaForMultipleChoice.from_pretrained(\n","            model_args.model_name_or_path,\n","            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","            config=config,\n","            cache_dir=model_args.cache_dir,\n","        )\n","\n","    train_dataset = None\n","    eval_dataset = None\n","\n","    # If do_train passed, train_dataset by default loads train split from file named train.csv in data directory\n","    if training_args.do_train:\n","        train_dataset = \\\n","            MultipleChoiceDataset(\n","                tokenizer=tokenizer,\n","                task=data_args.task_name,\n","                max_seq_length=data_args.max_seq_length,\n","                overwrite_cache=data_args.overwrite_cache,\n","                mode=Split.train,\n","            )\n","\n","    # If do_eval or do_predict passed, eval_dataset by default loads dev split from file named dev.csv in data directory\n","    if training_args.do_eval:\n","        eval_dataset = \\\n","            MultipleChoiceDataset(\n","                tokenizer=tokenizer,\n","                task=data_args.task_name,\n","                max_seq_length=data_args.max_seq_length,\n","                overwrite_cache=data_args.overwrite_cache,\n","                mode=Split.dev,\n","            )\n","\n","    if training_args.do_predict:\n","        predict_dataset = \\\n","            MultipleChoiceDataset(\n","                tokenizer=tokenizer,\n","                task=data_args.task_name,\n","                max_seq_length=data_args.max_seq_length,\n","                overwrite_cache=data_args.overwrite_cache,\n","                mode=Split.test,\n","            )\n","\n","    if training_args.do_train:\n","        if data_args.max_train_samples is not None:\n","            train_dataset = train_dataset[:data_args.max_train_samples]\n","        # Log a few random samples from the training set:\n","        for index in random.sample(range(len(train_dataset)), 3):\n","            logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n","\n","    if training_args.do_eval:\n","        if data_args.max_eval_samples is not None:\n","            eval_dataset = eval_dataset[:data_args.max_eval_samples]\n","\n","    if training_args.do_predict:\n","        if data_args.max_predict_samples is not None:\n","            predict_dataset = predict_dataset[:data_args.max_predict_samples]\n","\n","    # Define custom compute_metrics function, returns macro F1 metric for CaseHOLD task\n","    def compute_metrics(p: EvalPrediction):\n","        preds = np.argmax(p.predictions, axis=1)\n","        # Compute macro and micro F1 for 5-class CaseHOLD task\n","        macro_f1 = f1_score(y_true=p.label_ids, y_pred=preds, average='macro', zero_division=0)\n","        micro_f1 = f1_score(y_true=p.label_ids, y_pred=preds, average='micro', zero_division=0)\n","        return {'macro-f1': macro_f1, 'micro-f1': micro_f1}\n","\n","    # Initialize our Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=eval_dataset,\n","        compute_metrics=compute_metrics,\n","        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n","    )\n","\n","    # Training\n","    if training_args.do_train:\n","        trainer.train(\n","            model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n","        )\n","        trainer.save_model()\n","        # Re-save the tokenizer for model sharing\n","        if trainer.is_world_process_zero():\n","            tokenizer.save_pretrained(training_args.output_dir)\n","\n","    # Evaluation on eval_dataset\n","    if training_args.do_eval:\n","        logger.info(\"*** Evaluate ***\")\n","    metrics = trainer.evaluate(eval_dataset=eval_dataset)\n","\n","    max_eval_samples = (\n","        data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n","    )\n","    metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n","\n","    # Save the evaluation metrics to a file\n","    with open(\"eval_metrics.txt\", \"w\") as f:\n","        for key, value in metrics.items():\n","            f.write(f\"{key}: {value}\\n\")\n","\n","    for example in eval_dataset:\n","        print(\"###########\")\n","        print(example.context_params)\n","        break\n","    # Get predictions from the model\n","    predictions, labels, contexts = trainer.predict(eval_dataset)\n","\n","    correct_predictions = []\n","    incorrect_predictions = []\n","    incorrect_contexts = []\n","    incorrect_ending_lists =[]\n","\n","    for pred, label, context in zip(predictions, labels, contexts):\n","        predicted_label = np.argmax(pred)\n","        if predicted_label == label:\n","            correct_predictions.append((pred, label))\n","        else:\n","            incorrect_predictions.append((pred, label, contexts))\n","\n","    # Save correct predictions to a file\n","    with open(\"correct_predictions.txt\", \"w\") as f:\n","        for pred, label in correct_predictions:\n","            f.write(f\"Predicted: {pred}, Actual: {label}\\n\")\n","\n","    # Save incorrect predictions to a file\n","    with open(\"incorrect_predictions.txt\", \"w\") as f:\n","        for pred, label ,contexts in incorrect_predictions:\n","            f.write(f\"Predicted: {pred}, Actual: {label}\\n\")\n","            f.write(f\"Context: {contexts}\\n\")\n","    for pred, label, context, entry in zip(predictions, labels, contexts, eval_dataset):\n","        predicted_label = np.argmax(pred)\n","        context_params = entry.context_params\n","        ending_params = entry.ending_params\n","\n","        incorrect_contexts.append((context_params, label))\n","        incorrect_ending_lists.append(ending_params)\n","\n","        incorrect_predictions.append((pred, label, context_params,ending_params))\n","# Save incorrect contexts and ending_lists to a JSON file\n","    incorrect_entries = {\n","    \"contexts\": incorrect_contexts,\n","    \"ending_lists\": incorrect_ending_lists\n","}\n","\n","    with open(\"incorrect_entries.json\", \"w\") as json_file:\n","         json.dump(incorrect_entries, json_file, indent=4)\n","\n","    # Predict on eval_dataset\n","    if training_args.do_predict:\n","        logger.info(\"*** Predict ***\")\n","\n","        predictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix=\"predict\")\n","\n","        max_predict_samples = (\n","            data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)\n","        )\n","        metrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n","\n","        trainer.log_metrics(\"predict\", metrics)\n","        trainer.save_metrics(\"predict\", metrics)\n","\n","        output_predict_file = os.path.join(training_args.output_dir, \"test_predictions.csv\")\n","        if trainer.is_world_process_zero():\n","            with open(output_predict_file, \"w\") as writer:\n","                for index, pred_list in enumerate(predictions):\n","                    pred_line = '\\t'.join([f'{pred:.5f}' for pred in pred_list])\n","                    writer.write(f\"{index}\\t{pred_line}\\n\")\n","\n","\n","# Print the best hyperparameters and its corresponding validation macro-f1 score\n","\n","    # Clean up checkpoints\n","    checkpoints = [filepath for filepath in glob.glob(f'{training_args.output_dir}/*/') if '/checkpoint' in filepath]\n","    for checkpoint in checkpoints:\n","        shutil.rmtree(checkpoint)\n","\n","\n","# def _mp_fn(index):\n","# For xla_spawn (TPUs)\n","# main()\n","\n","\n","if __name__ == \"__main__\":\n","    # main()\n","\n","    training_args = TrainingArguments(\n","        do_train=False,\n","        do_eval=True,\n","        do_predict=True,\n","        output_dir=os.getcwd(),\n","        overwrite_output_dir=True,\n","        num_train_epochs=10,\n","        per_device_train_batch_size=8,\n","        per_device_eval_batch_size=8,\n","        save_steps=500,\n","        save_total_limit=2,\n","        fp16=False,\n","        logging_dir=\"./logs\",\n","        logging_steps=200,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=500,\n","        logging_first_step=False,\n","        load_best_model_at_end=True,\n","        metric_for_best_model=\"micro-f1\",\n","    )\n","    main(training_args)\n","\t\t # Train the model\n","    #train_result = trainer.train()\n","    #metrics = train_result.metrics\n","\n","    # Evaluate the model\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"J4afyzYcwEep","executionInfo":{"status":"error","timestamp":1692674870321,"user_tz":-60,"elapsed":9224,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"}},"outputId":"7c191413-0e41-4a58-9b97-6106d77dc5ec"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["[INFO|training_args.py:1299] 2023-08-22 03:27:40,934 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n","[INFO|training_args.py:1713] 2023-08-22 03:27:40,935 >> PyTorch: setting up devices\n","[INFO|training_args.py:1439] 2023-08-22 03:27:40,936 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False\n","[INFO|configuration_utils.py:712] 2023-08-22 03:27:41,200 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/config.json\n","[INFO|configuration_utils.py:768] 2023-08-22 03:27:41,202 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"finetuning_task\": \"case_hold\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","[INFO|configuration_utils.py:712] 2023-08-22 03:27:41,460 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/config.json\n","[INFO|configuration_utils.py:768] 2023-08-22 03:27:41,462 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","[INFO|tokenization_utils_base.py:1839] 2023-08-22 03:27:41,465 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/vocab.txt\n","[INFO|tokenization_utils_base.py:1839] 2023-08-22 03:27:41,465 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/tokenizer.json\n","[INFO|tokenization_utils_base.py:1839] 2023-08-22 03:27:41,466 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1839] 2023-08-22 03:27:41,468 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1839] 2023-08-22 03:27:41,469 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/tokenizer_config.json\n","[INFO|configuration_utils.py:712] 2023-08-22 03:27:41,473 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/config.json\n","[INFO|configuration_utils.py:768] 2023-08-22 03:27:41,475 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","[INFO|modeling_utils.py:2603] 2023-08-22 03:27:41,502 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/model.safetensors\n","[INFO|modeling_utils.py:3319] 2023-08-22 03:27:42,347 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultipleChoice: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:3331] 2023-08-22 03:27:42,348 >> Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|trainer.py:3081] 2023-08-22 03:27:49,676 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-08-22 03:27:49,677 >>   Num examples = 3900\n","[INFO|trainer.py:3086] 2023-08-22 03:27:49,678 >>   Batch size = 8\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-d7d6160925b8>\u001b[0m in \u001b[0;36m<cell line: 367>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mmetric_for_best_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"micro-f1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     )\n\u001b[0;32m--> 390\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m                  \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;31m#train_result = trainer.train()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-d7d6160925b8>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(training_args)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtraining_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_eval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*** Evaluate ***\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     max_eval_samples = (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2933\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2934\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   2935\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3111\u001b[0m         \u001b[0mobserved_num_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3112\u001b[0m         \u001b[0;31m# Main evaluation loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3113\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3114\u001b[0m             \u001b[0;31m# Update the observed num examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3115\u001b[0m             \u001b[0mobserved_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;31m# We iterate one batch ahead to check when we are at the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_remove_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36mdefault_data_collator\u001b[0;34m(features, return_tensors)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_tensors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_default_data_collator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mreturn_tensors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf_default_data_collator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36mtorch_default_data_collator\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: vars() argument must have __dict__ attribute"]}]},{"cell_type":"code","source":["if training_args.do_eval:\n","        logger.info(\"*** Evaluate ***\")\n","    metrics = trainer.evaluate(eval_dataset=eval_dataset)\n","\n","    max_eval_samples = (\n","        data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n","    )\n","    metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n","\n","    # Save the evaluation metrics to a file\n","    with open(\"eval_metrics.txt\", \"w\") as f:\n","        for key, value in metrics.items():\n","            f.write(f\"{key}: {value}\\n\")\n","\n","    # Get predictions from the model\n","    predictions, labels, contexts = trainer.predict(eval_dataset)\n","\n","    correct_predictions = []\n","    incorrect_predictions = []\n","    incorrect_contexts = []\n","    incorrect_ending_lists =[]\n","\n","    for pred, label, context in zip(predictions, labels, contexts):\n","        predicted_label = np.argmax(pred)\n","        if predicted_label == label:\n","            correct_predictions.append((pred, label))\n","        else:\n","            incorrect_predictions.append((pred, label, contexts))\n","\n","    # Save correct predictions to a file\n","    with open(\"correct_predictions.txt\", \"w\") as f:\n","        for pred, label in correct_predictions:\n","            f.write(f\"Predicted: {pred}, Actual: {label}\\n\")\n","\n","    # Save incorrect predictions to a file\n","    with open(\"incorrect_predictions.txt\", \"w\") as f:\n","        for pred, label ,contexts in incorrect_predictions:\n","            f.write(f\"Predicted: {pred}, Actual: {label}\\n\")\n","            f.write(f\"Context: {contexts}\\n\")\n","    for pred, label, context, entry in zip(predictions, labels, contexts, eval_dataset):\n","        predicted_label = np.argmax(pred)\n","    if predicted_label != label:\n","        context = entry['context_params']\n","        endings = entry['ending_params']\n","        token_type_ids = entry['token_type_ids']\n","        #context = entry['context']\n","        #endings = entry['endings']\n","\n","        incorrect_contexts.append((context, label))\n","        incorrect_ending_lists.append(endings)\n","\n","        incorrect_predictions.append((pred, label, context))\n","\n","# Save incorrect contexts and ending_lists to a JSON file\n","    incorrect_entries = {\n","    \"contexts\": incorrect_contexts,\n","    \"ending_lists\": incorrect_ending_lists\n","}\n","\n","    with open(\"incorrect_entries.json\", \"w\") as json_file:\n","         json.dump(incorrect_entries, json_file, indent=4)\n","\n","\n","\n"," training_args = TrainingArguments(\n","        do_train=True,\n","        do_eval=True,\n","        do_predict=True,\n","        output_dir=os.getcwd(),\n","        overwrite_output_dir=True,\n","        num_train_epochs=20,\n","        per_device_train_batch_size=4,\n","        per_device_eval_batch_size=4,\n","        save_steps=500,\n","        save_total_limit=2,\n","        fp16=False,\n","        logging_dir=\"./logs\",\n","        logging_steps=200,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=500,\n","        logging_first_step=False,\n","        load_best_model_at_end=True,\n","        metric_for_best_model=\"micro-f1\",\n","    )\n","    main(training_args)"],"metadata":{"id":"p4dmBHLdUh5B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","\n","# Read the \"incorrect-predictions.txt\" file\n","with open(\"/content/incorrect_predictions.txt\", \"r\") as f:\n","    lines = f.readlines()\n","\n","# Initialize variables\n","current_entry = {}\n","data_entries = []\n","\n","# Process the lines and create data entries\n","for line in lines:\n","    line = line.strip()\n","    if line.startswith(\"Index: \"):\n","        current_entry[\"Index\"] = int(line.split(\": \")[1])\n","    elif line.startswith(\"Input Text: \"):\n","        current_entry[\"text\"] = line.split(\": \")[1]\n","    elif line.startswith(\"Input IDs: \"):\n","        input_ids_str = line.split(\": \")[1]\n","        input_ids = [int(id_str) for id_str in re.findall(r'\\d+', input_ids_str)]\n","        current_entry[\"input_ids\"] = input_ids\n","    elif line.startswith(\"Labels: \"):  # Process labels\n","        labels_str = line.split(\": \")[1]\n","        labels = [int(label_str) for label_str in re.findall(r'\\d+', labels_str)]\n","        current_entry[\"labels\"] = labels\n","    elif line.startswith(\"Original_Labels: \"):  # Process original labels\n","        labels_str = line.split(\": \")[1]\n","        original_labels = [int(label_str) for label_str in re.findall(r'\\d+', labels_str)]\n","        current_entry[\"original_labels\"] = original_labels\n","        data_entries.append(current_entry)\n","        current_entry = {}\n","\n","# Extract the list of labels from data entries\n","label_lists = [entry.get(\"original_labels\", []) for entry in data_entries]\n","\n","print(label_lists)\n","print(len(label_lists))\n","filtered_list = [item for item in set(tuple(lst) for lst in label_lists) if item]\n","print(filtered_list)\n","# Print the list of labels for each entry\n","#for index, labels in enumerate(label_lists):\n","#    print(f\"Entry {index}: Labels = {labels}\")\n"],"metadata":{"id":"vmJ5NOXqz-mf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import re\n","\n","# Read the \"incorrect-predictions.txt\" file\n","with open(\"/content/incorrect_predictions.txt\", \"r\") as f:\n","    lines = f.readlines()\n","\n","# Initialize variables\n","current_entry = {}\n","data_entries = []\n","\n","# Process the lines and create data entries\n","for line in lines:\n","    line = line.strip()\n","    if line.startswith(\"Index: \"):\n","        current_entry[\"Index\"] = int(line.split(\": \")[1])\n","    elif line.startswith(\"Input Text: \"):\n","        current_entry[\"text\"] = line.split(\": \")[1]\n","    elif line.startswith(\"Input IDs: \"):\n","        input_ids_str = line.split(\": \")[1]\n","        input_ids = [int(id_str) for id_str in re.findall(r'\\d+', input_ids_str)]\n","        current_entry[\"input_ids\"] = input_ids\n","        current_entry[\"labels\"] = []  # Assuming no labels for incorrect predictions\n","        data_entries.append(current_entry)\n","        current_entry = {}\n","    elif line.startswith(\"Labels: \"):  # Process labels\n","        labels_str = line.split(\": \")[1]\n","        labels = [int(label_str) for label_str in re.findall(r'\\d+', labels_str)]\n","        current_entry[\"labels\"] = labels\n","        data_entries.append(current_entry)\n","        current_entry = {}\n","    elif line.startswith(\"Original_Labels: \"):  # Process original labels\n","        labels_str = line.split(\": \")[1]\n","        labels = [int(label_str) for label_str in re.findall(r'\\d+', labels_str)]\n","        current_entry[\"original_labels\"] = labels\n","        data_entries.append(current_entry)\n","        current_entry = {}\n","\n","# Save the data entries to a JSON file\n","output_filename = \"incorrect_predictions_dataset.json\"\n","with open(output_filename, \"w\") as json_file:\n","    json.dump(data_entries, json_file, indent=4)\n","\n","print(f\"Converted incorrect predictions saved to {output_filename}\")\n"],"metadata":{"id":"PgV0IoIu0HOc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["incorrect_predictions = []\n","with open(\"incorrect_predictions.txt\", \"r\") as f:\n","    lines = f.readlines()\n","    for line in lines:\n","        parts = line.strip().split(\", \")\n","        pred = eval(parts[0].replace(\"Predicted: \", \"\"))\n","        actual = int(parts[1].replace(\"Actual: \", \"\"))\n","        incorrect_predictions.append({\"predicted\": pred, \"actual\": actual})\n","\n","# Save incorrect predictions to a JSON file\n","with open(\"incorrect_predictions_dataset.json\", \"w\") as json_file:\n","    json.dump(incorrect_predictions, json_file, indent=4)"],"metadata":{"id":"ixca-CNl6XtE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","from datasets import Dataset\n","\n","import pandas as pd\n","\n","# Load JSON data from file\n","file_path = '/content/incorrect_predictions_dataset.json'\n","with open(file_path, 'r') as json_file:\n","    data = json.load(json_file)\n","\n","# Create a dictionary to store original_labels\n","labels_dict = {}\n","\n","# Iterate through the data and extract original_labels\n","for entry in data:\n","    if \"original_labels\" in entry:\n","        original_labels = entry[\"original_labels\"]\n","        if \"Index\" in entry:\n","            index = entry[\"Index\"]\n","            labels_dict[index] = original_labels\n","        elif len(labels_dict) > 0:\n","            last_index = max(labels_dict.keys())\n","            labels_dict[last_index].extend(original_labels)\n","\n","# Initialize lists to store texts and labels\n","texts = []\n","text= []\n","labels = label_lists[0:173]\n","print(\"#####\")\n","print(len(label_lists))\n","# Iterate through the data and extract relevant information\n","for entry in data:\n","    if \"text\" in entry:\n","        text = entry[\"text\"]\n","        if \"Index\" in entry:\n","            index = entry[\"Index\"]\n","            #labels.append(labels_dict.get(index, []))\n","        else:\n","            # Use the last index available in labels_dict\n","            last_index = max(labels_dict.keys())\n","            #labels.append(labels_dict.get(last_index, []))\n","        texts.append(text)\n","\n","print(\"Number of Texts:\", len(texts))\n","print(\"Number of Labels:\", len(labels))\n","from datasets import load_dataset\n","existing_dataset = load_dataset(\"lex_glue\", 'unfair_tos')\n","#dataset=dataset_dict['train']\n","#print(\"Number of Texts:\", len(existing_dataset['text']))\n","#print(\"Number of Labels:\", len(existing_dataset['text']))\n","#aligned_data = [{\"text\": text, \"labels\": label} for text, label in zip(texts, labels)]\n","\n","# Create a new Dataset from the aligned data\n","#new_dataset = Dataset.from_dict(aligned_data)\n","\n","aligned_data = {\"text\": texts, \"labels\": labels}\n","df = pd.DataFrame(aligned_data)\n","\n","# Create a new Dataset from the DataFrame\n","new_dataset = Dataset.from_pandas(df)\n","\n","#merged_dataset = existing_dataset[\"train\"].concatenate(new_dataset)\n","#text= existing_dataset[\"train\"][\"text\"] + texts\n","#labels= existing_dataset[\"train\"][\"labels\"] + labels\n","merged_dataset = Dataset.from_dict({\n","    \"text\": existing_dataset[\"train\"][\"text\"] + texts,\n","    \"labels\": existing_dataset[\"train\"][\"labels\"] + labels,\n","})\n","#existing_dataset[\"train\"][\"text\"] += texts\n","#existing_dataset[\"train\"][\"labels\"] += labels\n","print(\"Number of Texts:\", len(merged_dataset['text']))\n","print(\"Number of Labels:\", len(merged_dataset['labels']))\n"],"metadata":{"id":"GYDMqDqQ0Kfw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!/usr/bin/env python\n","# coding=utf-8\n","\"\"\" Finetuning models on CaseHOLD (e.g. Bert, RoBERTa, LEGAL-BERT).\"\"\"\n","\n","import logging\n","import os\n","from dataclasses import dataclass, field\n","from typing import Optional\n","from sklearn.model_selection import ParameterGrid\n","import numpy as np\n","import random\n","import shutil\n","import glob\n","import os\n","\n","import transformers\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","\t\tAutoModelForMultipleChoice,\n","    DataCollatorWithPadding,\n","    EvalPrediction,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    default_data_collator,\n","    set_seed,\n","    EarlyStoppingCallback,\n","    Trainer\n",")\n","from transformers.trainer_utils import is_main_process\n","from transformers import EarlyStoppingCallback\n","# from casehold_helpers import MultipleChoiceDataset, Split\n","from sklearn.metrics import f1_score\n","# from models.deberta import DebertaForMultipleChoice\n","\n","logger = logging.getLogger(__name__)\n","\n","param_grid = {\n","    'learning_rate': [1e-5, 2e-5],  # Learning rates to try\n","    'num_train_epochs': [1, 2],        # Number of training epochs to try\n","    'per_device_train_batch_size': [2, 4],  # Batch sizes for training\n","    'per_device_eval_batch_size': [2, 4],   # Batch sizes for evaluation\n","}\n","\n","\n","@dataclass\n","class ModelArguments:\n","    \"\"\"\n","    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n","    \"\"\"\n","\n","    model_name_or_path: str = field(\n","        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n","    )\n","    config_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n","    )\n","    tokenizer_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n","    )\n","    cache_dir: Optional[str] = field(\n","        default=None,\n","        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n","    )\n","\n","\n","@dataclass\n","class DataTrainingArguments:\n","    \"\"\"\n","    Arguments pertaining to what data we are going to input our model for training and eval.\n","    \"\"\"\n","\n","    task_name: str = field(default=\"case_hold\", metadata={\"help\": \"The name of the task to train on\"})\n","    max_seq_length: int = field(\n","        default=256,\n","        metadata={\n","            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n","            \"than this will be truncated, sequences shorter will be padded.\"\n","        },\n","    )\n","    pad_to_max_length: bool = field(\n","        default=True,\n","        metadata={\n","            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n","            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n","        },\n","    )\n","    max_train_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_eval_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_predict_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    overwrite_cache: bool = field(\n","        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n","    )\n","\n","\n","def main(training_args):\n","    # See all possible arguments in src/transformers/training_args.py\n","    # or by passing the --help flag to this script.\n","    # We now keep distinct sets of args, for a cleaner separation of concerns.\n","    data_args = DataTrainingArguments(\n","        max_seq_length=128,\n","        # max_segments=64,\n","        # max_seg_length=128,\n","        overwrite_cache=False,\n","        pad_to_max_length=True,\n","    )\n","    model_args = ModelArguments(\n","        model_name_or_path=\"microsoft/deberta-base\",\n","        # hierarchical=True,\n","        #do_lower_case=True,\n","        #use_fast_tokenizer=True,\n","    )\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n","    )\n","    logger.warning(\n","        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n","        training_args.local_rank,\n","        training_args.device,\n","        training_args.n_gpu,\n","        bool(training_args.local_rank != -1),\n","        training_args.fp16,\n","    )\n","    # Set the verbosity to info of the Transformers logger (on main process only):\n","    if is_main_process(training_args.local_rank):\n","        transformers.utils.logging.set_verbosity_info()\n","        transformers.utils.logging.enable_default_handler()\n","        transformers.utils.logging.enable_explicit_format()\n","    logger.info(\"Training/evaluation parameters %s\", training_args)\n","\n","    # Set seed\n","    set_seed(training_args.seed)\n","\n","    # Load pretrained model and tokenizer\n","    config = AutoConfig.from_pretrained(\n","        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n","        num_labels=5,\n","        finetuning_task=data_args.task_name,\n","        cache_dir=model_args.cache_dir,\n","    )\n","\n","    if config.model_type == 'big_bird':\n","        config.attention_type = 'original_full'\n","    elif config.model_type == 'longformer':\n","        config.attention_window = [data_args.max_seq_length] * config.num_hidden_layers\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n","        cache_dir=model_args.cache_dir,\n","        # Default fast tokenizer is buggy on CaseHOLD task, switch to legacy tokenizer\n","        use_fast=True,\n","    )\n","\n","    if config.model_type != 'deberta':\n","        model = AutoModelForMultipleChoice.from_pretrained(\n","            model_args.model_name_or_path,\n","            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","            config=config,\n","            cache_dir=model_args.cache_dir,\n","        )\n","    else:\n","        model = DebertaForMultipleChoice.from_pretrained(\n","            model_args.model_name_or_path,\n","            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","            config=config,\n","            cache_dir=model_args.cache_dir,\n","        )\n","\n","    train_dataset = None\n","    eval_dataset = None\n","\n","    # If do_train passed, train_dataset by default loads train split from file named train.csv in data directory\n","    if training_args.do_train:\n","        train_dataset = \\\n","            MultipleChoiceDataset(\n","                tokenizer=tokenizer,\n","                task=data_args.task_name,\n","                max_seq_length=data_args.max_seq_length,\n","                overwrite_cache=data_args.overwrite_cache,\n","                mode=Split.train,\n","            )\n","\n","    # If do_eval or do_predict passed, eval_dataset by default loads dev split from file named dev.csv in data directory\n","    if training_args.do_eval:\n","        eval_dataset = \\\n","            MultipleChoiceDataset(\n","                tokenizer=tokenizer,\n","                task=data_args.task_name,\n","                max_seq_length=data_args.max_seq_length,\n","                overwrite_cache=data_args.overwrite_cache,\n","                mode=Split.dev,\n","            )\n","\n","    if training_args.do_predict:\n","        predict_dataset = \\\n","            MultipleChoiceDataset(\n","                tokenizer=tokenizer,\n","                task=data_args.task_name,\n","                max_seq_length=data_args.max_seq_length,\n","                overwrite_cache=data_args.overwrite_cache,\n","                mode=Split.test,\n","            )\n","\n","    if training_args.do_train:\n","        if data_args.max_train_samples is not None:\n","            train_dataset = train_dataset[:data_args.max_train_samples]\n","        # Log a few random samples from the training set:\n","        for index in random.sample(range(len(train_dataset)), 3):\n","            logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n","\n","    if training_args.do_eval:\n","        if data_args.max_eval_samples is not None:\n","            eval_dataset = eval_dataset[:data_args.max_eval_samples]\n","\n","    if training_args.do_predict:\n","        if data_args.max_predict_samples is not None:\n","            predict_dataset = predict_dataset[:data_args.max_predict_samples]\n","\n","    # Define custom compute_metrics function, returns macro F1 metric for CaseHOLD task\n","    def compute_metrics(p: EvalPrediction):\n","        preds = np.argmax(p.predictions, axis=1)\n","        # Compute macro and micro F1 for 5-class CaseHOLD task\n","        macro_f1 = f1_score(y_true=p.label_ids, y_pred=preds, average='macro', zero_division=0)\n","        micro_f1 = f1_score(y_true=p.label_ids, y_pred=preds, average='micro', zero_division=0)\n","        return {'macro-f1': macro_f1, 'micro-f1': micro_f1}\n","\n","    # Initialize our Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=eval_dataset,\n","        compute_metrics=compute_metrics,\n","        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n","    )\n","\n","    # Training\n","    if training_args.do_train:\n","        trainer.train(\n","            model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n","        )\n","        trainer.save_model()\n","        # Re-save the tokenizer for model sharing\n","        if trainer.is_world_process_zero():\n","            tokenizer.save_pretrained(training_args.output_dir)\n","\n","    # Evaluation on eval_dataset\n","    if training_args.do_eval:\n","        logger.info(\"*** Evaluate ***\")\n","        metrics = trainer.evaluate(eval_dataset=eval_dataset)\n","\n","        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n","        metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n","\n","        trainer.log_metrics(\"eval\", metrics)\n","        trainer.save_metrics(\"eval\", metrics)\n","\n","    # Predict on eval_dataset\n","    if training_args.do_predict:\n","        logger.info(\"*** Predict ***\")\n","\n","        predictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix=\"predict\")\n","\n","        max_predict_samples = (\n","            data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)\n","        )\n","        metrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n","\n","        trainer.log_metrics(\"predict\", metrics)\n","        trainer.save_metrics(\"predict\", metrics)\n","\n","        output_predict_file = os.path.join(training_args.output_dir, \"test_predictions.csv\")\n","        if trainer.is_world_process_zero():\n","            with open(output_predict_file, \"w\") as writer:\n","                for index, pred_list in enumerate(predictions):\n","                    pred_line = '\\t'.join([f'{pred:.5f}' for pred in pred_list])\n","                    writer.write(f\"{index}\\t{pred_line}\\n\")\n","\n","\n","# Print the best hyperparameters and its corresponding validation macro-f1 score\n","\n","    # Clean up checkpoints\n","    checkpoints = [filepath for filepath in glob.glob(f'{training_args.output_dir}/*/') if '/checkpoint' in filepath]\n","    for checkpoint in checkpoints:\n","        shutil.rmtree(checkpoint)\n","\n","\n","# def _mp_fn(index):\n","# For xla_spawn (TPUs)\n","# main()\n","\n","\n","if __name__ == \"__main__\":\n","    # main()\n","\n","    training_args = TrainingArguments(\n","        do_train=True,\n","        do_eval=True,\n","        do_predict=True,\n","        output_dir=os.getcwd(),\n","        overwrite_output_dir=True,\n","        num_train_epochs=10,\n","        per_device_train_batch_size=8,\n","        per_device_eval_batch_size=8,\n","        save_steps=500,\n","        save_total_limit=2,\n","        fp16=False,\n","        logging_dir=\"./logs\",\n","        logging_steps=200,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=500,\n","        logging_first_step=False,\n","        load_best_model_at_end=True,\n","        metric_for_best_model=\"micro-f1\",\n","    )\n","    main(training_args)\n","\t\t # Train the model\n","    #train_result = trainer.train()\n","    #metrics = train_result.metrics\n","\n","    # Evaluate the model\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ADsQU_27ztbY","executionInfo":{"status":"error","timestamp":1691814758087,"user_tz":-60,"elapsed":179761,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"}},"outputId":"5a288ef0-20ca-4eab-91b5-da5187f4875a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[INFO|training_args.py:1299] 2023-08-12 04:29:38,078 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n","[INFO|training_args.py:1713] 2023-08-12 04:29:38,079 >> PyTorch: setting up devices\n","[INFO|training_args.py:1439] 2023-08-12 04:29:38,081 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False\n","[INFO|configuration_utils.py:712] 2023-08-12 04:29:38,302 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json\n","[INFO|configuration_utils.py:768] 2023-08-12 04:29:38,304 >> Model config DebertaConfig {\n","  \"_name_or_path\": \"microsoft/deberta-base\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"case_hold\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"c2p\",\n","    \"p2c\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"relative_attention\": true,\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|configuration_utils.py:712] 2023-08-12 04:29:38,566 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json\n","[INFO|configuration_utils.py:768] 2023-08-12 04:29:38,568 >> Model config DebertaConfig {\n","  \"_name_or_path\": \"microsoft/deberta-base\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"c2p\",\n","    \"p2c\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"relative_attention\": true,\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|tokenization_utils_base.py:1839] 2023-08-12 04:29:38,572 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json\n","[INFO|tokenization_utils_base.py:1839] 2023-08-12 04:29:38,573 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt\n","[INFO|tokenization_utils_base.py:1839] 2023-08-12 04:29:38,574 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:1839] 2023-08-12 04:29:38,575 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1839] 2023-08-12 04:29:38,576 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1839] 2023-08-12 04:29:38,578 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json\n","[INFO|configuration_utils.py:712] 2023-08-12 04:29:38,583 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json\n","[INFO|configuration_utils.py:768] 2023-08-12 04:29:38,585 >> Model config DebertaConfig {\n","  \"_name_or_path\": \"microsoft/deberta-base\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"c2p\",\n","    \"p2c\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"relative_attention\": true,\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|configuration_utils.py:712] 2023-08-12 04:29:38,667 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json\n","[INFO|configuration_utils.py:768] 2023-08-12 04:29:38,669 >> Model config DebertaConfig {\n","  \"_name_or_path\": \"microsoft/deberta-base\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"c2p\",\n","    \"p2c\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"relative_attention\": true,\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|modeling_utils.py:2603] 2023-08-12 04:29:38,764 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin\n","[INFO|modeling_utils.py:3319] 2023-08-12 04:29:40,447 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForMultipleChoice: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:3331] 2023-08-12 04:29:40,448 >> Some weights of DebertaForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'pooler.dense.weight', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1500: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","[INFO|trainer.py:1686] 2023-08-12 04:30:00,887 >> ***** Running training *****\n","[INFO|trainer.py:1687] 2023-08-12 04:30:00,888 >>   Num examples = 45,000\n","[INFO|trainer.py:1688] 2023-08-12 04:30:00,889 >>   Num Epochs = 10\n","[INFO|trainer.py:1689] 2023-08-12 04:30:00,890 >>   Instantaneous batch size per device = 8\n","[INFO|trainer.py:1692] 2023-08-12 04:30:00,891 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:1693] 2023-08-12 04:30:00,894 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1694] 2023-08-12 04:30:00,895 >>   Total optimization steps = 56,250\n","[INFO|trainer.py:1695] 2023-08-12 04:30:00,897 >>   Number of trainable parameters = 139,193,089\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='469' max='56250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [  469/56250 02:36 < 5:11:40, 2.98 it/s, Epoch 0.08/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-711b20fde82c>\u001b[0m in \u001b[0;36m<cell line: 315>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mmetric_for_best_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"micro-f1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     )\n\u001b[0;32m--> 338\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m                  \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;31m#train_result = trainer.train()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-711b20fde82c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(training_args)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtraining_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         trainer.train(\n\u001b[0m\u001b[1;32m    262\u001b[0m             \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m         )\n\u001b[0;32m-> 1539\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1540\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1809\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2654\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2677\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2678\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2679\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2680\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-cea87a31f5a5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    259\u001b[0m         )\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         outputs = self.deberta(\n\u001b[0m\u001b[1;32m    262\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    981\u001b[0m         )\n\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    984\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    474\u001b[0m                 )\n\u001b[1;32m    475\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m                 hidden_states = layer_module(\n\u001b[0m\u001b[1;32m    477\u001b[0m                     \u001b[0mnext_kv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     ):\n\u001b[0;32m--> 383\u001b[0;31m         attention_output = self.attention(\n\u001b[0m\u001b[1;32m    384\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     ):\n\u001b[0;32m--> 316\u001b[0;31m         self_output = self.self(\n\u001b[0m\u001b[1;32m    317\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelative_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0mrel_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             \u001b[0mrel_att\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisentangled_att_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrel_att\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mdisentangled_att_bias\u001b[0;34m(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0mc2p_att\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_key_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mc2p_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelative_pos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0matt_span\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt_span\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mc2p_att\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc2p_att\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc2p_dynamic_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc2p_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc2p_att\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}