{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V100","authorship_tag":"ABX9TyMmOIqsOwFpg7eb8IDa8aEE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"6392d7e75dcf4f5e80566b3fee3c8787":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_540d11b9a35e4af0a4fe5321a8da5e60","IPY_MODEL_b01f9cde0be14d83ad1cda2944536fbe","IPY_MODEL_a73a83965f7f4492b78487e6466be180"],"layout":"IPY_MODEL_f6d74d9cde6e49cbab9d3476339525e6"}},"540d11b9a35e4af0a4fe5321a8da5e60":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_12c55e2bfcb049c08ed818f3edae7f79","placeholder":"​","style":"IPY_MODEL_d3994877e600402eaa896faabf9a00d0","value":"Downloading (…)lve/main/config.json: 100%"}},"b01f9cde0be14d83ad1cda2944536fbe":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0583a870d5f74f6096970167d8fc4f87","max":481,"min":0,"orientation":"horizontal","style":"IPY_MODEL_71100575fd914c26af3c2dfe46e2dd5d","value":481}},"a73a83965f7f4492b78487e6466be180":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e20e2d94495943209cd8d864070251e5","placeholder":"​","style":"IPY_MODEL_d09c52d630094838b283a47c8da0a14e","value":" 481/481 [00:00&lt;00:00, 41.3kB/s]"}},"f6d74d9cde6e49cbab9d3476339525e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12c55e2bfcb049c08ed818f3edae7f79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3994877e600402eaa896faabf9a00d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0583a870d5f74f6096970167d8fc4f87":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71100575fd914c26af3c2dfe46e2dd5d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e20e2d94495943209cd8d864070251e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d09c52d630094838b283a47c8da0a14e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4e1f3ae1c49d4ff585a610de7ef1a695":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d543c54b07ca4e8aa733df10836eafef","IPY_MODEL_d5376e53289141a08a84ff7e8be8c2bb","IPY_MODEL_17f90bb64d5d41e5a56096e988880029"],"layout":"IPY_MODEL_ec8c1acb576f4f068f7452dc6e696d94"}},"d543c54b07ca4e8aa733df10836eafef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1be118695e82484aa48c9fa21d3b4a0f","placeholder":"​","style":"IPY_MODEL_1e6587eba3794099901faab3f0cfb05d","value":"Downloading (…)olve/main/vocab.json: 100%"}},"d5376e53289141a08a84ff7e8be8c2bb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb84e72157cc4095bd28ba4321f1b8de","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ff9ca10fec454eab8ab48be6e6ffe5fb","value":898823}},"17f90bb64d5d41e5a56096e988880029":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_71c08b5623ab477b97e9786c985a3e4f","placeholder":"​","style":"IPY_MODEL_4eb9c158a32c4cb8b3f92e0c2197a0fa","value":" 899k/899k [00:00&lt;00:00, 2.74MB/s]"}},"ec8c1acb576f4f068f7452dc6e696d94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1be118695e82484aa48c9fa21d3b4a0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e6587eba3794099901faab3f0cfb05d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bb84e72157cc4095bd28ba4321f1b8de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff9ca10fec454eab8ab48be6e6ffe5fb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"71c08b5623ab477b97e9786c985a3e4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4eb9c158a32c4cb8b3f92e0c2197a0fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cde17784f2a44d5e8d75fdf2e4485c62":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_03d7cd7d0890447ebf76d849b4c36661","IPY_MODEL_9cdac6edde0c4c8d906359c69aa02369","IPY_MODEL_ae27bcaaa34042a4abc02194bab25c1f"],"layout":"IPY_MODEL_d3d70c3e0174401884f29bd2389a2aaf"}},"03d7cd7d0890447ebf76d849b4c36661":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_28b5e8e2104144579e935f49d5d875f8","placeholder":"​","style":"IPY_MODEL_ea88c83caba8445c90367f17bda487c3","value":"Downloading (…)olve/main/merges.txt: 100%"}},"9cdac6edde0c4c8d906359c69aa02369":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0d300ebcb320429d9b1e90dccec5b08d","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9247f33810254a18bcfb64ac7fad1943","value":456318}},"ae27bcaaa34042a4abc02194bab25c1f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d07df81ec87a4ce08efb218d3f2037f3","placeholder":"​","style":"IPY_MODEL_468d254e7a1e4debb862029937027349","value":" 456k/456k [00:00&lt;00:00, 1.88MB/s]"}},"d3d70c3e0174401884f29bd2389a2aaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28b5e8e2104144579e935f49d5d875f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea88c83caba8445c90367f17bda487c3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d300ebcb320429d9b1e90dccec5b08d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9247f33810254a18bcfb64ac7fad1943":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d07df81ec87a4ce08efb218d3f2037f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"468d254e7a1e4debb862029937027349":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e90ebc97cf7b46739cb9f2ba12b63391":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0b557e8cb84e4cae936b16a003c3421b","IPY_MODEL_d7fcf469db694d4595dc42ea238cb91d","IPY_MODEL_fc8b5b78d3c941d1b7ba645d10a2f073"],"layout":"IPY_MODEL_4a0c7f2cf3e44cc7b440bd025d176c1f"}},"0b557e8cb84e4cae936b16a003c3421b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_34f0a503565d4cb6bd50303d0e91fbb0","placeholder":"​","style":"IPY_MODEL_b7e1f5c290c449b7b6f3f8c99d163f32","value":"Downloading (…)/main/tokenizer.json: 100%"}},"d7fcf469db694d4595dc42ea238cb91d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_89bce85b7eec4ec0bb4f48d93ed20487","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_273da878f9564d1c907e578a4aa518e2","value":1355863}},"fc8b5b78d3c941d1b7ba645d10a2f073":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_916bd2cc36004284974c713a1642c221","placeholder":"​","style":"IPY_MODEL_4c66fa0a76ee44bdb4e2938d52a45774","value":" 1.36M/1.36M [00:00&lt;00:00, 15.6MB/s]"}},"4a0c7f2cf3e44cc7b440bd025d176c1f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34f0a503565d4cb6bd50303d0e91fbb0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7e1f5c290c449b7b6f3f8c99d163f32":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"89bce85b7eec4ec0bb4f48d93ed20487":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"273da878f9564d1c907e578a4aa518e2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"916bd2cc36004284974c713a1642c221":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c66fa0a76ee44bdb4e2938d52a45774":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8834d031367e4772b97e05f289fbb3d6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_81f9163a88a04607add1cb17f04b9f02","IPY_MODEL_d18074777a0e4ff08088029b019147b5","IPY_MODEL_25c3d4de007543d5a5c6b9a24fd4162d"],"layout":"IPY_MODEL_7a006a3f9c6540d493a1a6dd8cf25213"}},"81f9163a88a04607add1cb17f04b9f02":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3fbc252118f495db4a5af43a7057d01","placeholder":"​","style":"IPY_MODEL_be7795a5a6a44add882edefd19e7e8c5","value":"Downloading model.safetensors: 100%"}},"d18074777a0e4ff08088029b019147b5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a84952679f10418884f4a5e2837beae0","max":498818054,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a38f5473769c42b4b53b719b2bd53601","value":498818054}},"25c3d4de007543d5a5c6b9a24fd4162d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f1f4d9d6af94a5593430e2a3e29b147","placeholder":"​","style":"IPY_MODEL_eaba7585fa14459fafea3ce7409d6425","value":" 499M/499M [00:01&lt;00:00, 381MB/s]"}},"7a006a3f9c6540d493a1a6dd8cf25213":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3fbc252118f495db4a5af43a7057d01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be7795a5a6a44add882edefd19e7e8c5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a84952679f10418884f4a5e2837beae0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a38f5473769c42b4b53b719b2bd53601":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6f1f4d9d6af94a5593430e2a3e29b147":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eaba7585fa14459fafea3ce7409d6425":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"829d1c9ed60a4efcb5819fde119050f2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_be8897ca11fd4b9398d0ecc48b6fe678","IPY_MODEL_f783da9d82d04f5781c50214c176436d","IPY_MODEL_7c99ff4eb8c14b54a985c35124e7c190"],"layout":"IPY_MODEL_a20c79d8b89343f786803936b0a6e9ec"}},"be8897ca11fd4b9398d0ecc48b6fe678":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b394c0ee754141a28c660bd5ae5491e2","placeholder":"​","style":"IPY_MODEL_5dcc52e65494416db0c9232f41b0b647","value":"Running tokenizer on train dataset: 100%"}},"f783da9d82d04f5781c50214c176436d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7873fb7835f43c9a26fe4661e8c5e8c","max":5532,"min":0,"orientation":"horizontal","style":"IPY_MODEL_08053af3322b492898a29d711738bf97","value":5532}},"7c99ff4eb8c14b54a985c35124e7c190":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ac57609c84f48909998eed753b45f2e","placeholder":"​","style":"IPY_MODEL_057e2e261d8e48b085e6885390ba75bc","value":" 5532/5532 [00:00&lt;00:00, 11589.99 examples/s]"}},"a20c79d8b89343f786803936b0a6e9ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b394c0ee754141a28c660bd5ae5491e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5dcc52e65494416db0c9232f41b0b647":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d7873fb7835f43c9a26fe4661e8c5e8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08053af3322b492898a29d711738bf97":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4ac57609c84f48909998eed753b45f2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"057e2e261d8e48b085e6885390ba75bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a079f6f1c3d343b6a80ccb3f5d2ffacf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e1bb1138d48b4a448b7404ecc9e2f3b3","IPY_MODEL_b10fd229cfe4458b825d8e4f2f529087","IPY_MODEL_6b57828957854a33923743358c049329"],"layout":"IPY_MODEL_d7798ed500e94664b5587f1b61cbfee9"}},"e1bb1138d48b4a448b7404ecc9e2f3b3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8049295863014de6b982711c24088e2b","placeholder":"​","style":"IPY_MODEL_7493a400b3234748bfadf6abcb403aad","value":"Running tokenizer on validation dataset: 100%"}},"b10fd229cfe4458b825d8e4f2f529087":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_edef25b9dff5444aa15f5aee9330a04f","max":2275,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4d480e9360e644e1a3335773085d3c0a","value":2275}},"6b57828957854a33923743358c049329":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5cc4e74345247eabb00f2c77407e0eb","placeholder":"​","style":"IPY_MODEL_c65b320b8da348f7abf6ab443e9d0174","value":" 2275/2275 [00:00&lt;00:00, 11345.09 examples/s]"}},"d7798ed500e94664b5587f1b61cbfee9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8049295863014de6b982711c24088e2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7493a400b3234748bfadf6abcb403aad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"edef25b9dff5444aa15f5aee9330a04f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d480e9360e644e1a3335773085d3c0a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d5cc4e74345247eabb00f2c77407e0eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c65b320b8da348f7abf6ab443e9d0174":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eaf34267ac3841fca9022a097ff67351":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d0b4cd69179d4d17a824e3609f188002","IPY_MODEL_0c173efb2dca4d50b118e62b67fd8d1b","IPY_MODEL_1a654a545df3420ca67b08df1296241f"],"layout":"IPY_MODEL_265d428cfca74bcb9910e22e616acaed"}},"d0b4cd69179d4d17a824e3609f188002":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d054549f965f48c6aefdf83b46ec965d","placeholder":"​","style":"IPY_MODEL_d4433026bcbd412f93a7160976146f43","value":"Running tokenizer on prediction dataset: 100%"}},"0c173efb2dca4d50b118e62b67fd8d1b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_517fdc876c554cac9f18032dbdd0e3b1","max":1607,"min":0,"orientation":"horizontal","style":"IPY_MODEL_50dc58d2e8034e3188dd87a6afbf779b","value":1607}},"1a654a545df3420ca67b08df1296241f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_25c91cbeae64411b8c425f29fc4066c0","placeholder":"​","style":"IPY_MODEL_948d00a43da24996a9e490dd0ee9f0a8","value":" 1607/1607 [00:00&lt;00:00, 9875.81 examples/s]"}},"265d428cfca74bcb9910e22e616acaed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d054549f965f48c6aefdf83b46ec965d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4433026bcbd412f93a7160976146f43":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"517fdc876c554cac9f18032dbdd0e3b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50dc58d2e8034e3188dd87a6afbf779b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"25c91cbeae64411b8c425f29fc4066c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"948d00a43da24996a9e490dd0ee9f0a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"34e6c1905b004a21959effcb22d9c4a6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4d053e2bb00548eeae9dfe8e6d7e3d75","IPY_MODEL_a054c1f33b8c47bbb32da60e77ba8c47","IPY_MODEL_92a5094521fa4f59acda84da899b3aa5"],"layout":"IPY_MODEL_49581b088f9344d6b499c7b8de63fb89"}},"4d053e2bb00548eeae9dfe8e6d7e3d75":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1fde492d5274cb08f08a2b438721831","placeholder":"​","style":"IPY_MODEL_9e4c6cd6a3b348dfad2484c3cfc44c96","value":"Filter: 100%"}},"a054c1f33b8c47bbb32da60e77ba8c47":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d29ee12b761449cebc9b87110f89b04a","max":1607,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7061c293e9604289b3af179a7e47e93d","value":1607}},"92a5094521fa4f59acda84da899b3aa5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_95c3c447036f4fbc90de8b1eb7191bc6","placeholder":"​","style":"IPY_MODEL_e818e55da2184505b7ba03aa2bfaab4c","value":" 1607/1607 [00:00&lt;00:00, 7155.34 examples/s]"}},"49581b088f9344d6b499c7b8de63fb89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1fde492d5274cb08f08a2b438721831":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e4c6cd6a3b348dfad2484c3cfc44c96":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d29ee12b761449cebc9b87110f89b04a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7061c293e9604289b3af179a7e47e93d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"95c3c447036f4fbc90de8b1eb7191bc6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e818e55da2184505b7ba03aa2bfaab4c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd5683abf8de40d18bd9cb8bb7d66b08":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0e03aec243784de5b7bc5f7649204a94","IPY_MODEL_e448e8af33c54b299a2eeb71b9765efe","IPY_MODEL_bc8d72f367494c8e837595ba9c7e47c3"],"layout":"IPY_MODEL_5b795445cb654657b4e03c0a10cb67f1"}},"0e03aec243784de5b7bc5f7649204a94":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_464293ffda9c4c10987c548beac9f167","placeholder":"​","style":"IPY_MODEL_f9cc090612b84ca5b8f40ee3cc72320b","value":"Filter: 100%"}},"e448e8af33c54b299a2eeb71b9765efe":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae05fad7fcf14dc8aa973e12fd25a179","max":1607,"min":0,"orientation":"horizontal","style":"IPY_MODEL_86d940e097d449e7bbb664cdb759919d","value":1607}},"bc8d72f367494c8e837595ba9c7e47c3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3dd32f24918d42e49bae9dd2352147e2","placeholder":"​","style":"IPY_MODEL_f4ff43ea30b14b22b9c03c48c4bb5abc","value":" 1607/1607 [00:00&lt;00:00, 6926.86 examples/s]"}},"5b795445cb654657b4e03c0a10cb67f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"464293ffda9c4c10987c548beac9f167":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9cc090612b84ca5b8f40ee3cc72320b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ae05fad7fcf14dc8aa973e12fd25a179":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86d940e097d449e7bbb664cdb759919d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3dd32f24918d42e49bae9dd2352147e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4ff43ea30b14b22b9c03c48c4bb5abc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dab03257b65446c8b22f7bfe24b3a1bd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c74ed66826b74ce7ad30150cf570fff5","IPY_MODEL_ad86697e6a144f6bb8b139531d56fa3e","IPY_MODEL_50a834b6544e4ca3bb686f8470816a7a"],"layout":"IPY_MODEL_5fd98b2874004dc1bd72f94ee375a2b7"}},"c74ed66826b74ce7ad30150cf570fff5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e0b4cf9f95d462babaffe261c7a4300","placeholder":"​","style":"IPY_MODEL_407de26ae8a345a1a2898f4d95d122ad","value":"Running tokenizer on train dataset: 100%"}},"ad86697e6a144f6bb8b139531d56fa3e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_557dd6e5dea34f7aaab2ba8b96da820a","max":5704,"min":0,"orientation":"horizontal","style":"IPY_MODEL_673d360d952b46eb88fabec47194c01a","value":5704}},"50a834b6544e4ca3bb686f8470816a7a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b3371a5ec74443c8c49f2a33f6ec0d2","placeholder":"​","style":"IPY_MODEL_0e8c7e57b22546488dc9bc1380d994d3","value":" 5704/5704 [00:00&lt;00:00, 11133.80 examples/s]"}},"5fd98b2874004dc1bd72f94ee375a2b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e0b4cf9f95d462babaffe261c7a4300":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"407de26ae8a345a1a2898f4d95d122ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"557dd6e5dea34f7aaab2ba8b96da820a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"673d360d952b46eb88fabec47194c01a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2b3371a5ec74443c8c49f2a33f6ec0d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e8c7e57b22546488dc9bc1380d994d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b0adb6c5d0b4135bda36ae3c7a9778d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4ead2138e79c40449cb5d2c0f87263bf","IPY_MODEL_780bb05d43da403db1b1983586b623eb","IPY_MODEL_fa577dbc66f74ba389447df79507fa58"],"layout":"IPY_MODEL_b47708a9f77048778dc40ff916279238"}},"4ead2138e79c40449cb5d2c0f87263bf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0835fa06d227476082942d7311109980","placeholder":"​","style":"IPY_MODEL_d0c210847a0740f59ff4237082bb342d","value":"Running tokenizer on train dataset: 100%"}},"780bb05d43da403db1b1983586b623eb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc1dbdb020cf4c418a33f27711e074a4","max":5704,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dc3e2d364bef4161877e4f1d4e1246f6","value":5704}},"fa577dbc66f74ba389447df79507fa58":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_83e135d7969847cbb21d4305cbcdcaed","placeholder":"​","style":"IPY_MODEL_0192dbf0c4b7475c915c411b43258f0f","value":" 5704/5704 [00:00&lt;00:00, 10701.39 examples/s]"}},"b47708a9f77048778dc40ff916279238":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0835fa06d227476082942d7311109980":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0c210847a0740f59ff4237082bb342d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fc1dbdb020cf4c418a33f27711e074a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc3e2d364bef4161877e4f1d4e1246f6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"83e135d7969847cbb21d4305cbcdcaed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0192dbf0c4b7475c915c411b43258f0f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"mrsMapuwB_lG"}},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xsgShwXRodPe","executionInfo":{"status":"ok","timestamp":1693761561154,"user_tz":-60,"elapsed":39278,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"}},"outputId":"5105c90d-7abc-4711-a965-3fcee915d89b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.32.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.10.1)\n","Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.23.5)\n","Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.32.1)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.22.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.3.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n","Requirement already satisfied: torch!=1.12.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (16.0.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n"]}],"source":["! pip install torch\n","! pip install transformers\n","! pip install scikit-learn\n","! pip install tqdm\n","! pip install numpy\n","! pip install datasets\n","! pip install nltk\n","import nltk\n","nltk.download('stopwords')\n","! pip install scipy\n","! pip install transformers[torch] accelerate\n"]},{"cell_type":"code","source":["from datasets import load_dataset\n","dataset_dict = load_dataset(\"lex_glue\", 'unfair_tos')\n","#print(dataset)\n","#Divide into train,dev,test\n","\n","from sklearn.model_selection import train_test_split\n","\n","#data_list = list(dataset_dict.items())\n","\n","train_set_dict = dataset_dict['test']\n","print(train_set_dict['labels'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oDo5tR3D7O6t","executionInfo":{"status":"ok","timestamp":1693761562688,"user_tz":-60,"elapsed":1542,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"}},"outputId":"8c54a16d-c3a3-4c3d-cc29-659e6ad971f2"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["[[], [], [], [], [7], [], [], [], [], [], [], [], [4], [], [], [2], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [2, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3, 1], [], [1], [], [], [], [], [], [1], [1], [1], [], [], [], [], [], [], [], [], [], [], [], [], [0], [0], [], [], [], [], [5], [], [], [], [], [], [], [], [7], [], [], [], [6], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [3, 1], [], [], [], [3], [], [], [], [3], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [5, 4], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [5], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3, 1], [], [], [2], [], [], [], [], [], [], [], [], [], [2, 1], [2], [], [1], [], [1], [1], [], [], [], [], [], [], [0], [], [0], [0], [], [], [], [], [], [], [], [6], [6], [5], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [4], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [1], [2], [3, 0], [2], [1], [2], [3, 0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [0], [0], [], [2], [2], [2], [4], [], [], [], [], [], [], [], [], [], [], [], [4], [], [4], [], [], [], [], [], [], [], [], [], [], [2], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2, 1], [], [1], [], [], [], [], [], [], [], [], [0], [0], [], [], [], [], [], [0], [], [2], [2], [2, 1], [], [], [], [], [], [], [0], [], [], [0], [0], [], [], [1], [1], [], [], [], [], [], [], [], [5, 6], [5], [6], [5], [6], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [2], [], [4], [], [], [], [], [2, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [2], [0], [], [], [], [2], [], [], [], [], [], [0], [0], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [0], [0], [], [], [1], [1], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [5], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [6], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [1], [], [], [], [2], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [0], [0], [], [0], [0], [], [], [5], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [4], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [2], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [2], [], [2], [], [0], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [0], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [5], [5], [5], [], [], [], [], [], [], [], [], [], [6], [6], [6], [6], [6], [6], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [3, 1], [], [], [1], [1], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [0], [], [0], [], [], [], [], [], [], [5], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [6], [6], [], [], [], [], [], [2], [2], [], [], [4], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [6]]\n"]}]},{"cell_type":"code","source":["from dataclasses import dataclass\n","from typing import Optional, Tuple\n","\n","import torch\n","import numpy as np\n","from torch import nn\n","from transformers.file_utils import ModelOutput\n","\n","\n","@dataclass\n","class SimpleOutput(ModelOutput):\n","    last_hidden_state: torch.FloatTensor = None\n","    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n","    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n","    attentions: Optional[Tuple[torch.FloatTensor]] = None\n","    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n","\n","\n","def sinusoidal_init(num_embeddings: int, embedding_dim: int):\n","    # keep dim 0 for padding token position encoding zero vector\n","    position_enc = np.array([\n","        [pos / np.power(10000, 2 * i / embedding_dim) for i in range(embedding_dim)]\n","        if pos != 0 else np.zeros(embedding_dim) for pos in range(num_embeddings)])\n","\n","    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2])  # dim 2i\n","    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2])  # dim 2i+1\n","    return torch.from_numpy(position_enc).type(torch.FloatTensor)\n","\n","\n","class HierarchicalBert(nn.Module):\n","\n","    def __init__(self, encoder, max_segments=64, max_segment_length=128):\n","        super(HierarchicalBert, self).__init__()\n","        supported_models = ['bert', 'roberta', 'deberta']\n","        assert encoder.config.model_type in supported_models  # other model types are not supported so far\n","        # Pre-trained segment (token-wise) encoder, e.g., BERT\n","        self.encoder = encoder\n","        # Specs for the segment-wise encoder\n","        self.hidden_size = encoder.config.hidden_size\n","        self.max_segments = max_segments\n","        self.max_segment_length = max_segment_length\n","        # Init sinusoidal positional embeddings\n","        self.seg_pos_embeddings = nn.Embedding(max_segments + 1, encoder.config.hidden_size,\n","                                               padding_idx=0,\n","                                               _weight=sinusoidal_init(max_segments + 1, encoder.config.hidden_size))\n","        # Init segment-wise transformer-based encoder\n","        self.seg_encoder = nn.Transformer(d_model=encoder.config.hidden_size,\n","                                          nhead=encoder.config.num_attention_heads,\n","                                          batch_first=True, dim_feedforward=encoder.config.intermediate_size,\n","                                          activation=encoder.config.hidden_act,\n","                                          dropout=encoder.config.hidden_dropout_prob,\n","                                          layer_norm_eps=encoder.config.layer_norm_eps,\n","                                          num_encoder_layers=2, num_decoder_layers=0).encoder\n","\n","    def forward(self,\n","                input_ids=None,\n","                attention_mask=None,\n","                token_type_ids=None,\n","                position_ids=None,\n","                head_mask=None,\n","                inputs_embeds=None,\n","                labels=None,\n","                output_attentions=None,\n","                output_hidden_states=None,\n","                return_dict=None,\n","                ):\n","        # Hypothetical Example\n","        # Batch of 4 documents: (batch_size, n_segments, max_segment_length) --> (4, 64, 128)\n","        # BERT-BASE encoder: 768 hidden units\n","\n","        # Squash samples and segments into a single axis (batch_size * n_segments, max_segment_length) --> (256, 128)\n","        input_ids_reshape = input_ids.contiguous().view(-1, input_ids.size(-1))\n","        attention_mask_reshape = attention_mask.contiguous().view(-1, attention_mask.size(-1))\n","        if token_type_ids is not None:\n","            token_type_ids_reshape = token_type_ids.contiguous().view(-1, token_type_ids.size(-1))\n","        else:\n","            token_type_ids_reshape = None\n","\n","        # Encode segments with BERT --> (256, 128, 768)\n","        encoder_outputs = self.encoder(input_ids=input_ids_reshape,\n","                                       attention_mask=attention_mask_reshape,\n","                                       token_type_ids=token_type_ids_reshape)[0]\n","\n","        # Reshape back to (batch_size, n_segments, max_segment_length, output_size) --> (4, 64, 128, 768)\n","        encoder_outputs = encoder_outputs.contiguous().view(input_ids.size(0), self.max_segments,\n","                                                            self.max_segment_length,\n","                                                            self.hidden_size)\n","\n","        # Gather CLS outputs per segment --> (4, 64, 768)\n","        encoder_outputs = encoder_outputs[:, :, 0]\n","\n","        # Infer real segments, i.e., mask paddings\n","        seg_mask = (torch.sum(input_ids, 2) != 0).to(input_ids.dtype)\n","        # Infer and collect segment positional embeddings\n","        seg_positions = torch.arange(1, self.max_segments + 1).to(input_ids.device) * seg_mask\n","        # Add segment positional embeddings to segment inputs\n","        encoder_outputs += self.seg_pos_embeddings(seg_positions)\n","\n","        # Encode segments with segment-wise transformer\n","        seg_encoder_outputs = self.seg_encoder(encoder_outputs)\n","\n","        # Collect document representation\n","        outputs, _ = torch.max(seg_encoder_outputs, 1)\n","\n","        return SimpleOutput(last_hidden_state=outputs, hidden_states=outputs)\n","\n","\n","if __name__ == \"__main__\":\n","    from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n","    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","\n","    # Use as a stand-alone encoder\n","    bert = AutoModel.from_pretrained('bert-base-uncased')\n","    model = HierarchicalBert(encoder=bert, max_segments=64, max_segment_length=128)\n","\n","    fake_inputs = {'input_ids': [], 'attention_mask': [], 'token_type_ids': []}\n","    for i in range(4):\n","        # Tokenize segment\n","        temp_inputs = tokenizer(['dog ' * 126] * 64)\n","        fake_inputs['input_ids'].append(temp_inputs['input_ids'])\n","        fake_inputs['attention_mask'].append(temp_inputs['attention_mask'])\n","        fake_inputs['token_type_ids'].append(temp_inputs['token_type_ids'])\n","\n","    fake_inputs['input_ids'] = torch.as_tensor(fake_inputs['input_ids'])\n","    fake_inputs['attention_mask'] = torch.as_tensor(fake_inputs['attention_mask'])\n","    fake_inputs['token_type_ids'] = torch.as_tensor(fake_inputs['token_type_ids'])\n","\n","    output = model(fake_inputs['input_ids'], fake_inputs['attention_mask'], fake_inputs['token_type_ids'])\n","\n","    # 4 document representations of 768 features are expected\n","    assert output[0].shape == torch.Size([4, 768])\n","\n","    # Use with HuggingFace AutoModelForSequenceClassification and Trainer API\n","\n","    # Init Classifier\n","    model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=10)\n","    # Replace flat BERT encoder with hierarchical BERT encoder\n","    model.bert = HierarchicalBert(encoder=model.bert, max_segments=64, max_segment_length=128)\n","    output = model(fake_inputs['input_ids'], fake_inputs['attention_mask'], fake_inputs['token_type_ids'])\n","\n","    # 4 document outputs with 10 (num_labels) logits are expected\n","    assert output.logits.shape == torch.Size([4, 10])\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FO90Zx6NqG5X","executionInfo":{"status":"ok","timestamp":1693761614518,"user_tz":-60,"elapsed":51835,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"}},"outputId":"affa3ab1-925f-451e-d93e-24d9354b1735"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["[WARNING|modeling_utils.py:3553] 2023-09-03 17:19:52,528 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["from torch import nn\n","from transformers import Trainer\n","\n","\n","class MultilabelTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.pop(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","        loss_fct = nn.BCEWithLogitsLoss()\n","        loss = loss_fct(logits.view(-1, self.model.config.num_labels),\n","                        labels.float().view(-1, self.model.config.num_labels))\n","        return (loss, outputs) if return_outputs else loss\n","\n","\n","\n"],"metadata":{"id":"8Zz544gbqKz4","executionInfo":{"status":"ok","timestamp":1693761614518,"user_tz":-60,"elapsed":5,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["#!/usr/bin/env python\n","# coding=utf-8\n","\"\"\" Finetuning models on UNFAIR-ToC (e.g. Bert, RoBERTa, LEGAL-BERT).\"\"\"\n","import pandas as pd\n","import logging\n","import os\n","import random\n","import sys\n","from dataclasses import dataclass, field\n","from typing import Optional\n","\n","import datasets\n","from datasets import load_dataset\n","from sklearn.metrics import f1_score\n","# from trainer import MultilabelTrainer\n","from scipy.special import expit\n","import glob\n","import shutil\n","import numpy as np\n","import csv\n","\n","import transformers\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    DataCollatorWithPadding,\n","    EvalPrediction,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    default_data_collator,\n","    set_seed,\n","    EarlyStoppingCallback,\n",")\n","from transformers.trainer_utils import get_last_checkpoint\n","from transformers.utils import check_min_version\n","from transformers.utils.versions import require_version\n","\n","\n","# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n","check_min_version(\"4.9.0\")\n","\n","require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/text-classification/requirements.txt\")\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","@dataclass\n","class DataTrainingArguments:\n","    \"\"\"\n","    Arguments pertaining to what data we are going to input our model for training and eval.\n","\n","    Using `HfArgumentParser` we can turn this class\n","    into argparse arguments to be able to specify them on\n","    the command line.\n","    \"\"\"\n","\n","    max_seq_length: Optional[int] = field(\n","        default=128,\n","        metadata={\n","            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n","            \"than this will be truncated, sequences shorter will be padded.\"\n","        },\n","    )\n","    overwrite_cache: bool = field(\n","        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n","    )\n","    pad_to_max_length: bool = field(\n","        default=True,\n","        metadata={\n","            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n","            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n","        },\n","    )\n","    max_train_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_eval_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_predict_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    server_ip: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n","    server_port: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n","\n","\n","@dataclass\n","class ModelArguments:\n","    \"\"\"\n","    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n","    \"\"\"\n","\n","    model_name_or_path: str = field(\n","        default=None, metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n","    )\n","    config_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n","    )\n","    tokenizer_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n","    )\n","    cache_dir: Optional[str] = field(\n","        default=None,\n","        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n","    )\n","    do_lower_case: Optional[bool] = field(\n","        default=True,\n","        metadata={\"help\": \"arg to indicate if tokenizer should do lower case in AutoTokenizer.from_pretrained()\"},\n","    )\n","    use_fast_tokenizer: bool = field(\n","        default=True,\n","        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n","    )\n","    model_revision: str = field(\n","        default=\"main\",\n","        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n","    )\n","    use_auth_token: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n","            \"with private models).\"\n","        },\n","    )\n","def compute_test_predictions(trainer, test_dataset):\n","    logger.info(\"*** Test Predictions ***\")\n","    predictions, labels, metrics = trainer.predict(test_dataset, metric_key_prefix=\"test\")\n","\n","    # Get the actual predicted indexes (class labels)\n","    threshold = 0.5  # Set the threshold for probability values to be considered as positive predictions\n","    actual_predictions = (predictions > threshold).astype(int)  # Convert probabilities to binary predictions\n","\n","    # Get the input text from the dataset\n","    input_texts = test_dataset['text']\n","\n","    # Create a DataFrame to hold the predictions, labels, and input text\n","    with open(\"predictions.txt\", \"w\") as f:\n","        for index, input_text, prediction, label in zip(range(len(input_texts)), input_texts, actual_predictions, labels):\n","            f.write(f\"Index: {index}\\n\")\n","            f.write(f\"Input Text: {input_text}\\n\")\n","            f.write(f\"Predictions: {prediction}\\n\")\n","            f.write(f\"Labels: {label}\\n\")\n","            f.write(\"\\n\")\n","\n","def main(training_args):\n","    # See all possible arguments in src/transformers/training_args.py\n","    # or by passing the --help flag to this script.\n","    # We now keep distinct sets of args, for a cleaner separation of concerns.\n","\n","    model_args = ModelArguments(\n","        model_name_or_path=\"roberta-base\",\n","        # hierarchical=True,\n","        do_lower_case=True,\n","        use_fast_tokenizer=True,\n","    )\n","    data_args = DataTrainingArguments(\n","        max_seq_length=128,\n","        # max_segments=64,\n","        # max_seg_length=128,\n","        overwrite_cache=False,\n","        pad_to_max_length=True,\n","    )\n","\n","    # Setup distant debugging if needed\n","    if data_args.server_ip and data_args.server_port:\n","        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n","        import ptvsd\n","\n","        print(\"Waiting for debugger attach\")\n","        ptvsd.enable_attach(address=(data_args.server_ip, data_args.server_port), redirect_output=True)\n","        ptvsd.wait_for_attach()\n","\n","    # Fix boolean parameter\n","    if model_args.do_lower_case == 'False' or not model_args.do_lower_case:\n","        model_args.do_lower_case = False\n","        'Tokenizer do_lower_case False'\n","    else:\n","        model_args.do_lower_case = True\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        handlers=[logging.StreamHandler(sys.stdout)],\n","    )\n","\n","    log_level = training_args.get_process_log_level()\n","    logger.setLevel(log_level)\n","    datasets.utils.logging.set_verbosity(log_level)\n","    transformers.utils.logging.set_verbosity(log_level)\n","    transformers.utils.logging.enable_default_handler()\n","    transformers.utils.logging.enable_explicit_format()\n","\n","    # Log on each process the small summary:\n","    logger.warning(\n","        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n","        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n","    )\n","    logger.info(f\"Training/evaluation parameters {training_args}\")\n","\n","    # Detecting last checkpoint.\n","    last_checkpoint = None\n","    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n","        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n","        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n","            raise ValueError(\n","                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n","                \"Use --overwrite_output_dir to overcome.\"\n","            )\n","        elif last_checkpoint is not None:\n","            logger.info(\n","                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n","                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n","            )\n","\n","    # Set seed before initializing model.\n","    set_seed(training_args.seed)\n","\n","    # In distributed training, the load_dataset function guarantees that only one local process can concurrently\n","    # download the dataset.\n","    # Downloading and loading eurlex dataset from the hub.\n","    if training_args.do_train:\n","        train_dataset = load_dataset(\"lex_glue\", \"unfair_tos\", split=\"train\", data_dir='data', cache_dir=model_args.cache_dir)\n","        print(train_dataset['labels'])\n","    if training_args.do_eval:\n","        eval_dataset = load_dataset(\"lex_glue\", \"unfair_tos\", split=\"validation\", data_dir='data', cache_dir=model_args.cache_dir)\n","\n","    if training_args.do_predict:\n","        predict_dataset = load_dataset(\"lex_glue\", \"unfair_tos\", split=\"test\", data_dir='data', cache_dir=model_args.cache_dir)\n","\n","    # Labels\n","    label_list = list(range(8))\n","    num_labels = len(label_list)\n","\n","    # Load pretrained model and tokenizer\n","    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n","    # download model & vocab.\n","    config = AutoConfig.from_pretrained(\n","        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n","        num_labels=num_labels,\n","        finetuning_task=\"unfair_toc\",\n","        cache_dir=model_args.cache_dir,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","\n","    if config.model_type == 'big_bird':\n","        config.attention_type = 'original_full'\n","\n","    if config.model_type == 'longformer':\n","        config.attention_window = [128] * config.num_hidden_layers\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n","        do_lower_case=model_args.do_lower_case,\n","        cache_dir=model_args.cache_dir,\n","        use_fast=model_args.use_fast_tokenizer,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        model_args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","        config=config,\n","        cache_dir=model_args.cache_dir,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","\n","    # Preprocessing the datasets\n","    # Padding strategy\n","    if data_args.pad_to_max_length:\n","        padding = \"max_length\"\n","    else:\n","        # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n","        padding = False\n","\n","    def preprocess_function(examples):\n","        print(examples[\"labels\"])\n","        # Tokenize the texts\n","        batch = tokenizer(\n","            examples[\"text\"],\n","            padding=padding,\n","            max_length=data_args.max_seq_length,\n","            truncation=True,\n","\n","\n","        )\n","        #print(\"##############\")\n","        #print(examples[\"labels\"])\n","        batch[\"original_labels\"] = [[label for label in label_list if label in labels] for labels in examples[\"labels\"]]\n","        #batch[\"original_labels\"] = [[label if label in labels else 0 for label in label_list] for labels in examples[\"labels\"]]\n","        batch[\"labels\"] = [[1 if label in labels else 0 for label in label_list] for labels in examples[\"labels\"]]\n","        #print(labels)\n","\n","\n","        #print(label)\n","        #print(batch)\n","        return batch\n","\n","    if training_args.do_train:\n","        if data_args.max_train_samples is not None:\n","            train_dataset = train_dataset.select(range(data_args.max_train_samples))\n","            #print(\"##############\")\n","            #print(train_dataset[\"labels\"])\n","        with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n","            train_dataset = train_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on train dataset\",\n","            )\n","        # Log a few random samples from the training set:\n","        for index in random.sample(range(len(train_dataset)), 3):\n","            logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n","\n","    if training_args.do_eval:\n","        if data_args.max_eval_samples is not None:\n","            eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))\n","        with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n","            eval_dataset = eval_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on validation dataset\",\n","            )\n","\n","    if training_args.do_predict:\n","        if data_args.max_predict_samples is not None:\n","           predict_dataset = predict_dataset.select(range(data_args.max_predict_samples))\n","        with training_args.main_process_first(desc=\"prediction dataset map pre-processing\"):\n","            predict_dataset = predict_dataset.map(\n","            preprocess_function,\n","            batched=True,\n","            load_from_cache_file=not data_args.overwrite_cache,\n","            desc=\"Running tokenizer on prediction dataset\",\n","        )\n","\n","\n","\n","    # Compute predictions for the test dataset\n","\n","\n","\n","    # You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n","    # predictions and label_ids field) and has to return a dictionary string to float.\n","    def compute_metrics(p: EvalPrediction):\n","        # Fix gold labels\n","        y_true = np.zeros((p.label_ids.shape[0], p.label_ids.shape[1] + 1), dtype=np.int32)\n","        y_true[:, :-1] = p.label_ids\n","        y_true[:, -1] = (np.sum(p.label_ids, axis=1) == 0).astype('int32')\n","        # Fix predictions\n","        logits = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","        preds = (expit(logits) > 0.5).astype('int32')\n","        y_pred = np.zeros((p.label_ids.shape[0], p.label_ids.shape[1] + 1), dtype=np.int32)\n","        y_pred[:, :-1] = preds\n","        y_pred[:, -1] = (np.sum(preds, axis=1) == 0).astype('int32')\n","        # Compute scores\n","        macro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='macro', zero_division=0)\n","        micro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='micro', zero_division=0)\n","        return {'macro-f1': macro_f1, 'micro-f1': micro_f1}\n","\n","\n","\n","    # Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n","    if data_args.pad_to_max_length:\n","        data_collator = default_data_collator\n","    elif training_args.fp16:\n","        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n","    else:\n","        data_collator = None\n","\n","    # Initialize our Trainer\n","    trainer = MultilabelTrainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset if training_args.do_train else None,\n","        eval_dataset=eval_dataset if training_args.do_eval else None,\n","        compute_metrics=compute_metrics,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n","    )\n","    compute_test_predictions(trainer, predict_dataset)\n","    # Training\n","    if training_args.do_train:\n","        checkpoint = None\n","        if training_args.resume_from_checkpoint is not None:\n","            checkpoint = training_args.resume_from_checkpoint\n","        elif last_checkpoint is not None:\n","            checkpoint = last_checkpoint\n","        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n","        metrics = train_result.metrics\n","        max_train_samples = (\n","            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n","        )\n","        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n","\n","        trainer.save_model()  # Saves the tokenizer too for easy upload\n","\n","        trainer.log_metrics(\"train\", metrics)\n","        trainer.save_metrics(\"train\", metrics)\n","        trainer.save_state()\n","\n","    # Evaluation\n","    if training_args.do_eval:\n","        logger.info(\"*** Predict ***\")\n","        predictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix=\"predict\")\n","\n","        # Get the actual predicted indexes (class labels)\n","        threshold = 0.5  # Set the threshold for probability values to be considered as positive predictions\n","        actual_predictions = (predictions > threshold).astype(int)  # Convert probabilities to binary predictions\n","\n","        # Get the input text from the dataset\n","        input_texts = predict_dataset['text']\n","\n","        # Create a DataFrame to hold the predictions, labels, and input text\n","        with open(\"predictions.txt\", \"w\") as f:\n","            for index, input_text, prediction, label in zip(range(len(input_texts)), input_texts, actual_predictions, labels):\n","                f.write(f\"Index: {index}\\n\")\n","                f.write(f\"Input Text: {input_text}\\n\")\n","                f.write(f\"Predictions: {prediction}\\n\")\n","                f.write(f\"Labels: {label}\\n\")\n","                f.write(\"\\n\")\n","\n","    # ... Your existing code ...\n","\n","    # Access the rows from the dataset associated with correct predictions\n","    # ... Your existing code ...\n","\n","# Access the rows from the dataset associated with correct predictions\n","    correct_predictions_dataset = predict_dataset.filter(\n","    lambda example, idx: all(actual_predictions[idx] == example[\"labels\"]),\n","    with_indices=True\n",")\n","\n","# Access the rows from the dataset associated with incorrect predictions\n","    incorrect_predictions_dataset = predict_dataset.filter(\n","    lambda example, idx: any(actual_predictions[idx] != example[\"labels\"]),\n","    with_indices=True\n",")\n","\n","# Access the rows from the dataset associated with original labels\n","    original_labels = predict_dataset['labels']\n","\n","# Get the input texts and input IDs from the datasets\n","    correct_input_texts = correct_predictions_dataset[\"text\"]\n","    correct_input_ids = correct_predictions_dataset[\"input_ids\"]\n","    incorrect_input_texts = incorrect_predictions_dataset[\"text\"]\n","    incorrect_input_ids = incorrect_predictions_dataset[\"input_ids\"]\n","\n","# Save the correct predictions to a file\n","    with open(\"correct_predictions.txt\", \"w\") as f:\n","       for idx, input_text, input_id, labels in zip(range(len(correct_input_texts)), correct_input_texts, correct_input_ids, correct_predictions_dataset[\"labels\"]):\n","         f.write(f\"Index: {idx}\\n\")\n","         f.write(f\"Input Text: {input_text}\\n\")\n","         f.write(f\"Input IDs: {input_id}\\n\")\n","         f.write(f\"Labels: {labels}\\n\")\n","         f.write(\"\\n\")\n","\n","# Save the incorrect predictions to a file\n","    # Save the incorrect predictions to a file\n","    with open(\"incorrect_predictions.txt\", \"w\") as f:\n","        for idx, input_text, input_id, labels, original_labels  in zip(\n","            range(len(incorrect_input_texts)),\n","            incorrect_input_texts,\n","            incorrect_input_ids,\n","            incorrect_predictions_dataset[\"labels\"],\n","            incorrect_predictions_dataset[\"original_labels\"],\n","\n","        ):\n","            f.write(f\"Index: {idx}\\n\")\n","            f.write(f\"Input Text: {input_text}\\n\")\n","            f.write(f\"Input IDs: {input_id}\\n\")\n","            f.write(f\"Labels: {labels}\\n\")  # Write the labels to the file\n","            f.write(f\"Original_Labels: {original_labels}\\n\")\n","            f.write(\"\\n\")\n","\n","\n","\n","\n","\n","\n","if __name__ == \"__main__\":\n","    training_args = TrainingArguments(\n","        do_train=True,\n","        do_eval=True,\n","        do_predict=True,\n","        output_dir=os.getcwd(),\n","        overwrite_output_dir=True,\n","        num_train_epochs=2,\n","        per_device_train_batch_size=32,\n","        per_device_eval_batch_size=32,\n","        save_steps=500,\n","        save_total_limit=2,\n","        fp16=False,\n","        logging_dir=\"./logs\",\n","        logging_steps=200,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=500,\n","        logging_first_step=False,\n","        load_best_model_at_end=True,\n","        metric_for_best_model=\"micro-f1\",\n","    )\n","    main(training_args)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":829,"referenced_widgets":["6392d7e75dcf4f5e80566b3fee3c8787","540d11b9a35e4af0a4fe5321a8da5e60","b01f9cde0be14d83ad1cda2944536fbe","a73a83965f7f4492b78487e6466be180","f6d74d9cde6e49cbab9d3476339525e6","12c55e2bfcb049c08ed818f3edae7f79","d3994877e600402eaa896faabf9a00d0","0583a870d5f74f6096970167d8fc4f87","71100575fd914c26af3c2dfe46e2dd5d","e20e2d94495943209cd8d864070251e5","d09c52d630094838b283a47c8da0a14e","4e1f3ae1c49d4ff585a610de7ef1a695","d543c54b07ca4e8aa733df10836eafef","d5376e53289141a08a84ff7e8be8c2bb","17f90bb64d5d41e5a56096e988880029","ec8c1acb576f4f068f7452dc6e696d94","1be118695e82484aa48c9fa21d3b4a0f","1e6587eba3794099901faab3f0cfb05d","bb84e72157cc4095bd28ba4321f1b8de","ff9ca10fec454eab8ab48be6e6ffe5fb","71c08b5623ab477b97e9786c985a3e4f","4eb9c158a32c4cb8b3f92e0c2197a0fa","cde17784f2a44d5e8d75fdf2e4485c62","03d7cd7d0890447ebf76d849b4c36661","9cdac6edde0c4c8d906359c69aa02369","ae27bcaaa34042a4abc02194bab25c1f","d3d70c3e0174401884f29bd2389a2aaf","28b5e8e2104144579e935f49d5d875f8","ea88c83caba8445c90367f17bda487c3","0d300ebcb320429d9b1e90dccec5b08d","9247f33810254a18bcfb64ac7fad1943","d07df81ec87a4ce08efb218d3f2037f3","468d254e7a1e4debb862029937027349","e90ebc97cf7b46739cb9f2ba12b63391","0b557e8cb84e4cae936b16a003c3421b","d7fcf469db694d4595dc42ea238cb91d","fc8b5b78d3c941d1b7ba645d10a2f073","4a0c7f2cf3e44cc7b440bd025d176c1f","34f0a503565d4cb6bd50303d0e91fbb0","b7e1f5c290c449b7b6f3f8c99d163f32","89bce85b7eec4ec0bb4f48d93ed20487","273da878f9564d1c907e578a4aa518e2","916bd2cc36004284974c713a1642c221","4c66fa0a76ee44bdb4e2938d52a45774","8834d031367e4772b97e05f289fbb3d6","81f9163a88a04607add1cb17f04b9f02","d18074777a0e4ff08088029b019147b5","25c3d4de007543d5a5c6b9a24fd4162d","7a006a3f9c6540d493a1a6dd8cf25213","f3fbc252118f495db4a5af43a7057d01","be7795a5a6a44add882edefd19e7e8c5","a84952679f10418884f4a5e2837beae0","a38f5473769c42b4b53b719b2bd53601","6f1f4d9d6af94a5593430e2a3e29b147","eaba7585fa14459fafea3ce7409d6425","829d1c9ed60a4efcb5819fde119050f2","be8897ca11fd4b9398d0ecc48b6fe678","f783da9d82d04f5781c50214c176436d","7c99ff4eb8c14b54a985c35124e7c190","a20c79d8b89343f786803936b0a6e9ec","b394c0ee754141a28c660bd5ae5491e2","5dcc52e65494416db0c9232f41b0b647","d7873fb7835f43c9a26fe4661e8c5e8c","08053af3322b492898a29d711738bf97","4ac57609c84f48909998eed753b45f2e","057e2e261d8e48b085e6885390ba75bc","a079f6f1c3d343b6a80ccb3f5d2ffacf","e1bb1138d48b4a448b7404ecc9e2f3b3","b10fd229cfe4458b825d8e4f2f529087","6b57828957854a33923743358c049329","d7798ed500e94664b5587f1b61cbfee9","8049295863014de6b982711c24088e2b","7493a400b3234748bfadf6abcb403aad","edef25b9dff5444aa15f5aee9330a04f","4d480e9360e644e1a3335773085d3c0a","d5cc4e74345247eabb00f2c77407e0eb","c65b320b8da348f7abf6ab443e9d0174","eaf34267ac3841fca9022a097ff67351","d0b4cd69179d4d17a824e3609f188002","0c173efb2dca4d50b118e62b67fd8d1b","1a654a545df3420ca67b08df1296241f","265d428cfca74bcb9910e22e616acaed","d054549f965f48c6aefdf83b46ec965d","d4433026bcbd412f93a7160976146f43","517fdc876c554cac9f18032dbdd0e3b1","50dc58d2e8034e3188dd87a6afbf779b","25c91cbeae64411b8c425f29fc4066c0","948d00a43da24996a9e490dd0ee9f0a8","34e6c1905b004a21959effcb22d9c4a6","4d053e2bb00548eeae9dfe8e6d7e3d75","a054c1f33b8c47bbb32da60e77ba8c47","92a5094521fa4f59acda84da899b3aa5","49581b088f9344d6b499c7b8de63fb89","e1fde492d5274cb08f08a2b438721831","9e4c6cd6a3b348dfad2484c3cfc44c96","d29ee12b761449cebc9b87110f89b04a","7061c293e9604289b3af179a7e47e93d","95c3c447036f4fbc90de8b1eb7191bc6","e818e55da2184505b7ba03aa2bfaab4c","cd5683abf8de40d18bd9cb8bb7d66b08","0e03aec243784de5b7bc5f7649204a94","e448e8af33c54b299a2eeb71b9765efe","bc8d72f367494c8e837595ba9c7e47c3","5b795445cb654657b4e03c0a10cb67f1","464293ffda9c4c10987c548beac9f167","f9cc090612b84ca5b8f40ee3cc72320b","ae05fad7fcf14dc8aa973e12fd25a179","86d940e097d449e7bbb664cdb759919d","3dd32f24918d42e49bae9dd2352147e2","f4ff43ea30b14b22b9c03c48c4bb5abc"]},"id":"IyjRnMl2qORw","executionInfo":{"status":"ok","timestamp":1693761708444,"user_tz":-60,"elapsed":93929,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"}},"outputId":"ec034f53-4d3f-4d13-97a2-cfca525130da"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n"]},{"output_type":"stream","name":"stdout","text":["[[], [], [], [], [], [], [], [], [4], [], [2], [], [2], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3, 2, 1], [], [1], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [3], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [3, 0], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [2], [2], [0], [], [], [], [4], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [0], [], [], [], [], [0], [0], [0], [], [], [], [7], [], [], [4], [], [], [], [], [], [], [], [], [6], [], [5], [], [], [5], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [1], [2, 1], [2, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [0], [], [0], [], [], [], [], [], [], [], [], [], [3], [], [1], [1], [], [], [], [], [], [], [0], [2, 1], [], [], [], [0], [], [], [], [0], [], [], [], [0], [], [], [], [], [0], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [0], [], [], [], [2], [2], [], [4], [], [], [], [5], [6], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [0], [], [], [], [], [], [], [], [2], [2], [6], [5], [], [], [], [4], [], [], [], [], [], [], [2, 1], [0], [], [], [], [1], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [3, 1], [3, 1], [3, 1], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [], [0], [], [0], [], [], [], [], [], [], [], [], [], [6], [5], [2], [2], [], [], [], [], [], [4], [], [], [], [], [2, 1], [0, 2, 1], [2], [2], [4], [], [], [], [], [], [], [], [], [], [], [0], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [0], [], [], [], [], [], [], [], [2], [], [2], [], [1], [1], [], [], [], [], [], [], [], [0], [], [], [], [5], [], [], [], [], [3], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [5], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [0], [0], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [5, 6], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [2], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [1], [], [], [0], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [3, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [1], [], [], [], [], [], [], [], [], [], [], [0], [0], [0], [0], [0], [0], [], [], [], [], [], [], [], [], [5], [6], [6], [], [2], [], [4], [4, 2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [1], [2], [], [], [], [], [2, 1], [0], [], [1], [1], [0, 1], [3], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [5, 6], [6], [], [], [], [], [], [], [], [], [], [], [], [], [4], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [0], [], [0], [], [], [], [], [], [], [], [], [], [], [], [3], [], [0], [], [], [0], [], [], [0], [0], [], [0], [0], [], [], [], [], [], [2], [], [1], [], [], [], [], [], [], [1], [], [1], [], [], [], [], [], [], [2], [2], [], [], [], [], [4], [5], [6], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [2], [], [], [], [2], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [2], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [3, 1], [3], [], [], [], [], [], [], [], [], [], [0], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [2], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [1], [], [], [], [], [], [], [0], [], [], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [], [], [0], [], [], [], [0], [], [], [], [2], [], [], [], [], [0], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [0], [], [5, 6], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [2], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [2, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [2], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [2], [4], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [1], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [3, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3, 1], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [4], [], [], [], [], [], [], [], [], [1], [1], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [0], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [2], [], [], [], [4], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [2], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3, 0], [0], [], [], [3, 0, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [1], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [0], [], [0], [0], [], [], [], [], [], [], [3, 1], [], [], [3, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [5], [], [], [7], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [2], [4], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [6], [], [], [], [], [], [], [], [], [], [0], [0], [0], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [1], [], [], [1], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [3, 1], [], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [4], [], [], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [1], [2, 1], [], [3], [], [2], [], [4], [2], [], [], [], [], [3], [], [0], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [0], [], [], [], [], [0], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [0], [0], [], [0], [], [], [0], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [5], [6], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [4], [], [], [], [], [], [], [], [], [3, 2], [0], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [0], [], [], [], [], [], [], [], [], [2], [4], [], [], [5], [], [7], [7], [], [], [], [], [4], [], [], [], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [1], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [2, 1], [0], [], [0], [], [], [], [], [], [0], [], [], [], [2], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [5], [], [], [], [], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [6], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [2, 1], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [0], [0], [0], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [5], [5], [6], [2], [2], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [0], [], [], [0], [0], [0], [0], [], [], [], [], [], [], [], [], [5], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [3, 1], [3, 1], [3, 1], [], [], [], [2], [4], [], [], [], [], [4], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [1], [], [1], [], [], [], [], [], [], [], [], [], [0], [0], [0], [], [], [2], [2], [], [], [], [], [], [], [], [], [], [], [5], [6], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [3], [], [], [], [1], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [1], [], [], [], [6], [5], [], [], [], [], [], [], [], [], [], [0], [0], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4, 2], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3, 2, 1], [], [1], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [0], [0], [], [], [0], [], [], [], [], [], [], [], [], [], [5], [7], [], [], [], [6], [], [5], [6], [], [], [], [], [], [], [2], [0, 2], [], [4], [], [], [0], [], [], [], [], [], [], [], [], [], [], [4], [], [2], [2], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [0], [], [], [1], [1], [3], [], [], [5], [6], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3, 1], [], [], [], [], [], [], [], [], [], [], [2, 1], [2], [], [], [], [], [], [], [], [1], [], [1], [1], [], [], [], [], [], [], [], [], [], [0], [], [0], [0], [], [], [0], [], [], [], [], [], [6], [6], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [6], [], [], [], [5], [], [], [], [], [], [], [], [], [2], [], [], [], [], [4], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [7], [], [], [], [2], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3, 1], [], [], [], [], [], [], [], [], [], [], [], [3, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [3], [3], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3, 1], [], [], [], [], [0], [], [], [], [2], [], [0], [0, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [1], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [0], [0], [], [], [], [], [], [], [], [], [], [0, 1], [1], [0, 2, 1], [], [], [], [], [], [5], [6], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0, 2, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0, 2, 1], [], [], [], [3, 2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [1], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [5], [6], [5, 6], [5, 6], [], [], [], [], [], [], [], [], [], [], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2, 1], [2], [2], [0, 2], [2], [], [], [], [], [], [], [], [1], [2, 1], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [0, 2, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [2], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [0], [0], [0], [], [], [], [], [], [], [], [], [], [5], [5], [5], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [6], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [0], [0], [0], [], [0], [], [], [7], [7], [6], [5], [], [], [1], [], [2, 1], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [4], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [1], [0], [3, 1], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [1], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [0], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [5], [5], [], [6], [6], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [2], [2], [4], [], [1], [7], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [3], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [2], [2], [], [], [], [], [], [], [], [], [], [3, 2], [], [3, 1], [0], [0], [3, 0, 2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [], [1], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [0], [], [0], [], [5], [7], [], [], [6], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [5], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [7], [], [], [], [], [], [], [2, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [0], [0], [0], [], [0], [], [2], [], [4], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [6], [5], [], [], [7], [], [], [], [], [], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [2], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [0], [], [], [0], [], [0], [0], [0], [], [], [1], [1], [], [], [], [], [], [], [], [], [], [], [], [5, 6], [], []]\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6392d7e75dcf4f5e80566b3fee3c8787"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e1f3ae1c49d4ff585a610de7ef1a695"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cde17784f2a44d5e8d75fdf2e4485c62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e90ebc97cf7b46739cb9f2ba12b63391"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8834d031367e4772b97e05f289fbb3d6"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[WARNING|modeling_utils.py:3553] 2023-09-03 17:20:23,598 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["Running tokenizer on train dataset:   0%|          | 0/5532 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"829d1c9ed60a4efcb5819fde119050f2"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[[], [], [], [], [], [], [], [], [4], [], [2], [], [2], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3, 2, 1], [], [1], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [3], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [3, 0], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [2], [2], [0], [], [], [], [4], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [0], [], [], [], [], [0], [0], [0], [], [], [], [7], [], [], [4], [], [], [], [], [], [], [], [], [6], [], [5], [], [], [5], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [1], [2, 1], [2, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [0], [], [0], [], [], [], [], [], [], [], [], [], [3], [], [1], [1], [], [], [], [], [], [], [0], [2, 1], [], [], [], [0], [], [], [], [0], [], [], [], [0], [], [], [], [], [0], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [0], [], [], [], [2], [2], [], [4], [], [], [], [5], [6], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [0], [], [], [], [], [], [], [], [2], [2], [6], [5], [], [], [], [4], [], [], [], [], [], [], [2, 1], [0], [], [], [], [1], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [3, 1], [3, 1], [3, 1], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [], [0], [], [0], [], [], [], [], [], [], [], [], [], [6], [5], [2], [2], [], [], [], [], [], [4], [], [], [], [], [2, 1], [0, 2, 1], [2], [2], [4], [], [], [], [], [], [], [], [], [], [], [0], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [0], [], [], [], [], [], [], [], [2], [], [2], [], [1], [1], [], [], [], [], [], [], [], [0], [], [], [], [5], [], [], [], [], [3], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [5], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [0], [0], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [5, 6], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [2], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [1], [], [], [0], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [3, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [1], [], [], [], [], [], [], [], [], [], [], [0], [0], [0], [0], [0], [0], [], [], [], [], [], [], [], [], [5], [6], [6], [], [2], [], [4], [4, 2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [1], [2], [], [], [], [], [2, 1], [0], [], [1], [1], [0, 1], [3], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [5, 6], [6], [], [], [], [], [], [], [], [], [], [], [], [], [4]]\n","[[4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [0], [], [0], [], [], [], [], [], [], [], [], [], [], [], [3], [], [0], [], [], [0], [], [], [0], [0], [], [0], [0], [], [], [], [], [], [2], [], [1], [], [], [], [], [], [], [1], [], [1], [], [], [], [], [], [], [2], [2], [], [], [], [], [4], [5], [6], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [2], [], [], [], [2], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [2], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [3, 1], [3], [], [], [], [], [], [], [], [], [], [0], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [2], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [1], [], [], [], [], [], [], [0], [], [], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [], [], [0], [], [], [], [0], [], [], [], [2], [], [], [], [], [0], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [0], [], [5, 6], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [2], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [2, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [2], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [2], [4], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [1], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [3, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3, 1], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [4], [], [], [], [], [], [], [], [], [1], [1], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [0], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [2], [], [], [], [4], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [2], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3, 0], [0], [], [], [3, 0, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [1], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [0], [], [0], [0], [], [], [], [], [], [], [3, 1], [], [], [3, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [5], [], [], [7], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n","[[1], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [2], [4], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [6], [], [], [], [], [], [], [], [], [], [0], [0], [0], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [1], [], [], [1], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [3, 1], [], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [4], [], [], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [1], [2, 1], [], [3], [], [2], [], [4], [2], [], [], [], [], [3], [], [0], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [0], [], [], [], [], [0], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [0], [0], [], [0], [], [], [0], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [5], [6], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [4], [], [], [], [], [], [], [], [], [3, 2], [0], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [0], [], [], [], [], [], [], [], [], [2], [4], [], [], [5], [], [7], [7], [], [], [], [], [4], [], [], [], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [1], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [2, 1], [0], [], [0], [], [], [], [], [], [0], [], [], [], [2], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [5], [], [], [], [], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [6], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [2, 1], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [0], [0], [0], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [5], [5], [6], [2], [2], [], [], [], [], [], [], [], [], [4], []]\n","[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [0], [], [], [0], [0], [0], [0], [], [], [], [], [], [], [], [], [5], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [3, 1], [3, 1], [3, 1], [], [], [], [2], [4], [], [], [], [], [4], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [1], [], [1], [], [], [], [], [], [], [], [], [], [0], [0], [0], [], [], [2], [2], [], [], [], [], [], [], [], [], [], [], [5], [6], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [3], [], [], [], [1], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [1], [], [], [], [6], [5], [], [], [], [], [], [], [], [], [], [0], [0], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4, 2], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3, 2, 1], [], [1], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [0], [0], [], [], [0], [], [], [], [], [], [], [], [], [], [5], [7], [], [], [], [6], [], [5], [6], [], [], [], [], [], [], [2], [0, 2], [], [4], [], [], [0], [], [], [], [], [], [], [], [], [], [], [4], [], [2], [2], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [0], [], [], [1], [1], [3], [], [], [5], [6], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3, 1], [], [], [], [], [], [], [], [], [], [], [2, 1], [2], [], [], [], [], [], [], [], [1], [], [1], [1], [], [], [], [], [], [], [], [], [], [0], [], [0], [0], [], [], [0], [], [], [], [], [], [6], [6], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [6], [], [], [], [5], [], [], [], [], [], [], [], [], [2], [], [], [], [], [4], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [7], [], [], [], [2], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3, 1], [], [], [], [], [], [], [], [], [], [], [], [3, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [3], [3], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3, 1], [], [], [], [], [0], [], [], [], [2], [], [0], [0, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [1], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [0], [0], [], [], [], [], [], [], [], [], [], [0, 1], [1], [0, 2, 1], [], [], [], [], [], [5], [6], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n","[[], [], [], [], [], [], [], [], [], [], [2], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0, 2, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0, 2, 1], [], [], [], [3, 2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [1], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [5], [6], [5, 6], [5, 6], [], [], [], [], [], [], [], [], [], [], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2, 1], [2], [2], [0, 2], [2], [], [], [], [], [], [], [], [1], [2, 1], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [0, 2, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [2], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [0], [0], [0], [], [], [], [], [], [], [], [], [], [5], [5], [5], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [6], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [0], [0], [0], [], [0], [], [], [7], [7], [6], [5], [], [], [1], [], [2, 1], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [4], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [1], [0], [3, 1], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [1], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [2], [], [], [], []]\n","[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [0], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [5], [5], [], [6], [6], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [2], [2], [4], [], [1], [7], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [3], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [2], [2], [], [], [], [], [], [], [], [], [], [3, 2], [], [3, 1], [0], [0], [3, 0, 2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [], [1], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [0], [], [0], [], [5], [7], [], [], [6], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [5], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [7], [], [], [], [], [], [], [2, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [0], [0], [0], [], [0], [], [2], [], [4], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [6], [5], [], [], [7], [], [], [], [], [], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [2], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [0], [], [], [0], [], [0], [0], [0], [], [], [1], [1], [], [], [], [], [], [], [], [], [], [], [], [5, 6], [], []]\n"]},{"output_type":"display_data","data":{"text/plain":["Running tokenizer on validation dataset:   0%|          | 0/2275 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a079f6f1c3d343b6a80ccb3f5d2ffacf"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [3], [], [], [], [], [3, 2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [0], [], [], [], [], [2], [], [], [1], [], [], [1], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [5], [], [5], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [1], [3, 1], [3, 1], [], [3, 1], [3, 1], [3, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [5], [6], [6], [5], [7], [], [], [5], [], [], [6], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [2], [], [4], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [3], [], [], [], [], [], [], [], [0], [0], [0], [], [], [], [], [], [], [0], [], [], [], [], [2], [], [], [], [], [], [6], [5], [], [], [], [], [], [], [], [], [4], [], [], [4, 2], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [3], [], [1], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [0], [], [0], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [2], [3, 1], [0, 1], [4], [5], [6], [6], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [2], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [1], [], [1], [3], [1], [1], [1], [1], [0], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [3], [], [], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [], [3], [0], [], [], [], [], [], [2], [0], [], [], [2], [], [], [], [], [], [], [0], [0], [], [], [], [], [5], [6], [5], [6], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], []]\n","[[], [], [], [4], [], [], [4], [], [], [], [], [], [7], [], [], [], [], [], [], [], [], [2], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [3, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [1], [], [], [], [], [], [], [], [0], [], [], [], [0], [0], [0], [0], [0], [0], [0], [0], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [6], [6], [6], [5], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [3, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2, 1], [0], [], [1], [3], [], [], [], [], [], [], [0], [0], [], [], [], [7], [5], [], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [6], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [3, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [5], [], [6], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [2], [2], [4], [], [], [], [], [], [], [], [], [], [1], [0], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [1], [], [], [], [], [], [], [0], [0], [], [], [], [2], [], [], [], [], [], [], [], [], [], [0], [0], [0], [], [], [], [], [], [6], [], [7], [], [], [], [], [], [5], [5], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [5], [], [5], [], [6], [2, 1], [0], [1], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3, 1], [], [3], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [0], [0], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n","[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3, 0], [], [1], [], [], [], [], [], [], [1], [0], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [0], [], [3, 0, 1], [], [], [], [], [1], [], [1], [0], [1], [1], [], [1], [1], [1], [1], [1], [0], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [0], [], [], [], [], [], [], [], [], [], [], [], [0], [0], [0], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [6], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [6], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [4], [2], [], [], [], [], [], [], [], [5], [5], [6], [], [], [], [], [], [], [], [], [], []]\n"]},{"output_type":"display_data","data":{"text/plain":["Running tokenizer on prediction dataset:   0%|          | 0/1607 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eaf34267ac3841fca9022a097ff67351"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[[], [], [], [], [7], [], [], [], [], [], [], [], [4], [], [], [2], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [2, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3, 1], [], [1], [], [], [], [], [], [1], [1], [1], [], [], [], [], [], [], [], [], [], [], [], [], [0], [0], [], [], [], [], [5], [], [], [], [], [], [], [], [7], [], [], [], [6], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [3, 1], [], [], [], [3], [], [], [], [3], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [5, 4], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [5], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3, 1], [], [], [2], [], [], [], [], [], [], [], [], [], [2, 1], [2], [], [1], [], [1], [1], [], [], [], [], [], [], [0], [], [0], [0], [], [], [], [], [], [], [], [6], [6], [5], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [4], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [1], [2], [3, 0], [2], [1], [2], [3, 0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [0], [0], [], [2], [2], [2], [4], [], [], [], [], [], [], [], [], [], [], [], [4], [], [4], [], [], [], [], [], [], [], [], [], [], [2], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2, 1], [], [1], [], [], [], [], [], [], [], [], [0], [0], [], [], [], [], [], [0], [], [2], [2], [2, 1], [], [], [], [], [], [], [0], [], [], [0], [0], [], [], [1], [1], [], [], [], [], [], [], [], [5, 6], [5], [6], [5], [6], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [2], [], [4], [], [], [], [], [2, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [2], [0], [], [], [], [2], [], [], [], [], [], [0], [0], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [0], [0], [], [], [1], [1], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [5], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [6], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [1]]\n","[[], [], [], [2], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [0], [0], [], [0], [0], [], [], [5], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [4], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2], [2], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [3], [], [], [], [], [], [2], [], [2], [], [0], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [0], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [5], [5], [5], [], [], [], [], [], [], [], [], [], [6], [6], [6], [6], [6], [6], [], [], [], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [3, 1], [], [], [1], [1], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [2, 1], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [1], [], [], [], [], [], [], [0], [], [0], [], [], [], [], [], [], [5], [7], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [6], [6], [], [], [], [], [], [2], [2], [], [], [4], [], [], [2], [], [], [], [], [], [], [], [], [], [], [], [6]]\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='346' max='346' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [346/346 01:13, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["***** train metrics *****\n","  epoch                    =        2.0\n","  total_flos               =   677820GF\n","  train_loss               =     0.0835\n","  train_runtime            = 0:01:14.06\n","  train_samples            =       5532\n","  train_samples_per_second =    149.383\n","  train_steps_per_second   =      4.672\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Filter:   0%|          | 0/1607 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34e6c1905b004a21959effcb22d9c4a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Filter:   0%|          | 0/1607 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd5683abf8de40d18bd9cb8bb7d66b08"}},"metadata":{}}]},{"cell_type":"code","source":["import re\n","\n","# Read the \"incorrect-predictions.txt\" file\n","with open(\"/content/incorrect_predictions.txt\", \"r\") as f:\n","    lines = f.readlines()\n","\n","# Initialize variables\n","current_entry = {}\n","data_entries = []\n","\n","# Process the lines and create data entries\n","for line in lines:\n","    line = line.strip()\n","    if line.startswith(\"Index: \"):\n","        current_entry[\"Index\"] = int(line.split(\": \")[1])\n","    elif line.startswith(\"Input Text: \"):\n","        current_entry[\"text\"] = line.split(\": \")[1]\n","    elif line.startswith(\"Input IDs: \"):\n","        input_ids_str = line.split(\": \")[1]\n","        input_ids = [int(id_str) for id_str in re.findall(r'\\d+', input_ids_str)]\n","        current_entry[\"input_ids\"] = input_ids\n","    elif line.startswith(\"Labels: \"):  # Process labels\n","        labels_str = line.split(\": \")[1]\n","        labels = [int(label_str) for label_str in re.findall(r'\\d+', labels_str)]\n","        current_entry[\"labels\"] = labels\n","    elif line.startswith(\"Original_Labels: \"):  # Process original labels\n","        labels_str = line.split(\": \")[1]\n","        original_labels = [int(label_str) for label_str in re.findall(r'\\d+', labels_str)]\n","        current_entry[\"original_labels\"] = original_labels\n","        data_entries.append(current_entry)\n","        current_entry = {}\n","\n","# Extract the list of labels from data entries\n","label_lists = [entry.get(\"original_labels\", []) for entry in data_entries]\n","\n","print(label_lists)\n","print(len(label_lists))\n","filtered_list = [item for item in set(tuple(lst) for lst in label_lists) if item]\n","print(filtered_list)\n","# Print the list of labels for each entry\n","#for index, labels in enumerate(label_lists):\n","#    print(f\"Entry {index}: Labels = {labels}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"orPfSzHUeTI2","executionInfo":{"status":"ok","timestamp":1693761808725,"user_tz":-60,"elapsed":4,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"}},"outputId":"13f101e2-ebd7-42a2-b411-83afa3c27fcc"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["[[7], [4], [2], [4], [1], [1], [1, 2], [1, 3], [1], [1], [1], [1], [0], [0], [5], [7], [6], [4], [1], [1, 3], [3], [3], [0], [0], [7], [4, 5], [2], [1], [2], [2], [1], [5], [2], [4], [3], [4], [1], [1, 3], [2], [1, 2], [2], [1], [1], [1], [0], [0], [0], [6], [6], [5], [4], [4], [3], [2], [1], [2], [0, 3], [2], [1], [2], [0, 3], [1], [0], [0], [2], [2], [2], [4], [4], [4], [2], [4], [3], [1, 2], [1], [0], [0], [0], [2], [2], [1, 2], [0], [0], [0], [1], [1], [5, 6], [5], [6], [5], [6], [2], [4], [2], [4], [1, 2], [1], [2], [0], [2], [0], [0], [0], [0], [0], [1], [1], [4], [5], [7], [6], [4], [1], [2], [4], [0], [3], [2], [0], [0], [0], [0], [0], [5], [7], [4], [4], [4], [2], [2], [4], [3], [2], [2], [0], [0], [1], [0], [0], [0], [0], [5], [5], [5], [6], [6], [6], [6], [6], [6], [7], [0], [4], [1], [1, 3], [1], [1], [2], [2], [1, 2], [1], [0], [0], [5], [7], [6], [6], [2], [2], [4], [2], [6]]\n","172\n","[(6,), (1, 2), (2,), (5,), (0, 3), (4,), (1,), (7,), (3,), (4, 5), (5, 6), (0,), (1, 3)]\n"]}]},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","# Load your dataset\n","#dataset_name = \"your_dataset_name\"  # Update with your dataset name\n","dataset = load_dataset(\"lex_glue\", 'unfair_tos')\n","datasetTrain=dataset['validation']\n","# Define the label_list\n","#filtered_list = [0, 1, 2]  # Update with your label list\n","\n","# Filter the dataset based on the label_list\n","filtered_dataset = datasetTrain.filter(lambda example: any(label in example[\"labels\"] for label in filtered_list))\n","\n","# Extract the text from the filtered dataset\n","texts = filtered_dataset[\"text\"]\n","print(texts)\n","# Print the extracted texts\n","for text in texts:\n","    print(text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"436g62opo_kv","executionInfo":{"status":"ok","timestamp":1693761819538,"user_tz":-60,"elapsed":1347,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"}},"outputId":"6b9462b3-c8e3-41ec-8fc6-95f522dd1a96"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["[]\n"]}]},{"cell_type":"code","source":["import json\n","import re\n","\n","# Read the \"incorrect-predictions.txt\" file\n","with open(\"/content/incorrect_predictions.txt\", \"r\") as f:\n","    lines = f.readlines()\n","\n","# Initialize variables\n","current_entry = {}\n","data_entries = []\n","\n","# Process the lines and create data entries\n","for line in lines:\n","    line = line.strip()\n","    if line.startswith(\"Index: \"):\n","        current_entry[\"Index\"] = int(line.split(\": \")[1])\n","    elif line.startswith(\"Input Text: \"):\n","        current_entry[\"text\"] = line.split(\": \")[1]\n","    elif line.startswith(\"Input IDs: \"):\n","        input_ids_str = line.split(\": \")[1]\n","        input_ids = [int(id_str) for id_str in re.findall(r'\\d+', input_ids_str)]\n","        current_entry[\"input_ids\"] = input_ids\n","        current_entry[\"labels\"] = []  # Assuming no labels for incorrect predictions\n","        data_entries.append(current_entry)\n","        current_entry = {}\n","    elif line.startswith(\"Labels: \"):  # Process labels\n","        labels_str = line.split(\": \")[1]\n","        labels = [int(label_str) for label_str in re.findall(r'\\d+', labels_str)]\n","        current_entry[\"labels\"] = labels\n","        data_entries.append(current_entry)\n","        current_entry = {}\n","    elif line.startswith(\"Original_Labels: \"):  # Process original labels\n","        labels_str = line.split(\": \")[1]\n","        labels = [int(label_str) for label_str in re.findall(r'\\d+', labels_str)]\n","        current_entry[\"original_labels\"] = labels\n","        data_entries.append(current_entry)\n","        current_entry = {}\n","\n","# Save the data entries to a JSON file\n","output_filename = \"incorrect_predictions_dataset.json\"\n","with open(output_filename, \"w\") as json_file:\n","    json.dump(data_entries, json_file, indent=4)\n","\n","print(f\"Converted incorrect predictions saved to {output_filename}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dhpBekgjqRnj","executionInfo":{"status":"ok","timestamp":1693761885073,"user_tz":-60,"elapsed":303,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"}},"outputId":"bbf9a162-74c0-4ba4-ff25-d9ac06653af1"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Converted incorrect predictions saved to incorrect_predictions_dataset.json\n"]}]},{"cell_type":"code","source":["import json\n","from datasets import Dataset\n","\n","# Load your existing dataset\n","from datasets import load_dataset\n","dataset_name = \"lex_glue\"  # Update with your dataset name\n","existing_dataset = load_dataset(dataset_name, 'unfair_tos')\n","\n","# Load the data from the \"incorrect_predictions_dataset.json\" file\n","with open(\"/content/incorrect_predictions_dataset.json\", \"r\") as json_file:\n","    incorrect_predictions_data = json.load(json_file)\n","\n","# Prepare the data entries for merging\n","merged_texts = []\n","merged_original_labels = []\n","\n","for entry in incorrect_predictions_data:\n","    if \"text\" in entry:\n","        text = entry[\"text\"]\n","        original_labels = entry.get(\"original_labels\", [])\n","        num_labels = len(original_labels)\n","        merged_texts.extend([text] * num_labels)\n","        merged_original_labels.extend(original_labels)\n","\n","# Convert the data to the dataset format\n","print(\"###########\")\n","print(len(merged_texts))\n","print(len(merged_original_labels))\n","merged_dataset = Dataset.from_dict({\n","    \"text\": existing_dataset[\"train\"][\"text\"] + merged_texts,\n","    \"labels\": existing_dataset[\"train\"][\"labels\"] + merged_original_labels,\n","})\n","\n","flattened_list = [label for sublist in filtered_list for label in sublist]\n","target_labels = flattened_list  # Update with the label you want to find\n","\n","print(flattened_list)\n","\n","input_texts = []\n","\n","print(len(existing_dataset['train']))\n","print(len(merged_dataset))\n","\n","# Iterate through the dataset examples\n","for example in merged_dataset:\n","    labels = example[\"labels\"]\n","    for target_label in target_labels:\n","        if target_label in labels:\n","            input_texts.append(example[\"text\"])\n","\n","# Print the input texts for target labels\n","#for text in input_texts:\n","    #print(text)\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"sqxLr_B4qSnX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693761918607,"user_tz":-60,"elapsed":2011,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"}},"outputId":"f7e5df6a-dc45-4b9f-cfec-8f2da1d93fec"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["###########\n","0\n","0\n","[6, 1, 2, 2, 5, 0, 3, 4, 1, 7, 3, 4, 5, 5, 6, 0, 1, 3]\n","5532\n","5532\n"]}]},{"cell_type":"code","source":["import json\n","from datasets import Dataset\n","\n","import pandas as pd\n","\n","# Load JSON data from file\n","file_path = '/content/incorrect_predictions_dataset.json'\n","with open(file_path, 'r') as json_file:\n","    data = json.load(json_file)\n","\n","# Create a dictionary to store original_labels\n","labels_dict = {}\n","\n","# Iterate through the data and extract original_labels\n","for entry in data:\n","    if \"original_labels\" in entry:\n","        original_labels = entry[\"original_labels\"]\n","        if \"Index\" in entry:\n","            index = entry[\"Index\"]\n","            labels_dict[index] = original_labels\n","        elif len(labels_dict) > 0:\n","            last_index = max(labels_dict.keys())\n","            labels_dict[last_index].extend(original_labels)\n","\n","# Initialize lists to store texts and labels\n","texts = []\n","text= []\n","labels = label_lists[0:173]\n","print(\"#####\")\n","print(len(label_lists))\n","# Iterate through the data and extract relevant information\n","for entry in data:\n","    if \"text\" in entry:\n","        text = entry[\"text\"]\n","        if \"Index\" in entry:\n","            index = entry[\"Index\"]\n","            #labels.append(labels_dict.get(index, []))\n","        else:\n","            # Use the last index available in labels_dict\n","            last_index = max(labels_dict.keys())\n","            #labels.append(labels_dict.get(last_index, []))\n","        texts.append(text)\n","\n","print(\"Number of Texts:\", len(texts))\n","print(\"Number of Labels:\", len(labels))\n","from datasets import load_dataset\n","existing_dataset = load_dataset(\"lex_glue\", 'unfair_tos')\n","#dataset=dataset_dict['train']\n","#print(\"Number of Texts:\", len(existing_dataset['text']))\n","#print(\"Number of Labels:\", len(existing_dataset['text']))\n","#aligned_data = [{\"text\": text, \"labels\": label} for text, label in zip(texts, labels)]\n","\n","# Create a new Dataset from the aligned data\n","#new_dataset = Dataset.from_dict(aligned_data)\n","\n","aligned_data = {\"text\": texts, \"labels\": labels}\n","df = pd.DataFrame(aligned_data)\n","\n","# Create a new Dataset from the DataFrame\n","new_dataset = Dataset.from_pandas(df)\n","\n","#merged_dataset = existing_dataset[\"train\"].concatenate(new_dataset)\n","#text= existing_dataset[\"train\"][\"text\"] + texts\n","#labels= existing_dataset[\"train\"][\"labels\"] + labels\n","merged_dataset = Dataset.from_dict({\n","    \"text\": existing_dataset[\"train\"][\"text\"] + texts,\n","    \"labels\": existing_dataset[\"train\"][\"labels\"] + labels,\n","})\n","#existing_dataset[\"train\"][\"text\"] += texts\n","#existing_dataset[\"train\"][\"labels\"] += labels\n","print(\"Number of Texts:\", len(merged_dataset['text']))\n","print(\"Number of Labels:\", len(merged_dataset['labels']))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N1PwCc9IBvWA","executionInfo":{"status":"ok","timestamp":1693762013707,"user_tz":-60,"elapsed":1362,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"}},"outputId":"60717862-48b3-45fa-fa8e-f4eba43167bb"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["#####\n","172\n","Number of Texts: 172\n","Number of Labels: 172\n","Number of Texts: 5704\n","Number of Labels: 5704\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"qr7eGI6Ad-FJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!/usr/bin/env python\n","# coding=utf-8\n","\"\"\" Finetuning models on UNFAIR-ToC (e.g. Bert, RoBERTa, LEGAL-BERT).\"\"\"\n","\n","import logging\n","import os\n","import random\n","import sys\n","from dataclasses import dataclass, field\n","from typing import Optional\n","\n","import datasets\n","from datasets import load_dataset\n","from sklearn.metrics import f1_score\n","\n","from scipy.special import expit\n","import glob\n","import shutil\n","import numpy as np\n","\n","import transformers\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    DataCollatorWithPadding,\n","    EvalPrediction,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    default_data_collator,\n","    set_seed,\n","    EarlyStoppingCallback,\n",")\n","from transformers.trainer_utils import get_last_checkpoint\n","from transformers.utils import check_min_version\n","from transformers.utils.versions import require_version\n","\n","\n","# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n","check_min_version(\"4.9.0\")\n","\n","require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/text-classification/requirements.txt\")\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","@dataclass\n","class DataTrainingArguments:\n","    \"\"\"\n","    Arguments pertaining to what data we are going to input our model for training and eval.\n","\n","    Using `HfArgumentParser` we can turn this class\n","    into argparse arguments to be able to specify them on\n","    the command line.\n","    \"\"\"\n","\n","    max_seq_length: Optional[int] = field(\n","        default=128,\n","        metadata={\n","            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n","            \"than this will be truncated, sequences shorter will be padded.\"\n","        },\n","    )\n","    overwrite_cache: bool = field(\n","        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n","    )\n","    pad_to_max_length: bool = field(\n","        default=True,\n","        metadata={\n","            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n","            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n","        },\n","    )\n","    max_train_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_eval_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_predict_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    server_ip: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n","    server_port: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n","\n","\n","@dataclass\n","class ModelArguments:\n","    \"\"\"\n","    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n","    \"\"\"\n","\n","    model_name_or_path: str = field(\n","        default=None, metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n","    )\n","    config_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n","    )\n","    tokenizer_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n","    )\n","    cache_dir: Optional[str] = field(\n","        default=None,\n","        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n","    )\n","    do_lower_case: Optional[bool] = field(\n","        default=True,\n","        metadata={\"help\": \"arg to indicate if tokenizer should do lower case in AutoTokenizer.from_pretrained()\"},\n","    )\n","    use_fast_tokenizer: bool = field(\n","        default=True,\n","        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n","    )\n","    model_revision: str = field(\n","        default=\"main\",\n","        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n","    )\n","    use_auth_token: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n","            \"with private models).\"\n","        },\n","    )\n","\n","\n","def main(training_args):\n","    # See all possible arguments in src/transformers/training_args.py\n","    # or by passing the --help flag to this script.\n","    # We now keep distinct sets of args, for a cleaner separation of concerns.\n","\n","    model_args = ModelArguments(\n","        model_name_or_path=\"bert-base-uncased\",\n","        #hierarchical=True,\n","        do_lower_case=True,\n","        use_fast_tokenizer=True,\n","    )\n","    data_args = DataTrainingArguments(\n","        max_seq_length=128,\n","        #max_segments=64,\n","        #max_seg_length=128,\n","        overwrite_cache=False,\n","        pad_to_max_length=True,\n","    )\n","\n","    # Setup distant debugging if needed\n","    if data_args.server_ip and data_args.server_port:\n","        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n","        import ptvsd\n","\n","        print(\"Waiting for debugger attach\")\n","        ptvsd.enable_attach(address=(data_args.server_ip, data_args.server_port), redirect_output=True)\n","        ptvsd.wait_for_attach()\n","\n","    # Fix boolean parameter\n","    if model_args.do_lower_case == 'False' or not model_args.do_lower_case:\n","        model_args.do_lower_case = False\n","        'Tokenizer do_lower_case False'\n","    else:\n","        model_args.do_lower_case = True\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        handlers=[logging.StreamHandler(sys.stdout)],\n","    )\n","\n","    log_level = training_args.get_process_log_level()\n","    logger.setLevel(log_level)\n","    datasets.utils.logging.set_verbosity(log_level)\n","    transformers.utils.logging.set_verbosity(log_level)\n","    transformers.utils.logging.enable_default_handler()\n","    transformers.utils.logging.enable_explicit_format()\n","\n","    # Log on each process the small summary:\n","    logger.warning(\n","        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n","        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n","    )\n","    logger.info(f\"Training/evaluation parameters {training_args}\")\n","\n","    # Detecting last checkpoint.\n","    last_checkpoint = None\n","    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n","        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n","        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n","            raise ValueError(\n","                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n","                \"Use --overwrite_output_dir to overcome.\"\n","            )\n","        elif last_checkpoint is not None:\n","            logger.info(\n","                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n","                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n","            )\n","\n","    # Set seed before initializing model.\n","    set_seed(training_args.seed)\n","\n","    # In distributed training, the load_dataset function guarantees that only one local process can concurrently\n","    # download the dataset.\n","    # Downloading and loading eurlex dataset from the hub.\n","    if training_args.do_train:\n","        train_dataset = merged_dataset\n","\n","    if training_args.do_eval:\n","        eval_dataset = load_dataset(\"lex_glue\", \"unfair_tos\", split=\"validation\", data_dir='data', cache_dir=model_args.cache_dir)\n","\n","    if training_args.do_predict:\n","        predict_dataset = load_dataset(\"lex_glue\", \"unfair_tos\", split=\"test\", data_dir='data', cache_dir=model_args.cache_dir)\n","\n","    # Labels\n","    label_list = list(range(8))\n","    num_labels = len(label_list)\n","\n","    # Load pretrained model and tokenizer\n","    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n","    # download model & vocab.\n","    config = AutoConfig.from_pretrained(\n","        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n","        num_labels=num_labels,\n","        finetuning_task=\"unfair_toc\",\n","        cache_dir=model_args.cache_dir,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","\n","    if config.model_type == 'big_bird':\n","        config.attention_type = 'original_full'\n","\n","    if config.model_type == 'longformer':\n","        config.attention_window = [128] * config.num_hidden_layers\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n","        do_lower_case=model_args.do_lower_case,\n","        cache_dir=model_args.cache_dir,\n","        use_fast=model_args.use_fast_tokenizer,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        model_args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","        config=config,\n","        cache_dir=model_args.cache_dir,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","\n","    # Preprocessing the datasets\n","    # Padding strategy\n","    if data_args.pad_to_max_length:\n","        padding = \"max_length\"\n","    else:\n","        # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n","        padding = False\n","\n","    def preprocess_function(examples):\n","        # Tokenize the texts\n","        batch = tokenizer(\n","            examples[\"text\"],\n","            padding=padding,\n","            max_length=data_args.max_seq_length,\n","            truncation=True,\n","        )\n","        batch[\"labels\"] = [[1 if label in labels else 0 for label in label_list] for labels in\n","                              examples[\"labels\"]]\n","\n","        return batch\n","\n","    if training_args.do_train:\n","\n","        if data_args.max_train_samples is not None:\n","            train_dataset = train_dataset.select(range(data_args.max_train_samples))\n","            print(\"#################\")\n","            print (train_dataset('labels'))\n","\n","        with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n","            train_dataset = train_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on train dataset\",\n","            )\n","        # Log a few random samples from the training set:\n","        for index in random.sample(range(len(train_dataset)), 3):\n","            logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n","\n","    if training_args.do_eval:\n","        if data_args.max_eval_samples is not None:\n","            eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))\n","        with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n","            eval_dataset = eval_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on validation dataset\",\n","            )\n","\n","    if training_args.do_predict:\n","        if data_args.max_predict_samples is not None:\n","            predict_dataset = predict_dataset.select(range(data_args.max_predict_samples))\n","        with training_args.main_process_first(desc=\"prediction dataset map pre-processing\"):\n","            predict_dataset = predict_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on prediction dataset\",\n","            )\n","\n","    # You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n","    # predictions and label_ids field) and has to return a dictionary string to float.\n","    def compute_metrics(p: EvalPrediction):\n","        # Fix gold labels\n","        y_true = np.zeros((p.label_ids.shape[0], p.label_ids.shape[1] + 1), dtype=np.int32)\n","        y_true[:, :-1] = p.label_ids\n","        y_true[:, -1] = (np.sum(p.label_ids, axis=1) == 0).astype('int32')\n","        # Fix predictions\n","        logits = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","        preds = (expit(logits) > 0.5).astype('int32')\n","        y_pred = np.zeros((p.label_ids.shape[0], p.label_ids.shape[1] + 1), dtype=np.int32)\n","        y_pred[:, :-1] = preds\n","        y_pred[:, -1] = (np.sum(preds, axis=1) == 0).astype('int32')\n","        # Compute scores\n","        macro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='macro', zero_division=0)\n","        micro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='micro', zero_division=0)\n","        return {'macro-f1': macro_f1, 'micro-f1': micro_f1}\n","\n","    # Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n","    if data_args.pad_to_max_length:\n","        data_collator = default_data_collator\n","    elif training_args.fp16:\n","        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n","    else:\n","        data_collator = None\n","\n","    # Initialize our Trainer\n","    trainer = MultilabelTrainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset if training_args.do_train else None,\n","        eval_dataset=eval_dataset if training_args.do_eval else None,\n","        compute_metrics=compute_metrics,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n","    )\n","\n","    # Training\n","    if training_args.do_train:\n","        checkpoint = None\n","        if training_args.resume_from_checkpoint is not None:\n","            checkpoint = training_args.resume_from_checkpoint\n","        elif last_checkpoint is not None:\n","            checkpoint = last_checkpoint\n","        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n","        metrics = train_result.metrics\n","        max_train_samples = (\n","            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n","        )\n","        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n","\n","        trainer.save_model()  # Saves the tokenizer too for easy upload\n","\n","        trainer.log_metrics(\"train\", metrics)\n","        trainer.save_metrics(\"train\", metrics)\n","        trainer.save_state()\n","\n","    # Evaluation\n","    if training_args.do_eval:\n","        logger.info(\"*** Evaluate ***\")\n","        metrics = trainer.evaluate(eval_dataset=eval_dataset)\n","\n","        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n","        metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n","\n","        trainer.log_metrics(\"eval\", metrics)\n","        trainer.save_metrics(\"eval\", metrics)\n","\n","    # Prediction\n","    if training_args.do_predict:\n","        logger.info(\"*** Predict ***\")\n","        predictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix=\"predict\")\n","\n","        max_predict_samples = (\n","            data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)\n","        )\n","        metrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n","\n","        trainer.log_metrics(\"predict\", metrics)\n","        trainer.save_metrics(\"predict\", metrics)\n","\n","        output_predict_file = os.path.join(training_args.output_dir, \"test_predictions.csv\")\n","        if trainer.is_world_process_zero():\n","            with open(output_predict_file, \"w\") as writer:\n","                if isinstance(predictions[0], np.float32):\n","                   pred_line = f'{predictions[0]:.5f}'\n","                   writer.write(pred_line)\n","                else:\n","                    for key, value in metrics.items():\n","                        if isinstance(value, float):\n","                           writer.write(f\"{key} = {value:.5f}\\n\")\n","                        else:\n","                           writer.write(f\"{key} = {value}\\n\")\n","\n","\n","    # Clean up checkpoints\n","    checkpoints = [filepath for filepath in glob.glob(f'{training_args.output_dir}/*/') if '/checkpoint' in filepath]\n","    for checkpoint in checkpoints:\n","        shutil.rmtree(checkpoint)\n","\n","\n","if __name__ == \"__main__\":\n"," #    For Evaluation\n","    training_args = TrainingArguments(\n","        do_train = True,\n","        do_eval = True,\n","        do_predict = True,\n","        output_dir=os.getcwd(),\n","        overwrite_output_dir=True,\n","        num_train_epochs=20,\n","        per_device_train_batch_size=8,\n","        per_device_eval_batch_size=8,\n","        save_steps=500,\n","        save_total_limit=2,\n","        fp16=False,\n","        logging_dir=\"./logs\",\n","        logging_steps=200,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=500,\n","        logging_first_step=False,\n","        load_best_model_at_end = True,\n","        metric_for_best_model=\"micro-f1\",\n","    )\n","    main(training_args)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":579,"referenced_widgets":["dab03257b65446c8b22f7bfe24b3a1bd","c74ed66826b74ce7ad30150cf570fff5","ad86697e6a144f6bb8b139531d56fa3e","50a834b6544e4ca3bb686f8470816a7a","5fd98b2874004dc1bd72f94ee375a2b7","1e0b4cf9f95d462babaffe261c7a4300","407de26ae8a345a1a2898f4d95d122ad","557dd6e5dea34f7aaab2ba8b96da820a","673d360d952b46eb88fabec47194c01a","2b3371a5ec74443c8c49f2a33f6ec0d2","0e8c7e57b22546488dc9bc1380d994d3"]},"id":"93AKQCsjqs_2","executionInfo":{"status":"error","timestamp":1693762089437,"user_tz":-60,"elapsed":66538,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"}},"outputId":"98b14d45-2841-4ad6-e4aa-54a47ba7c73c"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n","[WARNING|modeling_utils.py:3553] 2023-09-03 17:27:08,611 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["Running tokenizer on train dataset:   0%|          | 0/5704 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dab03257b65446c8b22f7bfe24b3a1bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='656' max='14260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [  656/14260 00:59 < 20:29, 11.07 it/s, Epoch 0.92/20]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro-f1</th>\n","      <th>Micro-f1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.096700</td>\n","      <td>0.070992</td>\n","      <td>0.105195</td>\n","      <td>0.895163</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-af59fb6fded0>\u001b[0m in \u001b[0;36m<cell line: 427>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0mmetric_for_best_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"micro-f1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     )\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-30-af59fb6fded0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(training_args)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlast_checkpoint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         max_train_samples = (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1555\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1556\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1840\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m                     \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1842\u001b[0;31m                     \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1843\u001b[0m                 ):\n\u001b[1;32m   1844\u001b[0m                     \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["#!/usr/bin/env python\n","# coding=utf-8\n","\"\"\" Finetuning models on UNFAIR-ToC (e.g. Bert, RoBERTa, LEGAL-BERT).\"\"\"\n","\n","import logging\n","import os\n","import random\n","import sys\n","from dataclasses import dataclass, field\n","from typing import Optional\n","\n","import datasets\n","from datasets import load_dataset\n","from sklearn.metrics import f1_score\n","\n","from scipy.special import expit\n","import glob\n","import shutil\n","import numpy as np\n","\n","import transformers\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    DataCollatorWithPadding,\n","    EvalPrediction,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    default_data_collator,\n","    set_seed,\n","    EarlyStoppingCallback,\n",")\n","from transformers.trainer_utils import get_last_checkpoint\n","from transformers.utils import check_min_version\n","from transformers.utils.versions import require_version\n","\n","\n","# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n","check_min_version(\"4.9.0\")\n","\n","require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/text-classification/requirements.txt\")\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","@dataclass\n","class DataTrainingArguments:\n","    \"\"\"\n","    Arguments pertaining to what data we are going to input our model for training and eval.\n","\n","    Using `HfArgumentParser` we can turn this class\n","    into argparse arguments to be able to specify them on\n","    the command line.\n","    \"\"\"\n","\n","    max_seq_length: Optional[int] = field(\n","        default=128,\n","        metadata={\n","            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n","            \"than this will be truncated, sequences shorter will be padded.\"\n","        },\n","    )\n","    overwrite_cache: bool = field(\n","        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n","    )\n","    pad_to_max_length: bool = field(\n","        default=True,\n","        metadata={\n","            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n","            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n","        },\n","    )\n","    max_train_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_eval_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_predict_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    server_ip: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n","    server_port: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n","\n","\n","@dataclass\n","class ModelArguments:\n","    \"\"\"\n","    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n","    \"\"\"\n","\n","    model_name_or_path: str = field(\n","        default=None, metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n","    )\n","    config_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n","    )\n","    tokenizer_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n","    )\n","    cache_dir: Optional[str] = field(\n","        default=None,\n","        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n","    )\n","    do_lower_case: Optional[bool] = field(\n","        default=True,\n","        metadata={\"help\": \"arg to indicate if tokenizer should do lower case in AutoTokenizer.from_pretrained()\"},\n","    )\n","    use_fast_tokenizer: bool = field(\n","        default=True,\n","        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n","    )\n","    model_revision: str = field(\n","        default=\"main\",\n","        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n","    )\n","    use_auth_token: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n","            \"with private models).\"\n","        },\n","    )\n","\n","\n","def main(training_args):\n","    # See all possible arguments in src/transformers/training_args.py\n","    # or by passing the --help flag to this script.\n","    # We now keep distinct sets of args, for a cleaner separation of concerns.\n","\n","    model_args = ModelArguments(\n","        model_name_or_path=\"bert-base-uncased\",\n","        #hierarchical=True,\n","        do_lower_case=True,\n","        use_fast_tokenizer=True,\n","    )\n","    data_args = DataTrainingArguments(\n","        max_seq_length=128,\n","        #max_segments=64,\n","        #max_seg_length=128,\n","        overwrite_cache=False,\n","        pad_to_max_length=True,\n","    )\n","\n","    # Setup distant debugging if needed\n","    if data_args.server_ip and data_args.server_port:\n","        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n","        import ptvsd\n","\n","        print(\"Waiting for debugger attach\")\n","        ptvsd.enable_attach(address=(data_args.server_ip, data_args.server_port), redirect_output=True)\n","        ptvsd.wait_for_attach()\n","\n","    # Fix boolean parameter\n","    if model_args.do_lower_case == 'False' or not model_args.do_lower_case:\n","        model_args.do_lower_case = False\n","        'Tokenizer do_lower_case False'\n","    else:\n","        model_args.do_lower_case = True\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        handlers=[logging.StreamHandler(sys.stdout)],\n","    )\n","\n","    log_level = training_args.get_process_log_level()\n","    logger.setLevel(log_level)\n","    datasets.utils.logging.set_verbosity(log_level)\n","    transformers.utils.logging.set_verbosity(log_level)\n","    transformers.utils.logging.enable_default_handler()\n","    transformers.utils.logging.enable_explicit_format()\n","\n","    # Log on each process the small summary:\n","    logger.warning(\n","        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n","        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n","    )\n","    logger.info(f\"Training/evaluation parameters {training_args}\")\n","\n","    # Detecting last checkpoint.\n","    last_checkpoint = None\n","    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n","        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n","        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n","            raise ValueError(\n","                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n","                \"Use --overwrite_output_dir to overcome.\"\n","            )\n","        elif last_checkpoint is not None:\n","            logger.info(\n","                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n","                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n","            )\n","\n","    # Set seed before initializing model.\n","    set_seed(training_args.seed)\n","\n","    # In distributed training, the load_dataset function guarantees that only one local process can concurrently\n","    # download the dataset.\n","    # Downloading and loading eurlex dataset from the hub.\n","    if training_args.do_train:\n","        train_dataset = merged_dataset\n","\n","    if training_args.do_eval:\n","        eval_dataset = load_dataset(\"lex_glue\", \"unfair_tos\", split=\"validation\", data_dir='data', cache_dir=model_args.cache_dir)\n","\n","    if training_args.do_predict:\n","        predict_dataset = load_dataset(\"lex_glue\", \"unfair_tos\", split=\"test\", data_dir='data', cache_dir=model_args.cache_dir)\n","\n","    # Labels\n","    label_list = list(range(8))\n","    num_labels = len(label_list)\n","\n","    # Load pretrained model and tokenizer\n","    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n","    # download model & vocab.\n","    config = AutoConfig.from_pretrained(\n","        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n","        num_labels=num_labels,\n","        finetuning_task=\"unfair_toc\",\n","        cache_dir=model_args.cache_dir,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","\n","    if config.model_type == 'big_bird':\n","        config.attention_type = 'original_full'\n","\n","    if config.model_type == 'longformer':\n","        config.attention_window = [128] * config.num_hidden_layers\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n","        do_lower_case=model_args.do_lower_case,\n","        cache_dir=model_args.cache_dir,\n","        use_fast=model_args.use_fast_tokenizer,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        model_args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","        config=config,\n","        cache_dir=model_args.cache_dir,\n","        revision=model_args.model_revision,\n","        use_auth_token=True if model_args.use_auth_token else None,\n","    )\n","\n","    # Preprocessing the datasets\n","    # Padding strategy\n","    if data_args.pad_to_max_length:\n","        padding = \"max_length\"\n","    else:\n","        # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n","        padding = False\n","\n","    def preprocess_function(examples):\n","        # Tokenize the texts\n","        batch = tokenizer(\n","            examples[\"text\"],\n","            padding=padding,\n","            max_length=data_args.max_seq_length,\n","            truncation=True,\n","        )\n","        batch[\"labels\"] = [[1 if label in labels else 0 for label in label_list] for labels in\n","                              examples[\"labels\"]]\n","\n","        return batch\n","\n","    if training_args.do_train:\n","\n","        if data_args.max_train_samples is not None:\n","            train_dataset = train_dataset.select(range(data_args.max_train_samples))\n","            print(\"#################\")\n","            print (train_dataset('labels'))\n","\n","        with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n","            train_dataset = train_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on train dataset\",\n","            )\n","        # Log a few random samples from the training set:\n","        for index in random.sample(range(len(train_dataset)), 3):\n","            logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n","\n","    if training_args.do_eval:\n","        if data_args.max_eval_samples is not None:\n","            eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))\n","        with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n","            eval_dataset = eval_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on validation dataset\",\n","            )\n","\n","    if training_args.do_predict:\n","        if data_args.max_predict_samples is not None:\n","            predict_dataset = predict_dataset.select(range(data_args.max_predict_samples))\n","        with training_args.main_process_first(desc=\"prediction dataset map pre-processing\"):\n","            predict_dataset = predict_dataset.map(\n","                preprocess_function,\n","                batched=True,\n","                load_from_cache_file=not data_args.overwrite_cache,\n","                desc=\"Running tokenizer on prediction dataset\",\n","            )\n","\n","    # You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n","    # predictions and label_ids field) and has to return a dictionary string to float.\n","    def compute_metrics(p: EvalPrediction):\n","        # Fix gold labels\n","        y_true = np.zeros((p.label_ids.shape[0], p.label_ids.shape[1] + 1), dtype=np.int32)\n","        y_true[:, :-1] = p.label_ids\n","        y_true[:, -1] = (np.sum(p.label_ids, axis=1) == 0).astype('int32')\n","        # Fix predictions\n","        logits = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","        preds = (expit(logits) > 0.5).astype('int32')\n","        y_pred = np.zeros((p.label_ids.shape[0], p.label_ids.shape[1] + 1), dtype=np.int32)\n","        y_pred[:, :-1] = preds\n","        y_pred[:, -1] = (np.sum(preds, axis=1) == 0).astype('int32')\n","        # Compute scores\n","        macro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='macro', zero_division=0)\n","        micro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='micro', zero_division=0)\n","        return {'macro-f1': macro_f1, 'micro-f1': micro_f1}\n","\n","    # Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n","    if data_args.pad_to_max_length:\n","        data_collator = default_data_collator\n","    elif training_args.fp16:\n","        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n","    else:\n","        data_collator = None\n","\n","    # Initialize our Trainer\n","    trainer = MultilabelTrainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset if training_args.do_train else None,\n","        eval_dataset=eval_dataset if training_args.do_eval else None,\n","        compute_metrics=compute_metrics,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n","    )\n","\n","    # Training\n","    if training_args.do_train:\n","        checkpoint = None\n","        if training_args.resume_from_checkpoint is not None:\n","            checkpoint = training_args.resume_from_checkpoint\n","        elif last_checkpoint is not None:\n","            checkpoint = last_checkpoint\n","        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n","        metrics = train_result.metrics\n","        max_train_samples = (\n","            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n","        )\n","        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n","\n","        trainer.save_model()  # Saves the tokenizer too for easy upload\n","\n","        trainer.log_metrics(\"train\", metrics)\n","        trainer.save_metrics(\"train\", metrics)\n","        trainer.save_state()\n","\n","    # Evaluation\n","    if training_args.do_eval:\n","        logger.info(\"*** Evaluate ***\")\n","        metrics = trainer.evaluate(eval_dataset=eval_dataset)\n","\n","        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n","        metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n","\n","        trainer.log_metrics(\"eval\", metrics)\n","        trainer.save_metrics(\"eval\", metrics)\n","\n","    # Prediction\n","    if training_args.do_predict:\n","        logger.info(\"*** Predict ***\")\n","        predictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix=\"predict\")\n","\n","        max_predict_samples = (\n","            data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)\n","        )\n","        metrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n","\n","        trainer.log_metrics(\"predict\", metrics)\n","        trainer.save_metrics(\"predict\", metrics)\n","\n","        output_predict_file = os.path.join(training_args.output_dir, \"test_predictions.csv\")\n","        if trainer.is_world_process_zero():\n","            with open(output_predict_file, \"w\") as writer:\n","                if isinstance(predictions[0], np.float32):\n","                   pred_line = f'{predictions[0]:.5f}'\n","                   writer.write(pred_line)\n","                else:\n","                    for key, value in metrics.items():\n","                        if isinstance(value, float):\n","                           writer.write(f\"{key} = {value:.5f}\\n\")\n","                        else:\n","                           writer.write(f\"{key} = {value}\\n\")\n","\n","\n","    # Clean up checkpoints\n","    checkpoints = [filepath for filepath in glob.glob(f'{training_args.output_dir}/*/') if '/checkpoint' in filepath]\n","    for checkpoint in checkpoints:\n","        shutil.rmtree(checkpoint)\n","\n","\n","if __name__ == \"__main__\":\n"," #    For Evaluation\n","    training_args = TrainingArguments(\n","        do_train = True,\n","        do_eval = True,\n","        do_predict = True,\n","        output_dir=os.getcwd(),\n","        overwrite_output_dir=True,\n","        num_train_epochs=20,\n","        per_device_train_batch_size=8,\n","        per_device_eval_batch_size=8,\n","        save_steps=500,\n","        save_total_limit=2,\n","        fp16=False,\n","        logging_dir=\"./logs\",\n","        logging_steps=200,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=500,\n","        logging_first_step=False,\n","        load_best_model_at_end = True,\n","        metric_for_best_model=\"micro-f1\",\n","    )\n","    main(training_args)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":581,"referenced_widgets":["2b0adb6c5d0b4135bda36ae3c7a9778d","4ead2138e79c40449cb5d2c0f87263bf","780bb05d43da403db1b1983586b623eb","fa577dbc66f74ba389447df79507fa58","b47708a9f77048778dc40ff916279238","0835fa06d227476082942d7311109980","d0c210847a0740f59ff4237082bb342d","fc1dbdb020cf4c418a33f27711e074a4","dc3e2d364bef4161877e4f1d4e1246f6","83e135d7969847cbb21d4305cbcdcaed","0192dbf0c4b7475c915c411b43258f0f"]},"id":"3U-DIWfyhOV3","executionInfo":{"status":"error","timestamp":1693762283267,"user_tz":-60,"elapsed":92208,"user":{"displayName":"Sumit Gupta","userId":"07308348120090087318"}},"outputId":"00e67777-81f8-47c2-b85e-f2b56841ce54"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n","[WARNING|modeling_utils.py:3553] 2023-09-03 17:29:55,888 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["Running tokenizer on train dataset:   0%|          | 0/5704 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b0adb6c5d0b4135bda36ae3c7a9778d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1001' max='14260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 1000/14260 01:23 < 18:24, 12.01 it/s, Epoch 1.40/20]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro-f1</th>\n","      <th>Micro-f1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.096700</td>\n","      <td>0.070992</td>\n","      <td>0.105195</td>\n","      <td>0.895163</td>\n","    </tr>\n","  </tbody>\n","</table><p>\n","    <div>\n","      \n","      <progress value='122' max='285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [122/285 00:02 < 00:03, 46.46 it/s]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-af59fb6fded0>\u001b[0m in \u001b[0;36m<cell line: 427>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0mmetric_for_best_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"micro-f1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     )\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-32-af59fb6fded0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(training_args)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlast_checkpoint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         max_train_samples = (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1555\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1556\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1927\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1929\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1930\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1931\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2254\u001b[0m                     \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2255\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2256\u001b[0;31m                 \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2257\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2972\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   2973\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2974\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3160\u001b[0m             \u001b[0;31m# Prediction step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3161\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_loss_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3162\u001b[0m             \u001b[0mmain_input_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"main_input_name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3163\u001b[0m             \u001b[0minputs_decode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmain_input_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_inputs_for_metrics\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mprediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   3374\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhas_labels\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mloss_without_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3375\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3376\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3377\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-cdc1fa4dba53>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1560\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1562\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1563\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1020\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m         )\n\u001b[0;32m-> 1022\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1023\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    610\u001b[0m                 )\n\u001b[1;32m    611\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    613\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    498\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         )\n\u001b[0;32m--> 436\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add attentions if we output them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dropout_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[],"metadata":{"id":"dCgq_GmoW_bp"},"execution_count":null,"outputs":[]}]}